{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Replication_package_FineTuning.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "l8zPvNrpypq7",
        "32bONo87yxhA",
        "-wrG3YHXy2qp",
        "w4_enLxdzI8L",
        "bVPMF378zg0X",
        "quC7CexKzjOM",
        "xBSXtFYczkZx",
        "EDaV873tzqQ4"
      ],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNcxbxQR6Bu9NrzvJG6tzJN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/masies/CRA/blob/main/replication_package/Replication_package_FineTuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWgILLAAy_N-"
      },
      "source": [
        "# T5 Fine_Tuning\n",
        "\n",
        "in this notebook we will fine-tune different models on the datasets we already processed.\n",
        "\n",
        "We start by setting the environment. connecting colab to the GCS bucket and setting everything up for the TPU processor. (This colab uses TPU and high ram settings)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FX9_YlEdy8gz",
        "outputId": "2eb9d6c0-2393-4402-fa43-dc4e136065de"
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "#@title ## Set Your GCS credential\n",
        "project_id = 'prova-314912'#@param {type:\"string\"}\n",
        "bucket_name = 'gatto_bucket'#@param {type:\"string\"}\n",
        "\n",
        "!gcloud config set project {project_id}\n",
        "\n",
        "!gsutil cp gs://{bucket_name}/replication_package/requirements/requirements_FineTuning.txt  requirements_FineTuning.txt\n",
        "\n",
        "!pip3 install --upgrade pip\n",
        "!pip install -r /content/requirements_FineTuning.txt\n",
        "!pip install -qU t5\n",
        "\n",
        "import functools\n",
        "import os\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "\n",
        "import tensorflow.compat.v1 as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "import t5\n",
        "\n",
        "#Set the base dir(Google cloud bucket)\n",
        "BASE_DIR = \"gs://\" + bucket_name\n",
        "\n",
        "if not BASE_DIR or BASE_DIR == \"gs://\":\n",
        "  raise ValueError(\"You must enter a BASE_DIR.\")\n",
        "ON_CLOUD = True\n",
        "\n",
        "\n",
        "if ON_CLOUD:\n",
        "  import tensorflow_gcs_config\n",
        "  from google.colab import auth\n",
        "  # Set credentials for GCS reading/writing from Colab and TPU.\n",
        "  TPU_TOPOLOGY = \"2x2\"\n",
        "  try:\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
        "    TPU_ADDRESS = tpu.get_master()\n",
        "    print('Running on TPU:', TPU_ADDRESS)\n",
        "  except ValueError:\n",
        "    raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n",
        "  auth.authenticate_user()\n",
        "  tf.config.experimental_connect_to_host(TPU_ADDRESS)\n",
        "  tensorflow_gcs_config.configure_gcs_from_colab_auth()\n",
        "\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "# Improve logging.\n",
        "from contextlib import contextmanager\n",
        "import logging as py_logging\n",
        "\n",
        "if ON_CLOUD:\n",
        "  tf.get_logger().propagate = False\n",
        "  py_logging.root.setLevel('INFO')\n",
        "\n",
        "@contextmanager\n",
        "def tf_verbosity_level(level):\n",
        "  og_level = tf.logging.get_verbosity()\n",
        "  tf.logging.set_verbosity(level)\n",
        "  yield\n",
        "  tf.logging.set_verbosity(og_level)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Updated property [core/project].\n",
            "Copying gs://code_review_automation/replication_package/requirements/requirements_FineTuning.txt...\n",
            "/ [1 files][  7.0 KiB/  7.0 KiB]                                                \n",
            "Operation completed over 1 objects/7.0 KiB.                                      \n",
            "Requirement already satisfied: absl-py==0.12.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 1)) (0.12.0)\n",
            "Requirement already satisfied: alabaster==0.7.12 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 2)) (0.7.12)\n",
            "Requirement already satisfied: albumentations==0.1.12 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 3)) (0.1.12)\n",
            "Requirement already satisfied: altair==4.1.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 4)) (4.1.0)\n",
            "Requirement already satisfied: appdirs==1.4.4 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 5)) (1.4.4)\n",
            "Requirement already satisfied: argon2-cffi==20.1.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 6)) (20.1.0)\n",
            "Requirement already satisfied: arviz==0.11.2 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 7)) (0.11.2)\n",
            "Requirement already satisfied: astor==0.8.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 8)) (0.8.1)\n",
            "Requirement already satisfied: astropy==4.2.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 9)) (4.2.1)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 10)) (1.6.3)\n",
            "Requirement already satisfied: async-generator==1.10 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 11)) (1.10)\n",
            "Requirement already satisfied: atari-py==0.2.9 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 12)) (0.2.9)\n",
            "Requirement already satisfied: atomicwrites==1.4.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 13)) (1.4.0)\n",
            "Requirement already satisfied: attrs==21.2.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 14)) (21.2.0)\n",
            "Requirement already satisfied: audioread==2.1.9 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 15)) (2.1.9)\n",
            "Requirement already satisfied: autograd==1.3 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 16)) (1.3)\n",
            "Requirement already satisfied: Babel==2.9.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 17)) (2.9.1)\n",
            "Requirement already satisfied: backcall==0.2.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 18)) (0.2.0)\n",
            "Requirement already satisfied: beautifulsoup4==4.6.3 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 19)) (4.6.3)\n",
            "Requirement already satisfied: bleach==3.3.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 20)) (3.3.0)\n",
            "Requirement already satisfied: blis==0.4.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 21)) (0.4.1)\n",
            "Requirement already satisfied: bokeh==2.3.2 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 22)) (2.3.2)\n",
            "Requirement already satisfied: Bottleneck==1.3.2 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 23)) (1.3.2)\n",
            "Requirement already satisfied: branca==0.4.2 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 24)) (0.4.2)\n",
            "Requirement already satisfied: bs4==0.0.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 25)) (0.0.1)\n",
            "Requirement already satisfied: CacheControl==0.12.6 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 26)) (0.12.6)\n",
            "Requirement already satisfied: cached-property==1.5.2 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 27)) (1.5.2)\n",
            "Requirement already satisfied: cachetools==4.2.2 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 28)) (4.2.2)\n",
            "Requirement already satisfied: catalogue==1.0.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 29)) (1.0.0)\n",
            "Requirement already satisfied: certifi==2020.12.5 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 30)) (2020.12.5)\n",
            "Requirement already satisfied: cffi==1.14.5 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 31)) (1.14.5)\n",
            "Requirement already satisfied: cftime==1.5.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 32)) (1.5.0)\n",
            "Requirement already satisfied: chainer==7.4.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 33)) (7.4.0)\n",
            "Requirement already satisfied: chardet==3.0.4 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 34)) (3.0.4)\n",
            "Requirement already satisfied: click==7.1.2 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 35)) (7.1.2)\n",
            "Requirement already satisfied: cloudpickle==1.3.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 36)) (1.3.0)\n",
            "Requirement already satisfied: cmake==3.12.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 37)) (3.12.0)\n",
            "Requirement already satisfied: cmdstanpy==0.9.5 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 38)) (0.9.5)\n",
            "Requirement already satisfied: colorcet==2.0.6 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 39)) (2.0.6)\n",
            "Requirement already satisfied: colorlover==0.3.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 40)) (0.3.0)\n",
            "Requirement already satisfied: community==1.0.0b1 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 41)) (1.0.0b1)\n",
            "Requirement already satisfied: contextlib2==0.5.5 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 42)) (0.5.5)\n",
            "Requirement already satisfied: convertdate==2.3.2 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 43)) (2.3.2)\n",
            "Requirement already satisfied: coverage==3.7.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 44)) (3.7.1)\n",
            "Requirement already satisfied: coveralls==0.5 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 45)) (0.5)\n",
            "Requirement already satisfied: crcmod==1.7 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 46)) (1.7)\n",
            "Requirement already satisfied: cufflinks==0.17.3 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 47)) (0.17.3)\n",
            "Requirement already satisfied: cvxopt==1.2.6 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 48)) (1.2.6)\n",
            "Requirement already satisfied: cvxpy==1.0.31 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 49)) (1.0.31)\n",
            "Requirement already satisfied: cycler==0.10.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 50)) (0.10.0)\n",
            "Requirement already satisfied: cymem==2.0.5 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 51)) (2.0.5)\n",
            "Requirement already satisfied: Cython==0.29.23 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 52)) (0.29.23)\n",
            "Requirement already satisfied: daft==0.0.4 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 53)) (0.0.4)\n",
            "Requirement already satisfied: dask==2.12.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 54)) (2.12.0)\n",
            "Requirement already satisfied: datascience==0.10.6 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 55)) (0.10.6)\n",
            "Requirement already satisfied: debugpy==1.0.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 56)) (1.0.0)\n",
            "Requirement already satisfied: decorator==4.4.2 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 57)) (4.4.2)\n",
            "Requirement already satisfied: defusedxml==0.7.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 58)) (0.7.1)\n",
            "Requirement already satisfied: descartes==1.1.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 59)) (1.1.0)\n",
            "Requirement already satisfied: dill==0.3.3 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 60)) (0.3.3)\n",
            "Requirement already satisfied: distributed==1.25.3 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 61)) (1.25.3)\n",
            "Requirement already satisfied: dlib==19.18.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 62)) (19.18.0)\n",
            "Requirement already satisfied: dm-tree==0.1.6 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 63)) (0.1.6)\n",
            "Requirement already satisfied: docopt==0.6.2 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 64)) (0.6.2)\n",
            "Requirement already satisfied: docutils==0.17.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 65)) (0.17.1)\n",
            "Requirement already satisfied: dopamine-rl==1.0.5 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 66)) (1.0.5)\n",
            "Requirement already satisfied: earthengine-api==0.1.266 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 67)) (0.1.266)\n",
            "Requirement already satisfied: easydict==1.9 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 68)) (1.9)\n",
            "Requirement already satisfied: ecos==2.0.7.post1 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 69)) (2.0.7.post1)\n",
            "Requirement already satisfied: editdistance==0.5.3 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 70)) (0.5.3)\n",
            "Requirement already satisfied: en-core-web-sm==2.2.5 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 71)) (2.2.5)\n",
            "Requirement already satisfied: entrypoints==0.3 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 72)) (0.3)\n",
            "Requirement already satisfied: ephem==3.7.7.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 73)) (3.7.7.1)\n",
            "Requirement already satisfied: et-xmlfile==1.1.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 74)) (1.1.0)\n",
            "Requirement already satisfied: fa2==0.3.5 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 75)) (0.3.5)\n",
            "Requirement already satisfied: fastai==1.0.61 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 76)) (1.0.61)\n",
            "Requirement already satisfied: fastdtw==0.3.4 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 77)) (0.3.4)\n",
            "Requirement already satisfied: fastprogress==1.0.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 78)) (1.0.0)\n",
            "Requirement already satisfied: fastrlock==0.6 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 79)) (0.6)\n",
            "Requirement already satisfied: fbprophet==0.7.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 80)) (0.7.1)\n",
            "Requirement already satisfied: feather-format==0.4.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 81)) (0.4.1)\n",
            "Requirement already satisfied: filelock==3.0.12 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 82)) (3.0.12)\n",
            "Requirement already satisfied: firebase-admin==4.4.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 83)) (4.4.0)\n",
            "Requirement already satisfied: fix-yahoo-finance==0.0.22 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 84)) (0.0.22)\n",
            "Requirement already satisfied: Flask==1.1.4 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 85)) (1.1.4)\n",
            "Requirement already satisfied: flatbuffers==1.12 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 86)) (1.12)\n",
            "Requirement already satisfied: folium==0.8.3 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 87)) (0.8.3)\n",
            "Requirement already satisfied: future==0.16.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 88)) (0.16.0)\n",
            "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 89)) (0.4.0)\n",
            "Requirement already satisfied: GDAL==2.2.2 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 90)) (2.2.2)\n",
            "Requirement already satisfied: gdown==3.6.4 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 91)) (3.6.4)\n",
            "Requirement already satisfied: gensim==3.6.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 92)) (3.6.0)\n",
            "Requirement already satisfied: geographiclib==1.50 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 93)) (1.50)\n",
            "Requirement already satisfied: geopy==1.17.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 94)) (1.17.0)\n",
            "Requirement already satisfied: gin-config==0.4.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 95)) (0.4.0)\n",
            "Requirement already satisfied: glob2==0.7 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 96)) (0.7)\n",
            "Requirement already satisfied: google==2.0.3 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 97)) (2.0.3)\n",
            "Requirement already satisfied: google-api-core==1.26.3 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 98)) (1.26.3)\n",
            "Requirement already satisfied: google-api-python-client==1.12.8 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 99)) (1.12.8)\n",
            "Requirement already satisfied: google-auth==1.30.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 100)) (1.30.0)\n",
            "Requirement already satisfied: google-auth-httplib2==0.0.4 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 101)) (0.0.4)\n",
            "Requirement already satisfied: google-auth-oauthlib==0.4.4 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 102)) (0.4.4)\n",
            "Requirement already satisfied: google-cloud-bigquery==1.21.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 103)) (1.21.0)\n",
            "Requirement already satisfied: google-cloud-bigquery-storage==1.1.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 104)) (1.1.0)\n",
            "Requirement already satisfied: google-cloud-core==1.0.3 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 105)) (1.0.3)\n",
            "Requirement already satisfied: google-cloud-datastore==1.8.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 106)) (1.8.0)\n",
            "Requirement already satisfied: google-cloud-firestore==1.7.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 107)) (1.7.0)\n",
            "Requirement already satisfied: google-cloud-language==1.2.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 108)) (1.2.0)\n",
            "Requirement already satisfied: google-cloud-storage==1.18.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 109)) (1.18.1)\n",
            "Requirement already satisfied: google-cloud-translate==1.5.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 110)) (1.5.0)\n",
            "Requirement already satisfied: google-colab==1.0.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 111)) (1.0.0)\n",
            "Requirement already satisfied: google-pasta==0.2.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 112)) (0.2.0)\n",
            "Requirement already satisfied: google-resumable-media==0.4.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 113)) (0.4.1)\n",
            "Requirement already satisfied: googleapis-common-protos==1.53.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 114)) (1.53.0)\n",
            "Requirement already satisfied: googledrivedownloader==0.4 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 115)) (0.4)\n",
            "Requirement already satisfied: graphviz==0.10.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 116)) (0.10.1)\n",
            "Requirement already satisfied: greenlet==1.1.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 117)) (1.1.0)\n",
            "Requirement already satisfied: grpcio==1.34.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 118)) (1.34.1)\n",
            "Requirement already satisfied: gspread==3.0.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 119)) (3.0.1)\n",
            "Requirement already satisfied: gspread-dataframe==3.0.8 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 120)) (3.0.8)\n",
            "Requirement already satisfied: gym==0.17.3 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 121)) (0.17.3)\n",
            "Requirement already satisfied: h5py==3.1.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 122)) (3.1.0)\n",
            "Requirement already satisfied: HeapDict==1.0.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 123)) (1.0.1)\n",
            "Requirement already satisfied: hijri-converter==2.1.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 124)) (2.1.1)\n",
            "Requirement already satisfied: holidays==0.10.5.2 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 125)) (0.10.5.2)\n",
            "Requirement already satisfied: holoviews==1.14.3 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 126)) (1.14.3)\n",
            "Requirement already satisfied: html5lib==1.0.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 127)) (1.0.1)\n",
            "Requirement already satisfied: httpimport==0.5.18 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 128)) (0.5.18)\n",
            "Requirement already satisfied: httplib2==0.17.4 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 129)) (0.17.4)\n",
            "Requirement already satisfied: httplib2shim==0.0.3 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 130)) (0.0.3)\n",
            "Collecting huggingface-hub==0.0.8\n",
            "  Downloading https://files.pythonhosted.org/packages/a1/88/7b1e45720ecf59c6c6737ff332f41c955963090a18e72acbcbeac6b25e86/huggingface_hub-0.0.8-py3-none-any.whl\n",
            "Requirement already satisfied: humanize==0.5.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 132)) (0.5.1)\n",
            "Requirement already satisfied: hyperopt==0.1.2 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 133)) (0.1.2)\n",
            "Requirement already satisfied: ideep4py==2.0.0.post3 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 134)) (2.0.0.post3)\n",
            "Requirement already satisfied: idna==2.10 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 135)) (2.10)\n",
            "Requirement already satisfied: imageio==2.4.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 136)) (2.4.1)\n",
            "Requirement already satisfied: imagesize==1.2.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 137)) (1.2.0)\n",
            "Requirement already satisfied: imbalanced-learn==0.4.3 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 138)) (0.4.3)\n",
            "Requirement already satisfied: imblearn==0.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 139)) (0.0)\n",
            "Requirement already satisfied: imgaug==0.2.9 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 140)) (0.2.9)\n",
            "Requirement already satisfied: importlib-metadata==4.0.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 141)) (4.0.1)\n",
            "Requirement already satisfied: importlib-resources==5.1.3 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 142)) (5.1.3)\n",
            "Requirement already satisfied: imutils==0.5.4 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 143)) (0.5.4)\n",
            "Requirement already satisfied: inflect==2.1.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 144)) (2.1.0)\n",
            "Requirement already satisfied: iniconfig==1.1.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 145)) (1.1.1)\n",
            "Requirement already satisfied: install==1.3.4 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 146)) (1.3.4)\n",
            "Requirement already satisfied: intel-openmp==2021.2.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 147)) (2021.2.0)\n",
            "Requirement already satisfied: intervaltree==2.1.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 148)) (2.1.0)\n",
            "Requirement already satisfied: ipykernel==4.10.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 149)) (4.10.1)\n",
            "Requirement already satisfied: ipython==5.5.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 150)) (5.5.0)\n",
            "Requirement already satisfied: ipython-genutils==0.2.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 151)) (0.2.0)\n",
            "Requirement already satisfied: ipython-sql==0.3.9 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 152)) (0.3.9)\n",
            "Requirement already satisfied: ipywidgets==7.6.3 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 153)) (7.6.3)\n",
            "Requirement already satisfied: itsdangerous==1.1.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 154)) (1.1.0)\n",
            "Requirement already satisfied: jax==0.2.13 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 155)) (0.2.13)\n",
            "Requirement already satisfied: jaxlib==0.1.66+cuda110 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 156)) (0.1.66+cuda110)\n",
            "Requirement already satisfied: jdcal==1.4.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 157)) (1.4.1)\n",
            "Requirement already satisfied: jedi==0.18.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 158)) (0.18.0)\n",
            "Requirement already satisfied: jieba==0.42.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 159)) (0.42.1)\n",
            "Requirement already satisfied: Jinja2==2.11.3 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 160)) (2.11.3)\n",
            "Requirement already satisfied: joblib==1.0.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 161)) (1.0.1)\n",
            "Requirement already satisfied: jpeg4py==0.1.4 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 162)) (0.1.4)\n",
            "Requirement already satisfied: jsonschema==2.6.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 163)) (2.6.0)\n",
            "Requirement already satisfied: jupyter==1.0.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 164)) (1.0.0)\n",
            "Requirement already satisfied: jupyter-client==5.3.5 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 165)) (5.3.5)\n",
            "Requirement already satisfied: jupyter-console==5.2.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 166)) (5.2.0)\n",
            "Requirement already satisfied: jupyter-core==4.7.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 167)) (4.7.1)\n",
            "Requirement already satisfied: jupyterlab-pygments==0.1.2 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 168)) (0.1.2)\n",
            "Requirement already satisfied: jupyterlab-widgets==1.0.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 169)) (1.0.0)\n",
            "Requirement already satisfied: kaggle==1.5.12 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 170)) (1.5.12)\n",
            "Requirement already satisfied: kapre==0.3.5 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 171)) (0.3.5)\n",
            "Requirement already satisfied: Keras==2.4.3 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 172)) (2.4.3)\n",
            "Requirement already satisfied: keras-nightly==2.5.0.dev2021032900 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 173)) (2.5.0.dev2021032900)\n",
            "Requirement already satisfied: Keras-Preprocessing==1.1.2 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 174)) (1.1.2)\n",
            "Requirement already satisfied: keras-vis==0.4.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 175)) (0.4.1)\n",
            "Requirement already satisfied: kiwisolver==1.3.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 176)) (1.3.1)\n",
            "Requirement already satisfied: korean-lunar-calendar==0.2.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 177)) (0.2.1)\n",
            "Requirement already satisfied: librosa==0.8.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 178)) (0.8.0)\n",
            "Requirement already satisfied: lightgbm==2.2.3 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 179)) (2.2.3)\n",
            "Requirement already satisfied: llvmlite==0.34.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 180)) (0.34.0)\n",
            "Requirement already satisfied: lmdb==0.99 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 181)) (0.99)\n",
            "Requirement already satisfied: LunarCalendar==0.0.9 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 182)) (0.0.9)\n",
            "Requirement already satisfied: lxml==4.2.6 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 183)) (4.2.6)\n",
            "Requirement already satisfied: Markdown==3.3.4 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 184)) (3.3.4)\n",
            "Requirement already satisfied: MarkupSafe==2.0.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 185)) (2.0.1)\n",
            "Requirement already satisfied: matplotlib==3.2.2 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 186)) (3.2.2)\n",
            "Requirement already satisfied: matplotlib-inline==0.1.2 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 187)) (0.1.2)\n",
            "Requirement already satisfied: matplotlib-venn==0.11.6 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 188)) (0.11.6)\n",
            "Collecting mesh-tensorflow==0.1.19\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ce/10/37df0bc87ebf84e1414613176340e3aadc3697d2bd112bf63d3d4b1e848a/mesh_tensorflow-0.1.19-py3-none-any.whl (366kB)\n",
            "\u001b[K     |████████████████████████████████| 368kB 8.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: missingno==0.4.2 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 190)) (0.4.2)\n",
            "Requirement already satisfied: mistune==0.8.4 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 191)) (0.8.4)\n",
            "Requirement already satisfied: mizani==0.6.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 192)) (0.6.0)\n",
            "Requirement already satisfied: mkl==2019.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 193)) (2019.0)\n",
            "Requirement already satisfied: mlxtend==0.14.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 194)) (0.14.0)\n",
            "Requirement already satisfied: more-itertools==8.7.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 195)) (8.7.0)\n",
            "Requirement already satisfied: moviepy==0.2.3.5 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 196)) (0.2.3.5)\n",
            "Requirement already satisfied: mpmath==1.2.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 197)) (1.2.1)\n",
            "Requirement already satisfied: msgpack==1.0.2 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 198)) (1.0.2)\n",
            "Requirement already satisfied: multiprocess==0.70.11.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 199)) (0.70.11.1)\n",
            "Requirement already satisfied: multitasking==0.0.9 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 200)) (0.0.9)\n",
            "Requirement already satisfied: murmurhash==1.0.5 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 201)) (1.0.5)\n",
            "Requirement already satisfied: music21==5.5.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 202)) (5.5.0)\n",
            "Requirement already satisfied: natsort==5.5.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 203)) (5.5.0)\n",
            "Requirement already satisfied: nbclient==0.5.3 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 204)) (0.5.3)\n",
            "Requirement already satisfied: nbconvert==5.6.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 205)) (5.6.1)\n",
            "Requirement already satisfied: nbformat==5.1.3 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 206)) (5.1.3)\n",
            "Requirement already satisfied: nest-asyncio==1.5.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 207)) (1.5.1)\n",
            "Requirement already satisfied: netCDF4==1.5.6 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 208)) (1.5.6)\n",
            "Requirement already satisfied: networkx==2.5.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 209)) (2.5.1)\n",
            "Requirement already satisfied: nibabel==3.0.2 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 210)) (3.0.2)\n",
            "Requirement already satisfied: nltk==3.2.5 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 211)) (3.2.5)\n",
            "Requirement already satisfied: notebook==5.3.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 212)) (5.3.1)\n",
            "Requirement already satisfied: numba==0.51.2 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 213)) (0.51.2)\n",
            "Requirement already satisfied: numexpr==2.7.3 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 214)) (2.7.3)\n",
            "Requirement already satisfied: numpy==1.19.5 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 215)) (1.19.5)\n",
            "Requirement already satisfied: nvidia-ml-py3==7.352.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 216)) (7.352.0)\n",
            "Requirement already satisfied: oauth2client==4.1.3 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 217)) (4.1.3)\n",
            "Requirement already satisfied: oauthlib==3.1.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 218)) (3.1.0)\n",
            "Requirement already satisfied: okgrade==0.4.3 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 219)) (0.4.3)\n",
            "Requirement already satisfied: opencv-contrib-python==4.1.2.30 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 220)) (4.1.2.30)\n",
            "Requirement already satisfied: opencv-python==4.1.2.30 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 221)) (4.1.2.30)\n",
            "Requirement already satisfied: openpyxl==2.5.9 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 222)) (2.5.9)\n",
            "Requirement already satisfied: opt-einsum==3.3.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 223)) (3.3.0)\n",
            "Requirement already satisfied: osqp==0.6.2.post0 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 224)) (0.6.2.post0)\n",
            "Requirement already satisfied: packaging==20.9 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 225)) (20.9)\n",
            "Requirement already satisfied: palettable==3.3.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 226)) (3.3.0)\n",
            "Requirement already satisfied: pandas==1.1.5 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 227)) (1.1.5)\n",
            "Requirement already satisfied: pandas-datareader==0.9.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 228)) (0.9.0)\n",
            "Requirement already satisfied: pandas-gbq==0.13.3 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 229)) (0.13.3)\n",
            "Requirement already satisfied: pandas-profiling==1.4.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 230)) (1.4.1)\n",
            "Requirement already satisfied: pandocfilters==1.4.3 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 231)) (1.4.3)\n",
            "Requirement already satisfied: panel==0.11.3 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 232)) (0.11.3)\n",
            "Requirement already satisfied: param==1.10.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 233)) (1.10.1)\n",
            "Requirement already satisfied: parso==0.8.2 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 234)) (0.8.2)\n",
            "Requirement already satisfied: pathlib==1.0.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 235)) (1.0.1)\n",
            "Requirement already satisfied: patsy==0.5.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 236)) (0.5.1)\n",
            "Requirement already satisfied: pexpect==4.8.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 237)) (4.8.0)\n",
            "Requirement already satisfied: pickleshare==0.7.5 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 238)) (0.7.5)\n",
            "Requirement already satisfied: Pillow==7.1.2 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 239)) (7.1.2)\n",
            "Requirement already satisfied: pip-tools==4.5.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 240)) (4.5.1)\n",
            "Requirement already satisfied: plac==1.1.3 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 241)) (1.1.3)\n",
            "Requirement already satisfied: plotly==4.4.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 242)) (4.4.1)\n",
            "Requirement already satisfied: plotnine==0.6.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 243)) (0.6.0)\n",
            "Requirement already satisfied: pluggy==0.7.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 244)) (0.7.1)\n",
            "Requirement already satisfied: pooch==1.3.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 245)) (1.3.0)\n",
            "Collecting portalocker==2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/89/a6/3814b7107e0788040870e8825eebf214d72166adf656ba7d4bf14759a06a/portalocker-2.0.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: portpicker==1.3.9 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 247)) (1.3.9)\n",
            "Requirement already satisfied: prefetch-generator==1.0.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 248)) (1.0.1)\n",
            "Requirement already satisfied: preshed==3.0.5 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 249)) (3.0.5)\n",
            "Requirement already satisfied: prettytable==2.1.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 250)) (2.1.0)\n",
            "Requirement already satisfied: progressbar2==3.38.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 251)) (3.38.0)\n",
            "Requirement already satisfied: prometheus-client==0.10.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 252)) (0.10.1)\n",
            "Requirement already satisfied: promise==2.3 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 253)) (2.3)\n",
            "Requirement already satisfied: prompt-toolkit==1.0.18 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 254)) (1.0.18)\n",
            "Requirement already satisfied: protobuf==3.12.4 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 255)) (3.12.4)\n",
            "Requirement already satisfied: psutil==5.4.8 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 256)) (5.4.8)\n",
            "Requirement already satisfied: psycopg2==2.7.6.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 257)) (2.7.6.1)\n",
            "Requirement already satisfied: ptyprocess==0.7.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 258)) (0.7.0)\n",
            "Requirement already satisfied: py==1.10.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 259)) (1.10.0)\n",
            "Requirement already satisfied: pyarrow==3.0.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 260)) (3.0.0)\n",
            "Requirement already satisfied: pyasn1==0.4.8 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 261)) (0.4.8)\n",
            "Requirement already satisfied: pyasn1-modules==0.2.8 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 262)) (0.2.8)\n",
            "Requirement already satisfied: pycocotools==2.0.2 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 263)) (2.0.2)\n",
            "Requirement already satisfied: pycparser==2.20 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 264)) (2.20)\n",
            "Requirement already satisfied: pyct==0.4.8 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 265)) (0.4.8)\n",
            "Requirement already satisfied: pydata-google-auth==1.2.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 266)) (1.2.0)\n",
            "Requirement already satisfied: pydot==1.3.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 267)) (1.3.0)\n",
            "Requirement already satisfied: pydot-ng==2.0.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 268)) (2.0.0)\n",
            "Requirement already satisfied: pydotplus==2.0.2 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 269)) (2.0.2)\n",
            "Requirement already satisfied: PyDrive==1.3.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 270)) (1.3.1)\n",
            "Requirement already satisfied: pyemd==0.5.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 271)) (0.5.1)\n",
            "Requirement already satisfied: pyerfa==2.0.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 272)) (2.0.0)\n",
            "Requirement already satisfied: pyglet==1.5.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 273)) (1.5.0)\n",
            "Requirement already satisfied: Pygments==2.6.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 274)) (2.6.1)\n",
            "Requirement already satisfied: pygobject==3.26.1 in /usr/lib/python3/dist-packages (from -r /content/requirements_FineTuning.txt (line 275)) (3.26.1)\n",
            "Requirement already satisfied: pymc3==3.11.2 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 276)) (3.11.2)\n",
            "Requirement already satisfied: PyMeeus==0.5.11 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 277)) (0.5.11)\n",
            "Requirement already satisfied: pymongo==3.11.4 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 278)) (3.11.4)\n",
            "Requirement already satisfied: pymystem3==0.2.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 279)) (0.2.0)\n",
            "Requirement already satisfied: PyOpenGL==3.1.5 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 280)) (3.1.5)\n",
            "Requirement already satisfied: pyparsing==2.4.7 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 281)) (2.4.7)\n",
            "Requirement already satisfied: pyrsistent==0.17.3 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 282)) (0.17.3)\n",
            "Requirement already satisfied: pysndfile==1.3.8 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 283)) (1.3.8)\n",
            "Requirement already satisfied: PySocks==1.7.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 284)) (1.7.1)\n",
            "Requirement already satisfied: pystan==2.19.1.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 285)) (2.19.1.1)\n",
            "Requirement already satisfied: pytest==3.6.4 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 286)) (3.6.4)\n",
            "Requirement already satisfied: python-apt==0.0.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 287)) (0.0.0)\n",
            "Requirement already satisfied: python-chess==0.23.11 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 288)) (0.23.11)\n",
            "Requirement already satisfied: python-dateutil==2.8.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 289)) (2.8.1)\n",
            "Requirement already satisfied: python-louvain==0.15 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 290)) (0.15)\n",
            "Requirement already satisfied: python-slugify==5.0.2 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 291)) (5.0.2)\n",
            "Requirement already satisfied: python-utils==2.5.6 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 292)) (2.5.6)\n",
            "Requirement already satisfied: pytz==2018.9 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 293)) (2018.9)\n",
            "Requirement already satisfied: pyviz-comms==2.0.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 294)) (2.0.1)\n",
            "Requirement already satisfied: PyWavelets==1.1.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 295)) (1.1.1)\n",
            "Requirement already satisfied: PyYAML==3.13 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 296)) (3.13)\n",
            "Requirement already satisfied: pyzmq==22.0.3 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 297)) (22.0.3)\n",
            "Requirement already satisfied: qdldl==0.1.5.post0 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 298)) (0.1.5.post0)\n",
            "Requirement already satisfied: qtconsole==5.1.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 299)) (5.1.0)\n",
            "Requirement already satisfied: QtPy==1.9.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 300)) (1.9.0)\n",
            "Requirement already satisfied: regex==2019.12.20 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 301)) (2019.12.20)\n",
            "Requirement already satisfied: requests==2.23.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 302)) (2.23.0)\n",
            "Requirement already satisfied: requests-oauthlib==1.3.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 303)) (1.3.0)\n",
            "Requirement already satisfied: resampy==0.2.2 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 304)) (0.2.2)\n",
            "Requirement already satisfied: retrying==1.3.3 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 305)) (1.3.3)\n",
            "Collecting rouge-score==0.0.4\n",
            "  Downloading https://files.pythonhosted.org/packages/1f/56/a81022436c08b9405a5247b71635394d44fe7e1dbedc4b28c740e09c2840/rouge_score-0.0.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: rpy2==3.4.4 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 307)) (3.4.4)\n",
            "Requirement already satisfied: rsa==4.7.2 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 308)) (4.7.2)\n",
            "Collecting sacrebleu==1.5.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7e/57/0c7ca4e31a126189dab99c19951910bd081dea5bbd25f24b77107750eae7/sacrebleu-1.5.1-py3-none-any.whl (54kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 4.3MB/s \n",
            "\u001b[?25hCollecting sacremoses==0.0.45\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 8.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-image==0.16.2 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 311)) (0.16.2)\n",
            "Requirement already satisfied: scikit-learn==0.22.2.post1 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 312)) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 313)) (1.4.1)\n",
            "Requirement already satisfied: screen-resolution-extra==0.0.0 in /usr/lib/python3/dist-packages (from -r /content/requirements_FineTuning.txt (line 314)) (0.0.0)\n",
            "Requirement already satisfied: scs==2.1.3 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 315)) (2.1.3)\n",
            "Requirement already satisfied: seaborn==0.11.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 316)) (0.11.1)\n",
            "Requirement already satisfied: semver==2.13.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 317)) (2.13.0)\n",
            "Requirement already satisfied: Send2Trash==1.5.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 318)) (1.5.0)\n",
            "Collecting sentencepiece==0.1.95\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 19.2MB/s \n",
            "\u001b[?25hCollecting seqio==0.0.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/37/28/0a58b6eae1eaf11cac66969cb231826b8c4a9eef3479cc96ba085361af6d/seqio-0.0.5-py3-none-any.whl (249kB)\n",
            "\u001b[K     |████████████████████████████████| 256kB 29.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools-git==1.2 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 321)) (1.2)\n",
            "Requirement already satisfied: Shapely==1.7.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 322)) (1.7.1)\n",
            "Requirement already satisfied: simplegeneric==0.8.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 323)) (0.8.1)\n",
            "Requirement already satisfied: six==1.15.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 324)) (1.15.0)\n",
            "Requirement already satisfied: sklearn==0.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 325)) (0.0)\n",
            "Requirement already satisfied: sklearn-pandas==1.8.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 326)) (1.8.0)\n",
            "Requirement already satisfied: smart-open==5.0.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 327)) (5.0.0)\n",
            "Requirement already satisfied: snowballstemmer==2.1.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 328)) (2.1.0)\n",
            "Requirement already satisfied: sortedcontainers==2.4.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 329)) (2.4.0)\n",
            "Requirement already satisfied: SoundFile==0.10.3.post1 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 330)) (0.10.3.post1)\n",
            "Requirement already satisfied: spacy==2.2.4 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 331)) (2.2.4)\n",
            "Requirement already satisfied: Sphinx==1.8.5 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 332)) (1.8.5)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml==1.1.4 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 333)) (1.1.4)\n",
            "Requirement already satisfied: sphinxcontrib-websupport==1.2.4 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 334)) (1.2.4)\n",
            "Requirement already satisfied: SQLAlchemy==1.4.15 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 335)) (1.4.15)\n",
            "Requirement already satisfied: sqlparse==0.4.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 336)) (0.4.1)\n",
            "Requirement already satisfied: srsly==1.0.5 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 337)) (1.0.5)\n",
            "Requirement already satisfied: statsmodels==0.10.2 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 338)) (0.10.2)\n",
            "Requirement already satisfied: sympy==1.7.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 339)) (1.7.1)\n",
            "Collecting t5==0.9.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/e4/e2dc66207464795aafecc5c8cef9a35b5c9a61b974ac60a2c306c12bfd4c/t5-0.9.1-py3-none-any.whl (152kB)\n",
            "\u001b[K     |████████████████████████████████| 153kB 28.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: tables==3.4.4 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 341)) (3.4.4)\n",
            "Requirement already satisfied: tabulate==0.8.9 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 342)) (0.8.9)\n",
            "Requirement already satisfied: tblib==1.7.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 343)) (1.7.0)\n",
            "Requirement already satisfied: tensorboard==2.5.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 344)) (2.5.0)\n",
            "Requirement already satisfied: tensorboard-data-server==0.6.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 345)) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit==1.8.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 346)) (1.8.0)\n",
            "Requirement already satisfied: tensorflow==2.5.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 347)) (2.5.0)\n",
            "Requirement already satisfied: tensorflow-datasets==4.0.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 348)) (4.0.1)\n",
            "Requirement already satisfied: tensorflow-estimator==2.5.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 349)) (2.5.0)\n",
            "Requirement already satisfied: tensorflow-gcs-config==2.5.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 350)) (2.5.0)\n",
            "Requirement already satisfied: tensorflow-hub==0.12.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 351)) (0.12.0)\n",
            "Requirement already satisfied: tensorflow-metadata==1.0.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 352)) (1.0.0)\n",
            "Requirement already satisfied: tensorflow-probability==0.12.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 353)) (0.12.1)\n",
            "Collecting tensorflow-text==2.5.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c0/ed/bbb51e9eccca0c2bfdf9df66e54cdff563b6f32daed9255da9b9a541368f/tensorflow_text-2.5.0-cp37-cp37m-manylinux1_x86_64.whl (4.3MB)\n",
            "\u001b[K     |████████████████████████████████| 4.3MB 30.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor==1.1.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 355)) (1.1.0)\n",
            "Requirement already satisfied: terminado==0.10.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 356)) (0.10.0)\n",
            "Requirement already satisfied: testpath==0.5.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 357)) (0.5.0)\n",
            "Requirement already satisfied: text-unidecode==1.3 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 358)) (1.3)\n",
            "Requirement already satisfied: textblob==0.15.3 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 359)) (0.15.3)\n",
            "Collecting tfds-nightly==4.3.0.dev202106070106\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/16/a9/2ecfb0ce9687bce3ba79c2e4e2c001fbb846df1c9b44a0cc79fa4772c1dc/tfds_nightly-4.3.0.dev202106070106-py3-none-any.whl (3.9MB)\n",
            "\u001b[K     |████████████████████████████████| 3.9MB 50.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: Theano-PyMC==1.1.2 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 361)) (1.1.2)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 362)) (7.4.0)\n",
            "Requirement already satisfied: tifffile==2021.4.8 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 363)) (2021.4.8)\n",
            "Collecting tokenizers==0.10.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/e2/df3543e8ffdab68f5acc73f613de9c2b155ac47f162e725dcac87c521c11/tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 49.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: toml==0.10.2 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 365)) (0.10.2)\n",
            "Requirement already satisfied: toolz==0.11.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 366)) (0.11.1)\n",
            "Requirement already satisfied: torch==1.8.1+cu101 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 367)) (1.8.1+cu101)\n",
            "Requirement already satisfied: torchsummary==1.5.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 368)) (1.5.1)\n",
            "Requirement already satisfied: torchtext==0.9.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 369)) (0.9.1)\n",
            "Requirement already satisfied: torchvision==0.9.1+cu101 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 370)) (0.9.1+cu101)\n",
            "Requirement already satisfied: tornado==5.1.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 371)) (5.1.1)\n",
            "Requirement already satisfied: tqdm==4.41.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 372)) (4.41.1)\n",
            "Requirement already satisfied: traitlets==5.0.5 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 373)) (5.0.5)\n",
            "Collecting transformers==4.6.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d5/43/cfe4ee779bbd6a678ac6a97c5a5cdeb03c35f9eaebbb9720b036680f9a2d/transformers-4.6.1-py3-none-any.whl (2.2MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3MB 46.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: tweepy==3.10.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 375)) (3.10.0)\n",
            "Requirement already satisfied: typeguard==2.7.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 376)) (2.7.1)\n",
            "Requirement already satisfied: typing-extensions==3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 377)) (3.7.4.3)\n",
            "Requirement already satisfied: tzlocal==1.5.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 378)) (1.5.1)\n",
            "Requirement already satisfied: uritemplate==3.0.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 379)) (3.0.1)\n",
            "Requirement already satisfied: urllib3==1.24.3 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 380)) (1.24.3)\n",
            "Requirement already satisfied: vega-datasets==0.9.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 381)) (0.9.0)\n",
            "Requirement already satisfied: wasabi==0.8.2 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 382)) (0.8.2)\n",
            "Requirement already satisfied: wcwidth==0.2.5 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 383)) (0.2.5)\n",
            "Requirement already satisfied: webencodings==0.5.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 384)) (0.5.1)\n",
            "Requirement already satisfied: Werkzeug==1.0.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 385)) (1.0.1)\n",
            "Requirement already satisfied: widgetsnbextension==3.5.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 386)) (3.5.1)\n",
            "Requirement already satisfied: wordcloud==1.5.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 387)) (1.5.0)\n",
            "Requirement already satisfied: wrapt==1.12.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 388)) (1.12.1)\n",
            "Requirement already satisfied: xarray==0.18.2 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 389)) (0.18.2)\n",
            "Requirement already satisfied: xgboost==0.90 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 390)) (0.90)\n",
            "Requirement already satisfied: xkit==0.0.0 in /usr/lib/python3/dist-packages (from -r /content/requirements_FineTuning.txt (line 391)) (0.0.0)\n",
            "Requirement already satisfied: xlrd==1.1.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 392)) (1.1.0)\n",
            "Requirement already satisfied: xlwt==1.3.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 393)) (1.3.0)\n",
            "Requirement already satisfied: yellowbrick==0.9.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 394)) (0.9.1)\n",
            "Requirement already satisfied: zict==2.0.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 395)) (2.0.0)\n",
            "Requirement already satisfied: zipp==3.4.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/requirements_FineTuning.txt (line 396)) (3.4.1)\n",
            "Requirement already satisfied: setuptools>=38.4 in /usr/local/lib/python3.7/dist-packages (from arviz==0.11.2->-r /content/requirements_FineTuning.txt (line 7)) (57.0.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse==1.6.3->-r /content/requirements_FineTuning.txt (line 10)) (0.36.2)\n",
            "Installing collected packages: huggingface-hub, mesh-tensorflow, portalocker, rouge-score, sacrebleu, sacremoses, sentencepiece, tensorflow-text, tfds-nightly, seqio, tokenizers, transformers, t5\n",
            "Successfully installed huggingface-hub-0.0.8 mesh-tensorflow-0.1.19 portalocker-2.0.0 rouge-score-0.0.4 sacrebleu-1.5.1 sacremoses-0.0.45 sentencepiece-0.1.95 seqio-0.0.5 t5-0.9.1 tensorflow-text-2.5.0 tfds-nightly-4.3.0.dev202106070106 tokenizers-0.10.3 transformers-4.6.1\n",
            "Running on TPU: grpc://10.43.199.66:8470\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAo86QxiwX9F"
      },
      "source": [
        "We specify the paths and the sizes of all our datasets to later build our tasks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IC03zV3sy8oT"
      },
      "source": [
        "## tasks large dataset\n",
        "nq_tsv_path_code_code_large = {\n",
        "    \"train\":      'gs://' + bucket_name + '/replication_package/dataset/fine-tuning/large/code_code/train.tsv',\n",
        "    \"validation\": 'gs://' + bucket_name + '/replication_package/dataset/fine-tuning/large/code_code/val.tsv'\n",
        "}\n",
        "num_nq_examples_code_code_large = dict(train=134442, validation=16805)\n",
        "\n",
        "nq_tsv_path_code_comment_large = {\n",
        "    \"train\":      'gs://' + bucket_name + '/replication_package/dataset/fine-tuning/large/code_comment/train.tsv',\n",
        "    \"validation\": 'gs://' + bucket_name + '/replication_package/dataset/fine-tuning/large/code_comment/val.tsv'\n",
        "}\n",
        "num_nq_examples_code_comment_large = dict(train=134442, validation=16805)\n",
        "\n",
        "nq_tsv_path_codeANDcomment_code_large = {\n",
        "    \"train\":      'gs://' + bucket_name + '/replication_package/dataset/fine-tuning/large/codeANDcomment_code/train.tsv',\n",
        "    \"validation\": 'gs://' + bucket_name + '/replication_package/dataset/fine-tuning/large/codeANDcomment_code/val.tsv'\n",
        "}\n",
        "num_nq_examples_codeANDcomment_code_large = dict(train=134442, validation=16805)\n",
        "\n",
        "nq_tsv_path_marked_code_large = {\n",
        "    \"train\":      'gs://' + bucket_name + '/replication_package/dataset/fine-tuning/large/marked_code/train.tsv',\n",
        "    \"validation\": 'gs://' + bucket_name + '/replication_package/dataset/fine-tuning/large/marked_code/val.tsv'\n",
        "}\n",
        "num_nq_examples_marked_code_large = dict(train=134442, validation=16805)\n",
        "\n",
        "\n",
        "## tasks small dataset v1\n",
        "\n",
        "nq_tsv_path_code_code_small_v1 = {\n",
        "    \"train\":      'gs://' + bucket_name + '/replication_package/dataset/fine-tuning/small/v1/code_code/train.tsv',\n",
        "    \"validation\": 'gs://' + bucket_name + '/replication_package/dataset/fine-tuning/small/v1/code_code/val.tsv'\n",
        "}\n",
        "num_nq_examples_code_code_small_v1 = dict(train=13671, validation=1714)\n",
        "\n",
        "nq_tsv_path_codeANDcomment_code_small_v1 = {\n",
        "    \"train\":      'gs://' + bucket_name + '/replication_package/dataset/fine-tuning/small/v1/codeANDcomment_code/train.tsv',\n",
        "    \"validation\": 'gs://' + bucket_name + '/replication_package/dataset/fine-tuning/small/v1/codeANDcomment_code/val.tsv'\n",
        "}\n",
        "num_nq_examples_codeANDcomment_code_small_v1 = dict(train=13671, validation=1714)\n",
        "\n",
        "## tasks small dataset v2\n",
        "nq_tsv_path_code_code_small_v2 = {\n",
        "    \"train\":      'gs://' + bucket_name + '/replication_package/dataset/fine-tuning/small/v2/code_code/train.tsv',\n",
        "    \"validation\": 'gs://' + bucket_name + '/replication_package/dataset/fine-tuning/small/v2/code_code/val.tsv'\n",
        "}\n",
        "num_nq_examples_code_code_small_v2 = dict(train=13671, validation=1714)\n",
        "\n",
        "nq_tsv_path_codeANDcomment_code_small_v2 = {\n",
        "    \"train\":      'gs://' + bucket_name + '/replication_package/dataset/fine-tuning/small/v2/codeANDcomment_code/train.tsv',\n",
        "    \"validation\": 'gs://' + bucket_name + '/replication_package/dataset/fine-tuning/small/v2/codeANDcomment_code/val.tsv'\n",
        "}\n",
        "num_nq_examples_codeANDcomment_code_small_v2 = dict(train=13671, validation=1714)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBVqrfNF4EoQ"
      },
      "source": [
        "We specify the model and vocab path of the previusly trained sentencepiece model in the GCS bucket"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_twenV5ZwhPf"
      },
      "source": [
        "from t5.data import postprocessors as t5_postprocessors\n",
        "from t5.seqio import Feature,SentencePieceVocabulary\n",
        "\n",
        "# # Set the path of sentencepiece model and vocab files\n",
        "# # Must be the same used for the pre-trained phase\n",
        "\n",
        "vocab_model_path = 'gs://' + bucket_name + '/replication_package/code_review_model/TestModel.model'\n",
        "vocab_path = 'gs://' + bucket_name + '/replication_package/code_review_model/TestModel.vocab'\n",
        "\n",
        "TaskRegistry = t5.data.TaskRegistry\n",
        "TfdsTask = t5.data.TfdsTask\n",
        "\n",
        "def get_default_vocabulary():\n",
        "  return SentencePieceVocabulary(vocab_model_path, 100)\n",
        "\n",
        "DEFAULT_OUTPUT_FEATURES = {\n",
        "    \"inputs\": Feature(\n",
        "        vocabulary=get_default_vocabulary(), add_eos=True, required=False),\n",
        "\n",
        "    \"targets\": Feature(\n",
        "        vocabulary=get_default_vocabulary(), add_eos=True)\n",
        "}"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDvropGqwja9"
      },
      "source": [
        "# Setting up all the tasks\n",
        "\n",
        "We will set 8 tasks\n",
        "- code prediction (large dataset)\n",
        "- code prediction (small dataset v1)\n",
        "- code prediction (small dataset v2)\n",
        "- comment implementation (large dataset)\n",
        "- comment implementation (small dataset v1)\n",
        "- comment implementation (small dataset v2)\n",
        "- code prediction, given marked code (large dataset)\n",
        "- comment prediction (large dataset)\n",
        "\n",
        "then we will later chose which one or which mixture to tune\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8zPvNrpypq7"
      },
      "source": [
        "## FIRST TASK : CODE to CODE large_dataset\n",
        "- task name = `code_code`\n",
        "- task prefix = `code2code: `"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddHRgYezy8uE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "777d6670-001a-4254-8553-ee27dae65633"
      },
      "source": [
        "def nq_dataset_code_code_large(split, shuffle_files=True):\n",
        "  # We only have one file for each split.\n",
        "  del shuffle_files\n",
        "\n",
        "  # Load lines from the text file as examples.\n",
        "  ds = tf.data.TextLineDataset(nq_tsv_path_code_code_large[split])\n",
        "  ds = ds.map(\n",
        "      functools.partial(tf.io.decode_csv, record_defaults=[\"string\",\"string\"],\n",
        "                        field_delim=\"\\t\", use_quote_delim=False),\n",
        "      num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "  \n",
        "  ds = ds.map(lambda *ex: dict(zip([\"input\", \"output\"], ex)))\n",
        "  return ds\n",
        "\n",
        "print(\"A few raw validation examples...\")\n",
        "for ex in tfds.as_numpy(nq_dataset_code_code_large(\"validation\").take(2)):\n",
        "  print(ex)\n",
        "print(\"A few raw training examples...\")\n",
        "for ex in tfds.as_numpy(nq_dataset_code_code_large(\"train\").take(2)):\n",
        "  print(ex)\n",
        "\n",
        "def code_code_preprocessing(ds):\n",
        "  def to_inputs_and_targets(ex):\n",
        "        inputs = tf.strings.join(['code2code: ' + ex['input']], separator=' ')\n",
        "        class_label = tf.strings.join([ex['output']], separator=' ')\n",
        "        return {'inputs': inputs, 'targets': class_label }\n",
        "    \n",
        "  return ds.map(to_inputs_and_targets, \n",
        "                num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "  \n",
        "t5.data.TaskRegistry.remove('code_code')\n",
        "t5.data.TaskRegistry.add(\n",
        "    \"code_code\",\n",
        "    dataset_fn=nq_dataset_code_code_large,\n",
        "    splits=[\"train\", \"validation\"],\n",
        "    text_preprocessor=[code_code_preprocessing],\n",
        "    output_features = DEFAULT_OUTPUT_FEATURES,\n",
        "    metric_fns=[t5.evaluation.metrics.accuracy],\n",
        "    num_input_examples=num_nq_examples_code_code_large\n",
        ")\n",
        "\n",
        "nq_task = t5.data.TaskRegistry.get(\"code_code\")\n",
        "ds = nq_task.get_dataset(split=\"train\", sequence_length={\"inputs\": 512, \"targets\": 512})\n",
        "# print(\"A few preprocessed training examples...\")\n",
        "# for ex in tfds.as_numpy(ds.take(3)):\n",
        "#   print(ex)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A few raw validation examples...\n",
            "{'input': b'public void execute() throws BuildException { String generatedPassword=\"\"; if (addproperty == null || addproperty.equals(\"\")) { throw new BuildException(\"\\\\tThe output property is required for this task.\"); } if (password == null || password.equals(\"\")) { throw new BuildException(\"\\\\tThe password property is required for this task.\"); } try { MessageDigest md = MessageDigest.getInstance(\"SHA-256\"); md.update(password.getBytes()); byte[] bytes = md.digest(); generatedPassword = new String(Base64.encodeBase64(bytes)); } catch (NoSuchAlgorithmException e) { throw new BuildException(\"\\\\tThere is a problem encrypting the password with MD5 algorithm\"); } if (addproperty != null && !addproperty.equals(\"\")) { getProject().setProperty(addproperty, generatedPassword); } }', 'output': b'public void execute() throws BuildException { String generatedPassword=\"\"; if (addproperty == null || addproperty.equals(\"\")) { throw new BuildException(\"\\\\tThe output property is required for this task.\"); } if (password == null || password.equals(\"\")) { throw new BuildException(\"\\\\tThe password property is required for this task.\"); } generatedPassword = DigestUtils.sha256Hex(password); if (addproperty != null && !addproperty.equals(\"\")) { getProject().setProperty(addproperty, generatedPassword); } }'}\n",
            "{'input': b'protected VdcActionParametersBase getParametersForTask(VdcActionType actionType, VdcActionParametersBase parameters) { VdcActionType parentCommandType = getParentCommand(actionType); VdcActionParametersBase parentParameters = null; if (parentHasCallback()) { if (getTaskType() == AsyncTaskType.notSupported) { parentParameters = getParameters().getParentParameters(); } } else { parentParameters = getParameters().getParentParameters(); } if (parentCommandType == VdcActionType.Unknown || parentParameters == null) { return parameters; } parentParameters.setExecutionReason(parameters.getExecutionReason()); parentParameters.setCommandType(parentCommandType); return parentParameters; }', 'output': b'protected VdcActionParametersBase getParametersForTask(VdcActionType actionType, VdcActionParametersBase parameters) { VdcActionType parentCommandType = getParentCommandType(actionType); VdcActionParametersBase parentParameters = getParentParameters(); if (parentCommandType == VdcActionType.Unknown || parentParameters == null) { return parameters; } parentParameters.setExecutionReason(parameters.getExecutionReason()); parentParameters.setCommandType(parentCommandType); return parentParameters; }'}\n",
            "A few raw training examples...\n",
            "{'input': b'public static Region parseRegion(String regionString) { Region region = null; if (regionString != null && !regionString.equals(\"\")) { if (regionString.indexOf(\\':\\') != -1) { String[] fields = regionString.split(\"[:-]\", -1); if (fields.length == 3) { region = new Region(fields[0], Integer.parseInt(fields[1]), Integer.parseInt(fields[2])); } else if (fields.length == 2) { region = new Region(fields[0], Integer.parseInt(fields[1]), Integer.MAX_VALUE); } } else { region = new Region(regionString, 0, Integer.MAX_VALUE); } } return region; }', 'output': b'public static Region parseRegion(String regionString) { Region region = null; if (regionString != null && !regionString.equals(\"\")) { if (regionString.indexOf(\\':\\') != -1) { String[] fields = regionString.split(\"[:-]\", -1); if (fields.length == 3) { region = new Region(fields[0], Integer.parseInt(fields[1]), Integer.parseInt(fields[2])); } else if (fields.length == 2) { region = new Region(fields[0], Integer.parseInt(fields[1])); } } else { region = new Region(regionString); } } return region; }'}\n",
            "{'input': b'public VdcActionParametersBase getParameters(VM incoming, org.ovirt.engine.core.common.businessentities.VM entity) { VmStatic updated = getMapper(modelType, VmStatic.class).map(incoming, entity.getStaticData()); updated.setUsbPolicy(VmMapper.getUsbPolicyOnUpdate(incoming.getUsb(), entity.getUsbPolicy(), lookupCluster(updated.getVdsGroupId()))); VmManagementParametersBase params = new VmManagementParametersBase(updated); if (incoming.isSetPayloads()) { if (incoming.isSetPayloads() && incoming.getPayloads().isSetPayload()) { params.setVmPayload(parent.getPayload(incoming)); } else { params.setClearPayload(true); } } if (incoming.isSetMemoryPolicy() && incoming.getMemoryPolicy().isSetBallooning()) { params.setBalloonEnabled(incoming.getMemoryPolicy().isBallooning()); } if (incoming.isSetConsoleDeviceEnabled()) { params.setConsoleEnabled(incoming.isConsoleDeviceEnabled()); } return params; }', 'output': b'public VdcActionParametersBase getParameters(VM incoming, org.ovirt.engine.core.common.businessentities.VM entity) { VmStatic updated = getMapper(modelType, VmStatic.class).map(incoming, entity.getStaticData()); updated.setUsbPolicy(VmMapper.getUsbPolicyOnUpdate(incoming.getUsb(), entity.getUsbPolicy(), lookupCluster(updated.getVdsGroupId()))); VmManagementParametersBase params = new VmManagementParametersBase(updated); if (incoming.isSetPayloads()) { if (incoming.isSetPayloads() && incoming.getPayloads().isSetPayload()) { params.setVmPayload(parent.getPayload(incoming)); } else { params.setClearPayload(true); } } if (incoming.isSetMemoryPolicy() && incoming.getMemoryPolicy().isSetBallooning()) { params.setBalloonEnabled(incoming.getMemoryPolicy().isBallooning()); } if (incoming.isSetConsole() && incoming.getConsole().isSetEnabled()) { params.setConsoleEnabled(incoming.getConsole().isEnabled()); } return params; }'}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seqio/preprocessors.py:90: UserWarning: Creating resources inside a function passed to Dataset.map() is not supported. Create each resource outside the function, and capture it inside the function to use it.\n",
            "  _tokenize, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32bONo87yxhA"
      },
      "source": [
        "## SECOND TASK : CODE to COMMENT large_dataset\n",
        "- task name = `code_comment`\n",
        "- task prefix = `code2comment: `"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p68qZUBTxWiF",
        "outputId": "58ba9236-aedc-4ebb-9687-b65c1467591f"
      },
      "source": [
        "def nq_dataset_code_comment_large(split, shuffle_files=False):\n",
        "  # We only have one file for each split.\n",
        "  del shuffle_files\n",
        "\n",
        "  # Load lines from the text file as examples.\n",
        "  ds = tf.data.TextLineDataset(nq_tsv_path_code_comment_large[split])\n",
        "  ds = ds.map(\n",
        "      functools.partial(tf.io.decode_csv, record_defaults=[\"string\",\"string\"],\n",
        "                        field_delim=\"\\t\", use_quote_delim=False),\n",
        "      num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "  \n",
        "  ds = ds.map(lambda *ex: dict(zip([\"input\", \"output\"], ex)))\n",
        "  return ds\n",
        "\n",
        "print(\"A few raw validation examples...\")\n",
        "for ex in tfds.as_numpy(nq_dataset_code_comment_large(\"validation\").take(2)):\n",
        "  print(ex)\n",
        "print(\"A few raw training examples...\")\n",
        "for ex in tfds.as_numpy(nq_dataset_code_comment_large(\"train\").take(2)):\n",
        "  print(ex)\n",
        "\n",
        "def code_comment_preprocessing(ds):\n",
        "  def to_inputs_and_targets(ex):\n",
        "\n",
        "        inputs = tf.strings.join(['code2comment: ' + ex['input']], separator=' ')\n",
        "        class_label = tf.strings.join([ex['output']], separator=' ')\n",
        "        return {'inputs': inputs, 'targets': class_label }\n",
        "    \n",
        "  return ds.map(to_inputs_and_targets, \n",
        "                num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "#Create a new training task\n",
        "t5.data.TaskRegistry.remove('code_comment')\n",
        "t5.data.TaskRegistry.add(\n",
        "    \"code_comment\",\n",
        "    dataset_fn=nq_dataset_code_comment_large,\n",
        "    splits=[\"train\", \"validation\"],\n",
        "    text_preprocessor=[code_comment_preprocessing],\n",
        "    output_features = DEFAULT_OUTPUT_FEATURES,\n",
        "    metric_fns=[t5.evaluation.metrics.accuracy],\n",
        "    num_input_examples=num_nq_examples_code_comment_large\n",
        ")\n",
        "\n",
        "nq_task = t5.data.TaskRegistry.get(\"code_comment\")\n",
        "ds = nq_task.get_dataset(split=\"train\", sequence_length={\"inputs\": 512, \"targets\": 512})\n",
        "# print(\"A few preprocessed training examples...\")\n",
        "# for ex in tfds.as_numpy(ds.take(3)):\n",
        "#   print(ex)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A few raw validation examples...\n",
            "{'input': b'public void execute() throws BuildException { String generatedPassword=\"\"; if (addproperty == null || addproperty.equals(\"\")) { throw new BuildException(\"\\\\tThe output property is required for this task.\"); } if (password == null || password.equals(\"\")) { throw new BuildException(\"\\\\tThe password property is required for this task.\"); } try { MessageDigest md = MessageDigest.getInstance(\"SHA-256\"); md.update(password.getBytes()); byte[] bytes = md.digest(); generatedPassword = new String(Base64.encodeBase64(bytes)); } catch (NoSuchAlgorithmException e) { throw new BuildException(\"\\\\tThere is a problem encrypting the password with MD5 algorithm\"); } if (addproperty != null && !addproperty.equals(\"\")) { getProject().setProperty(addproperty, generatedPassword); } }', 'output': b\"If we're going to include commons-codec as a dependency, then I think you should just replace the whole block of code above with this:  generatedPassword = DigestUtils.sha256Hex(password);\"}\n",
            "{'input': b'protected VdcActionParametersBase getParametersForTask(VdcActionType actionType, VdcActionParametersBase parameters) { VdcActionType parentCommandType = getParentCommand(actionType); VdcActionParametersBase parentParameters = null; if (parentHasCallback()) { if (getTaskType() == AsyncTaskType.notSupported) { parentParameters = getParameters().getParentParameters(); } } else { parentParameters = getParameters().getParentParameters(); } if (parentCommandType == VdcActionType.Unknown || parentParameters == null) { return parameters; } parentParameters.setExecutionReason(parameters.getExecutionReason()); parentParameters.setCommandType(parentCommandType); return parentParameters; }', 'output': b\"couldn't this be replaced with getParentCommand() ?\"}\n",
            "A few raw training examples...\n",
            "{'input': b'public static Region parseRegion(String regionString) { Region region = null; if (regionString != null && !regionString.equals(\"\")) { if (regionString.indexOf(\\':\\') != -1) { String[] fields = regionString.split(\"[:-]\", -1); if (fields.length == 3) { region = new Region(fields[0], Integer.parseInt(fields[1]), Integer.parseInt(fields[2])); } else if (fields.length == 2) { region = new Region(fields[0], Integer.parseInt(fields[1]), Integer.MAX_VALUE); } } else { region = new Region(regionString, 0, Integer.MAX_VALUE); } } return region; }', 'output': b'Could use the two arg constructor, that way it centralises the default values. e.g.: region = new Region(fields[0], Integer.parseInt(fields[1]));'}\n",
            "{'input': b'public VdcActionParametersBase getParameters(VM incoming, org.ovirt.engine.core.common.businessentities.VM entity) { VmStatic updated = getMapper(modelType, VmStatic.class).map(incoming, entity.getStaticData()); updated.setUsbPolicy(VmMapper.getUsbPolicyOnUpdate(incoming.getUsb(), entity.getUsbPolicy(), lookupCluster(updated.getVdsGroupId()))); VmManagementParametersBase params = new VmManagementParametersBase(updated); if (incoming.isSetPayloads()) { if (incoming.isSetPayloads() && incoming.getPayloads().isSetPayload()) { params.setVmPayload(parent.getPayload(incoming)); } else { params.setClearPayload(true); } } if (incoming.isSetMemoryPolicy() && incoming.getMemoryPolicy().isSetBallooning()) { params.setBalloonEnabled(incoming.getMemoryPolicy().isBallooning()); } if (incoming.isSetConsoleDeviceEnabled()) { params.setConsoleEnabled(incoming.isConsoleDeviceEnabled()); } return params; }', 'output': b'this should be in the doPopulate() not in deprecatedPopulate()'}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seqio/preprocessors.py:90: UserWarning: Creating resources inside a function passed to Dataset.map() is not supported. Create each resource outside the function, and capture it inside the function to use it.\n",
            "  _tokenize, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wrG3YHXy2qp"
      },
      "source": [
        "## THIRD TASK : CODE and COMMENT to CODE large_dataset\n",
        "- task name = `codeANDcomment_code`\n",
        "- task prefix = `code&comment2code: `"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c_kJneaJxWmm",
        "outputId": "7d2d0de7-405d-4882-9f69-9b235756a1f5"
      },
      "source": [
        "############### THIRD TASK : CODE&COMMENT2CODE ###############\n",
        "\n",
        "def nq_dataset_codeANDcomment_code_large(split, shuffle_files=False):\n",
        "  # We only have one file for each split.\n",
        "  del shuffle_files\n",
        "\n",
        "  # Load lines from the text file as examples.\n",
        "  ds = tf.data.TextLineDataset(nq_tsv_path_codeANDcomment_code_large[split])\n",
        "  ds = ds.map(\n",
        "      functools.partial(tf.io.decode_csv, record_defaults=[\"string\",\"string\"],\n",
        "                        field_delim=\"\\t\", use_quote_delim=False),\n",
        "      num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "  \n",
        "  ds = ds.map(lambda *ex: dict(zip([\"input\", \"output\"], ex)))\n",
        "  return ds\n",
        "\n",
        "print(\"A few raw validation examples...\")\n",
        "for ex in tfds.as_numpy(nq_dataset_codeANDcomment_code_large(\"validation\").take(2)):\n",
        "  print(ex)\n",
        "print(\"A few raw training examples...\")\n",
        "for ex in tfds.as_numpy(nq_dataset_codeANDcomment_code_large(\"train\").take(2)):\n",
        "  print(ex)\n",
        "\n",
        "def codeANDcomment_code_preprocessing(ds):\n",
        "  \n",
        "  def to_inputs_and_targets(ex):\n",
        "\n",
        "        inputs = tf.strings.join(['code&comment2code: ' + ex['input']], separator=' ')\n",
        "        class_label = tf.strings.join([ex['output']], separator=' ')\n",
        "        return {'inputs': inputs, 'targets': class_label }\n",
        "    \n",
        "  return ds.map(to_inputs_and_targets, \n",
        "                num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "#Create a new training task\n",
        "t5.data.TaskRegistry.remove('codeANDcomment_code')\n",
        "t5.data.TaskRegistry.add(\n",
        "    \"codeANDcomment_code\",\n",
        "    dataset_fn=nq_dataset_codeANDcomment_code_large,\n",
        "    splits=[\"train\", \"validation\"],\n",
        "    text_preprocessor=[codeANDcomment_code_preprocessing],\n",
        "    output_features = DEFAULT_OUTPUT_FEATURES,\n",
        "    metric_fns=[t5.evaluation.metrics.accuracy],\n",
        "    num_input_examples=num_nq_examples_codeANDcomment_code_large\n",
        ")\n",
        "\n",
        "nq_task = t5.data.TaskRegistry.get(\"codeANDcomment_code\")\n",
        "ds = nq_task.get_dataset(split=\"train\", sequence_length={\"inputs\": 512, \"targets\": 512})\n",
        "# print(\"A few preprocessed training examples...\")\n",
        "# for ex in tfds.as_numpy(ds.take(3)):\n",
        "#   print(ex)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A few raw validation examples...\n",
            "{'input': b'<code>public void execute() throws BuildException { String generatedPassword=\"\"; if (addproperty == null || addproperty.equals(\"\")) { throw new BuildException(\"\\\\tThe output property is required for this task.\"); } if (password == null || password.equals(\"\")) { throw new BuildException(\"\\\\tThe password property is required for this task.\"); } try { MessageDigest md = MessageDigest.getInstance(\"SHA-256\"); md.update(password.getBytes()); byte[] bytes = md.digest(); <START> generatedPassword = new String(Base64.encodeBase64(bytes)); <END> } catch (NoSuchAlgorithmException e) { throw new BuildException(\"\\\\tThere is a problem encrypting the password with MD5 algorithm\"); } if (addproperty != null && !addproperty.equals(\"\")) { getProject().setProperty(addproperty, generatedPassword); } }</code><technical_language>If include commons-codec a dependency, I replace block of code this: generatedPassword = DigestUtils.sha256Hex(password);</technical_language>', 'output': b'public void execute() throws BuildException { String generatedPassword=\"\"; if (addproperty == null || addproperty.equals(\"\")) { throw new BuildException(\"\\\\tThe output property is required for this task.\"); } if (password == null || password.equals(\"\")) { throw new BuildException(\"\\\\tThe password property is required for this task.\"); } generatedPassword = DigestUtils.sha256Hex(password); if (addproperty != null && !addproperty.equals(\"\")) { getProject().setProperty(addproperty, generatedPassword); } }'}\n",
            "{'input': b'<code>protected VdcActionParametersBase getParametersForTask(VdcActionType actionType, VdcActionParametersBase parameters) { VdcActionType parentCommandType = getParentCommand(actionType); VdcActionParametersBase parentParameters = null; <START> if (parentHasCallback()) { if (getTaskType() == AsyncTaskType.notSupported) { parentParameters = getParameters().getParentParameters(); } } else { parentParameters = getParameters().getParentParameters(); } <END> if (parentCommandType == VdcActionType.Unknown || parentParameters == null) { return parameters; } parentParameters.setExecutionReason(parameters.getExecutionReason()); parentParameters.setCommandType(parentCommandType); return parentParameters; }</code><technical_language>this replaced getParentCommand()</technical_language>', 'output': b'protected VdcActionParametersBase getParametersForTask(VdcActionType actionType, VdcActionParametersBase parameters) { VdcActionType parentCommandType = getParentCommandType(actionType); VdcActionParametersBase parentParameters = getParentParameters(); if (parentCommandType == VdcActionType.Unknown || parentParameters == null) { return parameters; } parentParameters.setExecutionReason(parameters.getExecutionReason()); parentParameters.setCommandType(parentCommandType); return parentParameters; }'}\n",
            "A few raw training examples...\n",
            "{'input': b'<code>public static Region parseRegion(String regionString) { Region region = null; if (regionString != null && !regionString.equals(\"\")) { if (regionString.indexOf(\\':\\') != -1) { String[] fields = regionString.split(\"[:-]\", -1); if (fields.length == 3) { region = new Region(fields[0], Integer.parseInt(fields[1]), Integer.parseInt(fields[2])); } else if (fields.length == 2) { <START> region = new Region(fields[0], Integer.parseInt(fields[1]), Integer.MAX_VALUE); <END> } } else { region = new Region(regionString, 0, Integer.MAX_VALUE); } } return region; }</code><technical_language>arg constructor, centralises default values. e.g.: region = new Region(fields[0], Integer.parseInt(fields[1]));</technical_language>', 'output': b'public static Region parseRegion(String regionString) { Region region = null; if (regionString != null && !regionString.equals(\"\")) { if (regionString.indexOf(\\':\\') != -1) { String[] fields = regionString.split(\"[:-]\", -1); if (fields.length == 3) { region = new Region(fields[0], Integer.parseInt(fields[1]), Integer.parseInt(fields[2])); } else if (fields.length == 2) { region = new Region(fields[0], Integer.parseInt(fields[1])); } } else { region = new Region(regionString); } } return region; }'}\n",
            "{'input': b'<code>public VdcActionParametersBase getParameters(VM incoming, org.ovirt.engine.core.common.businessentities.VM entity) { VmStatic updated = getMapper(modelType, VmStatic.class).map(incoming, entity.getStaticData()); updated.setUsbPolicy(VmMapper.getUsbPolicyOnUpdate(incoming.getUsb(), entity.getUsbPolicy(), lookupCluster(updated.getVdsGroupId()))); VmManagementParametersBase params = new VmManagementParametersBase(updated); if (incoming.isSetPayloads()) { if (incoming.isSetPayloads() && incoming.getPayloads().isSetPayload()) { params.setVmPayload(parent.getPayload(incoming)); } else { params.setClearPayload(true); } } if (incoming.isSetMemoryPolicy() && incoming.getMemoryPolicy().isSetBallooning()) { params.setBalloonEnabled(incoming.getMemoryPolicy().isBallooning()); } <START> if (incoming.isSetConsoleDeviceEnabled()) { <END> params.setConsoleEnabled(incoming.isConsoleDeviceEnabled()); } return params; }</code><technical_language>this in doPopulate() in deprecatedPopulate()</technical_language>', 'output': b'public VdcActionParametersBase getParameters(VM incoming, org.ovirt.engine.core.common.businessentities.VM entity) { VmStatic updated = getMapper(modelType, VmStatic.class).map(incoming, entity.getStaticData()); updated.setUsbPolicy(VmMapper.getUsbPolicyOnUpdate(incoming.getUsb(), entity.getUsbPolicy(), lookupCluster(updated.getVdsGroupId()))); VmManagementParametersBase params = new VmManagementParametersBase(updated); if (incoming.isSetPayloads()) { if (incoming.isSetPayloads() && incoming.getPayloads().isSetPayload()) { params.setVmPayload(parent.getPayload(incoming)); } else { params.setClearPayload(true); } } if (incoming.isSetMemoryPolicy() && incoming.getMemoryPolicy().isSetBallooning()) { params.setBalloonEnabled(incoming.getMemoryPolicy().isBallooning()); } if (incoming.isSetConsole() && incoming.getConsole().isSetEnabled()) { params.setConsoleEnabled(incoming.getConsole().isEnabled()); } return params; }'}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seqio/preprocessors.py:90: UserWarning: Creating resources inside a function passed to Dataset.map() is not supported. Create each resource outside the function, and capture it inside the function to use it.\n",
            "  _tokenize, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4_enLxdzI8L"
      },
      "source": [
        "## FOURTH TASK : MARKED CODE to CODE large_dataset\n",
        "- task name = `marked_code`\n",
        "- task prefix = `markedCode2code: `"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YnC3Lw3OxWpY",
        "outputId": "0d4892f8-cf90-4ce8-8f3b-b9f02cd022a6"
      },
      "source": [
        "def nq_dataset_marked_code_large(split, shuffle_files=False):\n",
        "  # We only have one file for each split.\n",
        "  del shuffle_files\n",
        "\n",
        "  # Load lines from the text file as examples.\n",
        "  ds = tf.data.TextLineDataset(nq_tsv_path_marked_code_large[split])\n",
        "  ds = ds.map(\n",
        "      functools.partial(tf.io.decode_csv, record_defaults=[\"string\",\"string\"],\n",
        "                        field_delim=\"\\t\", use_quote_delim=False),\n",
        "      num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "  \n",
        "  ds = ds.map(lambda *ex: dict(zip([\"input\", \"output\"], ex)))\n",
        "  return ds\n",
        "\n",
        "print(\"A few raw validation examples...\")\n",
        "for ex in tfds.as_numpy(nq_dataset_marked_code_large(\"validation\").take(2)):\n",
        "  print(ex)\n",
        "print(\"A few raw training examples...\")\n",
        "for ex in tfds.as_numpy(nq_dataset_marked_code_large(\"train\").take(2)):\n",
        "  print(ex)\n",
        "\n",
        "def marked_code_preprocessing(ds):\n",
        "  \n",
        "  def to_inputs_and_targets(ex):\n",
        "\n",
        "        inputs = tf.strings.join(['markedCode2code: ' + ex['input']], separator=' ')\n",
        "        class_label = tf.strings.join([ex['output']], separator=' ')\n",
        "        return {'inputs': inputs, 'targets': class_label }\n",
        "    \n",
        "  return ds.map(to_inputs_and_targets, \n",
        "                num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "#Create a new training task\n",
        "t5.data.TaskRegistry.remove('marked_code')\n",
        "t5.data.TaskRegistry.add(\n",
        "    \"marked_code\",\n",
        "    dataset_fn=nq_dataset_marked_code_large,\n",
        "    splits=[\"train\", \"validation\"],\n",
        "    text_preprocessor=[marked_code_preprocessing],\n",
        "    output_features = DEFAULT_OUTPUT_FEATURES,\n",
        "    metric_fns=[t5.evaluation.metrics.accuracy],\n",
        "    num_input_examples=num_nq_examples_marked_code_large\n",
        ")\n",
        "\n",
        "nq_task = t5.data.TaskRegistry.get(\"marked_code\")\n",
        "ds = nq_task.get_dataset(split=\"train\", sequence_length={\"inputs\": 512, \"targets\": 512})\n",
        "# print(\"A few preprocessed training examples...\")\n",
        "# for ex in tfds.as_numpy(ds.take(3)):\n",
        "#   print(ex)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A few raw validation examples...\n",
            "{'input': b'public void execute() throws BuildException { String generatedPassword=\"\"; if (addproperty == null || addproperty.equals(\"\")) { throw new BuildException(\"\\\\tThe output property is required for this task.\"); } if (password == null || password.equals(\"\")) { throw new BuildException(\"\\\\tThe password property is required for this task.\"); } try { MessageDigest md = MessageDigest.getInstance(\"SHA-256\"); md.update(password.getBytes()); byte[] bytes = md.digest(); <START> generatedPassword = new String(Base64.encodeBase64(bytes)); <END> } catch (NoSuchAlgorithmException e) { throw new BuildException(\"\\\\tThere is a problem encrypting the password with MD5 algorithm\"); } if (addproperty != null && !addproperty.equals(\"\")) { getProject().setProperty(addproperty, generatedPassword); } }', 'output': b'public void execute() throws BuildException { String generatedPassword=\"\"; if (addproperty == null || addproperty.equals(\"\")) { throw new BuildException(\"\\\\tThe output property is required for this task.\"); } if (password == null || password.equals(\"\")) { throw new BuildException(\"\\\\tThe password property is required for this task.\"); } generatedPassword = DigestUtils.sha256Hex(password); if (addproperty != null && !addproperty.equals(\"\")) { getProject().setProperty(addproperty, generatedPassword); } }'}\n",
            "{'input': b'protected VdcActionParametersBase getParametersForTask(VdcActionType actionType, VdcActionParametersBase parameters) { VdcActionType parentCommandType = getParentCommand(actionType); VdcActionParametersBase parentParameters = null; <START> if (parentHasCallback()) { if (getTaskType() == AsyncTaskType.notSupported) { parentParameters = getParameters().getParentParameters(); } } else { parentParameters = getParameters().getParentParameters(); } <END> if (parentCommandType == VdcActionType.Unknown || parentParameters == null) { return parameters; } parentParameters.setExecutionReason(parameters.getExecutionReason()); parentParameters.setCommandType(parentCommandType); return parentParameters; }', 'output': b'protected VdcActionParametersBase getParametersForTask(VdcActionType actionType, VdcActionParametersBase parameters) { VdcActionType parentCommandType = getParentCommandType(actionType); VdcActionParametersBase parentParameters = getParentParameters(); if (parentCommandType == VdcActionType.Unknown || parentParameters == null) { return parameters; } parentParameters.setExecutionReason(parameters.getExecutionReason()); parentParameters.setCommandType(parentCommandType); return parentParameters; }'}\n",
            "A few raw training examples...\n",
            "{'input': b'public static Region parseRegion(String regionString) { Region region = null; if (regionString != null && !regionString.equals(\"\")) { if (regionString.indexOf(\\':\\') != -1) { String[] fields = regionString.split(\"[:-]\", -1); if (fields.length == 3) { region = new Region(fields[0], Integer.parseInt(fields[1]), Integer.parseInt(fields[2])); } else if (fields.length == 2) { <START> region = new Region(fields[0], Integer.parseInt(fields[1]), Integer.MAX_VALUE); <END> } } else { region = new Region(regionString, 0, Integer.MAX_VALUE); } } return region; }', 'output': b'public static Region parseRegion(String regionString) { Region region = null; if (regionString != null && !regionString.equals(\"\")) { if (regionString.indexOf(\\':\\') != -1) { String[] fields = regionString.split(\"[:-]\", -1); if (fields.length == 3) { region = new Region(fields[0], Integer.parseInt(fields[1]), Integer.parseInt(fields[2])); } else if (fields.length == 2) { region = new Region(fields[0], Integer.parseInt(fields[1])); } } else { region = new Region(regionString); } } return region; }'}\n",
            "{'input': b'public VdcActionParametersBase getParameters(VM incoming, org.ovirt.engine.core.common.businessentities.VM entity) { VmStatic updated = getMapper(modelType, VmStatic.class).map(incoming, entity.getStaticData()); updated.setUsbPolicy(VmMapper.getUsbPolicyOnUpdate(incoming.getUsb(), entity.getUsbPolicy(), lookupCluster(updated.getVdsGroupId()))); VmManagementParametersBase params = new VmManagementParametersBase(updated); if (incoming.isSetPayloads()) { if (incoming.isSetPayloads() && incoming.getPayloads().isSetPayload()) { params.setVmPayload(parent.getPayload(incoming)); } else { params.setClearPayload(true); } } if (incoming.isSetMemoryPolicy() && incoming.getMemoryPolicy().isSetBallooning()) { params.setBalloonEnabled(incoming.getMemoryPolicy().isBallooning()); } <START> if (incoming.isSetConsoleDeviceEnabled()) { <END> params.setConsoleEnabled(incoming.isConsoleDeviceEnabled()); } return params; }', 'output': b'public VdcActionParametersBase getParameters(VM incoming, org.ovirt.engine.core.common.businessentities.VM entity) { VmStatic updated = getMapper(modelType, VmStatic.class).map(incoming, entity.getStaticData()); updated.setUsbPolicy(VmMapper.getUsbPolicyOnUpdate(incoming.getUsb(), entity.getUsbPolicy(), lookupCluster(updated.getVdsGroupId()))); VmManagementParametersBase params = new VmManagementParametersBase(updated); if (incoming.isSetPayloads()) { if (incoming.isSetPayloads() && incoming.getPayloads().isSetPayload()) { params.setVmPayload(parent.getPayload(incoming)); } else { params.setClearPayload(true); } } if (incoming.isSetMemoryPolicy() && incoming.getMemoryPolicy().isSetBallooning()) { params.setBalloonEnabled(incoming.getMemoryPolicy().isBallooning()); } if (incoming.isSetConsole() && incoming.getConsole().isSetEnabled()) { params.setConsoleEnabled(incoming.getConsole().isEnabled()); } return params; }'}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seqio/preprocessors.py:90: UserWarning: Creating resources inside a function passed to Dataset.map() is not supported. Create each resource outside the function, and capture it inside the function to use it.\n",
            "  _tokenize, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVPMF378zg0X"
      },
      "source": [
        "## FIFTH TASK : CODE to CODE small_dataset_v1\n",
        "- task name = `code_code_small_v1`\n",
        "- task prefix = `code2code: `"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m43eoZAKxWry",
        "outputId": "809aa2b6-4e88-4d83-b1e3-79f46403b7fa"
      },
      "source": [
        "def nq_dataset_code_code_small_v1(split, shuffle_files=False):\n",
        "  # We only have one file for each split.\n",
        "  del shuffle_files\n",
        "\n",
        "  # Load lines from the text file as examples.\n",
        "  ds = tf.data.TextLineDataset(nq_tsv_path_code_code_small_v1[split])\n",
        "  ds = ds.map(\n",
        "      functools.partial(tf.io.decode_csv, record_defaults=[\"string\",\"string\"],\n",
        "                        field_delim=\"\\t\", use_quote_delim=False),\n",
        "      num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "  \n",
        "  ds = ds.map(lambda *ex: dict(zip([\"input\", \"output\"], ex)))\n",
        "  return ds\n",
        "\n",
        "print(\"A few raw validation examples...\")\n",
        "for ex in tfds.as_numpy(nq_dataset_code_code_small_v1(\"validation\").take(2)):\n",
        "  print(ex)\n",
        "print(\"A few raw training examples...\")\n",
        "for ex in tfds.as_numpy(nq_dataset_code_code_small_v1(\"train\").take(2)):\n",
        "  print(ex)\n",
        "\n",
        "def marked_code_preprocessing(ds):\n",
        "  \n",
        "  def to_inputs_and_targets(ex):\n",
        "\n",
        "        inputs = tf.strings.join(['code2code: ' + ex['input']], separator=' ')\n",
        "        class_label = tf.strings.join([ex['output']], separator=' ')\n",
        "        return {'inputs': inputs, 'targets': class_label }\n",
        "    \n",
        "  return ds.map(to_inputs_and_targets, \n",
        "                num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "#Create a new training task\n",
        "t5.data.TaskRegistry.remove('code_code_small_v1')\n",
        "t5.data.TaskRegistry.add(\n",
        "    \"code_code_small_v1\",\n",
        "    dataset_fn=nq_dataset_code_code_small_v1,\n",
        "    splits=[\"train\", \"validation\"],\n",
        "    text_preprocessor=[marked_code_preprocessing],\n",
        "    output_features = DEFAULT_OUTPUT_FEATURES,\n",
        "    metric_fns=[t5.evaluation.metrics.accuracy],\n",
        "    num_input_examples=num_nq_examples_codeANDcomment_code_small_v2\n",
        ")\n",
        "\n",
        "nq_task = t5.data.TaskRegistry.get(\"code_code_small_v1\")\n",
        "ds = nq_task.get_dataset(split=\"train\", sequence_length={\"inputs\": 512, \"targets\": 512})\n",
        "# print(\"A few preprocessed training examples...\")\n",
        "# for ex in tfds.as_numpy(ds.take(3)):\n",
        "#   print(ex)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A few raw validation examples...\n",
            "{'input': b'public void startRuntime() { String tempDir = AppConstants.getInstance().getString(\"log.dir\", null); v8 = V8.createV8Runtime(\"J2V8Javascript\", tempDir); }', 'output': b'public void startRuntime() { v8 = V8.createV8Runtime(); }'}\n",
            "{'input': b'public GWCConfig getConfig() { if (gsEnvironment != null && gsEnvironment.isStale()) { syncEnvironment(); } return gwcConfigPersister.getConfig(); }', 'output': b'public GWCConfig getConfig() { return gwcConfigPersister.getConfig(); }'}\n",
            "A few raw training examples...\n",
            "{'input': b'protected static String commentFormat(String comment) { if (comment == null || comment.isEmpty()) return \"\"; while (comment.getBytes(ENCODING).length > 255) { comment = comment.substring(0, comment.length() - 1); } return comment; }', 'output': b'protected static String commentFormat(String comment) { if (comment == null || comment.length() == 0) return \"\"; while (comment.getBytes(ENCODING).length > 255) { comment = comment.substring(0, comment.length() - 1); } return comment; }'}\n",
            "{'input': b'public PageTitle(final String namespace, final String text) { this.namespace = namespace; this.text = text.replaceAll(\"\\\\\\\\s+\", \"_\"); }', 'output': b'public PageTitle(final String namespace, final String text) { this.namespace = namespace; this.text = text; }'}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seqio/preprocessors.py:90: UserWarning: Creating resources inside a function passed to Dataset.map() is not supported. Create each resource outside the function, and capture it inside the function to use it.\n",
            "  _tokenize, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quC7CexKzjOM"
      },
      "source": [
        "## SIXTH TASK : CODE and COMMENT to CODE small_dataset_v1\n",
        "- task name = `codeANDcomment_code_small_v1`\n",
        "- task prefix = `code&comment2code: `"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZU9aB3cxWuP",
        "outputId": "fd3efb36-0da6-4cd6-bd3c-ea0d5f27b561"
      },
      "source": [
        "def nq_dataset_codeANDcomment_code_small_v1(split, shuffle_files=False):\n",
        "  # We only have one file for each split.\n",
        "  del shuffle_files\n",
        "\n",
        "  # Load lines from the text file as examples.\n",
        "  ds = tf.data.TextLineDataset(nq_tsv_path_codeANDcomment_code_small_v1[split])\n",
        "  ds = ds.map(\n",
        "      functools.partial(tf.io.decode_csv, record_defaults=[\"string\",\"string\"],\n",
        "                        field_delim=\"\\t\", use_quote_delim=False),\n",
        "      num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "  \n",
        "  ds = ds.map(lambda *ex: dict(zip([\"input\", \"output\"], ex)))\n",
        "  return ds\n",
        "\n",
        "print(\"A few raw validation examples...\")\n",
        "for ex in tfds.as_numpy(nq_dataset_codeANDcomment_code_small_v1(\"validation\").take(2)):\n",
        "  print(ex)\n",
        "print(\"A few raw training examples...\")\n",
        "for ex in tfds.as_numpy(nq_dataset_codeANDcomment_code_small_v1(\"train\").take(2)):\n",
        "  print(ex)\n",
        "\n",
        "def marked_code_preprocessing(ds):\n",
        "  \n",
        "  def to_inputs_and_targets(ex):\n",
        "\n",
        "        inputs = tf.strings.join(['code&comment2code: ' + ex['input']], separator=' ')\n",
        "        class_label = tf.strings.join([ex['output']], separator=' ')\n",
        "        return {'inputs': inputs, 'targets': class_label }\n",
        "    \n",
        "  return ds.map(to_inputs_and_targets, \n",
        "                num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "#Create a new training task\n",
        "t5.data.TaskRegistry.remove('codeANDcomment_code_small_v1')\n",
        "t5.data.TaskRegistry.add(\n",
        "    \"codeANDcomment_code_small_v1\",\n",
        "    dataset_fn=nq_dataset_codeANDcomment_code_small_v1,\n",
        "    splits=[\"train\", \"validation\"],\n",
        "    text_preprocessor=[marked_code_preprocessing],\n",
        "    output_features = DEFAULT_OUTPUT_FEATURES,\n",
        "    metric_fns=[t5.evaluation.metrics.accuracy],\n",
        "    num_input_examples=num_nq_examples_codeANDcomment_code_small_v1\n",
        ")\n",
        "\n",
        "nq_task = t5.data.TaskRegistry.get(\"codeANDcomment_code_small_v1\")\n",
        "ds = nq_task.get_dataset(split=\"train\", sequence_length={\"inputs\": 512, \"targets\": 512})\n",
        "# print(\"A few preprocessed training examples...\")\n",
        "# for ex in tfds.as_numpy(ds.take(3)):\n",
        "#   print(ex)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A few raw validation examples...\n",
            "{'input': b'<code> public void startRuntime() { String tempDir = AppConstants.getInstance().getString(\"log.dir\", null); <START> v8 = V8.createV8Runtime(\"J2V8Javascript\", tempDir); <END> } </code><technical_language> Setting the alias and temp directory can be quite important. Perhaps make this an optional argument? So you can overwrite the alias when needed (i.e. to improve performance). Right now all instances will be cached under the same name. </technical_language>', 'output': b'public void startRuntime() { v8 = V8.createV8Runtime(); }'}\n",
            "{'input': b'<code> public GWCConfig getConfig() { <START> if (gsEnvironment != null && gsEnvironment.isStale()) { <END> syncEnvironment(); } return gwcConfigPersister.getConfig(); } </code><technical_language> This bloc of code is repeated many times, should be centralized in a single method. </technical_language>', 'output': b'public GWCConfig getConfig() { return gwcConfigPersister.getConfig(); }'}\n",
            "A few raw training examples...\n",
            "{'input': b'<code> protected static String commentFormat(String comment) { <START> if (comment == null || comment.isEmpty()) return \"\"; <END> while (comment.getBytes(ENCODING).length > 255) { comment = comment.substring(0, comment.length() - 1); } return comment; } </code><technical_language> String.isEmpty() is avaible only as of JDK 1.6. Please use if (comment == null || comment.length() == 0) return \"\"; to keep this compilable with JDK 1.5 </technical_language>', 'output': b'protected static String commentFormat(String comment) { if (comment == null || comment.length() == 0) return \"\"; while (comment.getBytes(ENCODING).length > 255) { comment = comment.substring(0, comment.length() - 1); } return comment; }'}\n",
            "{'input': b'<code> public PageTitle(final String namespace, final String text) { this.namespace = namespace; <START> this.text = text.replaceAll(\"\\\\\\\\s+\", \"_\"); <END> } </code><technical_language> Does \\\\s include zero-width non-joiner and stuff? Oh, languages, we are such fun. :D </technical_language>', 'output': b'public PageTitle(final String namespace, final String text) { this.namespace = namespace; this.text = text; }'}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seqio/preprocessors.py:90: UserWarning: Creating resources inside a function passed to Dataset.map() is not supported. Create each resource outside the function, and capture it inside the function to use it.\n",
            "  _tokenize, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBSXtFYczkZx"
      },
      "source": [
        "## SEVENTH TASK : CODE to CODE small_dataset_v2\n",
        "- task name = `codeANDcomment_code_small_v2`\n",
        "- task prefix = `code2code: `"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tw4W8yMMxWwe",
        "outputId": "2899a31c-4897-4c02-ea6e-c06d4908e0b0"
      },
      "source": [
        "def nq_dataset_code_code_small_v2(split, shuffle_files=False):\n",
        "  # We only have one file for each split.\n",
        "  del shuffle_files\n",
        "\n",
        "  # Load lines from the text file as examples.\n",
        "  ds = tf.data.TextLineDataset(nq_tsv_path_code_code_small_v2[split])\n",
        "  ds = ds.map(\n",
        "      functools.partial(tf.io.decode_csv, record_defaults=[\"string\",\"string\"],\n",
        "                        field_delim=\"\\t\", use_quote_delim=False),\n",
        "      num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "  \n",
        "  ds = ds.map(lambda *ex: dict(zip([\"input\", \"output\"], ex)))\n",
        "  return ds\n",
        "\n",
        "print(\"A few raw validation examples...\")\n",
        "for ex in tfds.as_numpy(nq_dataset_code_code_small_v2(\"validation\").take(2)):\n",
        "  print(ex)\n",
        "print(\"A few raw training examples...\")\n",
        "for ex in tfds.as_numpy(nq_dataset_code_code_small_v2(\"train\").take(2)):\n",
        "  print(ex)\n",
        "\n",
        "def marked_code_preprocessing(ds):\n",
        "  \n",
        "  def to_inputs_and_targets(ex):\n",
        "\n",
        "        inputs = tf.strings.join(['code2code: ' + ex['input']], separator=' ')\n",
        "        class_label = tf.strings.join([ex['output']], separator=' ')\n",
        "        return {'inputs': inputs, 'targets': class_label }\n",
        "    \n",
        "  return ds.map(to_inputs_and_targets, \n",
        "                num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "#Create a new training task\n",
        "t5.data.TaskRegistry.remove('code_code_small_v2')\n",
        "t5.data.TaskRegistry.add(\n",
        "    \"code_code_small_v2\",\n",
        "    dataset_fn=nq_dataset_code_code_small_v2,\n",
        "    splits=[\"train\", \"validation\"],\n",
        "    text_preprocessor=[marked_code_preprocessing],\n",
        "    output_features = DEFAULT_OUTPUT_FEATURES,\n",
        "    metric_fns=[t5.evaluation.metrics.accuracy],\n",
        "    num_input_examples=num_nq_examples_codeANDcomment_code_small_v2\n",
        ")\n",
        "\n",
        "nq_task = t5.data.TaskRegistry.get(\"code_code_small_v2\")\n",
        "ds = nq_task.get_dataset(split=\"train\", sequence_length={\"inputs\": 512, \"targets\": 512})\n",
        "# print(\"A few preprocessed training examples...\")\n",
        "# for ex in tfds.as_numpy(ds.take(3)):\n",
        "#   print(ex)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A few raw validation examples...\n",
            "{'input': b'public void startRuntime() { String tempDir = AppConstants.getInstance().getString(\"log.dir\", null); v8 = V8.createV8Runtime(\"J2V8Javascript\", tempDir); }', 'output': b'public void startRuntime() { v8 = V8.createV8Runtime(); }'}\n",
            "{'input': b'public GWCConfig getConfig() { if (gsEnvironment != null && gsEnvironment.isStale()) { syncEnvironment(); } return gwcConfigPersister.getConfig(); }', 'output': b'public GWCConfig getConfig() { return gwcConfigPersister.getConfig(); }'}\n",
            "A few raw training examples...\n",
            "{'input': b'protected static String commentFormat(String comment) { if (comment == null || comment.isEmpty()) return \"\"; while (comment.getBytes(ENCODING).length > 255) { comment = comment.substring(0, comment.length() - 1); } return comment; }', 'output': b'protected static String commentFormat(String comment) { if (comment == null || comment.length() == 0) return \"\"; while (comment.getBytes(ENCODING).length > 255) { comment = comment.substring(0, comment.length() - 1); } return comment; }'}\n",
            "{'input': b'public PageTitle(final String namespace, final String text) { this.namespace = namespace; this.text = text.replaceAll(\"\\\\\\\\s+\", \"_\"); }', 'output': b'public PageTitle(final String namespace, final String text) { this.namespace = namespace; this.text = text; }'}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seqio/preprocessors.py:90: UserWarning: Creating resources inside a function passed to Dataset.map() is not supported. Create each resource outside the function, and capture it inside the function to use it.\n",
            "  _tokenize, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDaV873tzqQ4"
      },
      "source": [
        "## EIGHT TASK : CODE and COMMENT to CODE small_dataset_v2\n",
        "- task name = `codeANDcomment_code_small_v2`\n",
        "- task prefix = `code&comment2code: `"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_eytfKk5xWy8",
        "outputId": "d28a8c6a-ad19-456e-dec5-226a83258ea8"
      },
      "source": [
        "def nq_dataset_codeANDcomment_code_small_v2(split, shuffle_files=False):\n",
        "  # We only have one file for each split.\n",
        "  del shuffle_files\n",
        "\n",
        "  # Load lines from the text file as examples.\n",
        "  ds = tf.data.TextLineDataset(nq_tsv_path_codeANDcomment_code_small_v2[split])\n",
        "  ds = ds.map(\n",
        "      functools.partial(tf.io.decode_csv, record_defaults=[\"string\",\"string\"],\n",
        "                        field_delim=\"\\t\", use_quote_delim=False),\n",
        "      num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "  \n",
        "  ds = ds.map(lambda *ex: dict(zip([\"input\", \"output\"], ex)))\n",
        "  return ds\n",
        "\n",
        "print(\"A few raw validation examples...\")\n",
        "for ex in tfds.as_numpy(nq_dataset_codeANDcomment_code_small_v2(\"validation\").take(2)):\n",
        "  print(ex)\n",
        "print(\"A few raw training examples...\")\n",
        "for ex in tfds.as_numpy(nq_dataset_codeANDcomment_code_small_v2(\"train\").take(2)):\n",
        "  print(ex)\n",
        "\n",
        "def marked_code_preprocessing(ds):\n",
        "  \n",
        "  def to_inputs_and_targets(ex):\n",
        "\n",
        "        inputs = tf.strings.join(['code&comment2code: ' + ex['input']], separator=' ')\n",
        "        class_label = tf.strings.join([ex['output']], separator=' ')\n",
        "        return {'inputs': inputs, 'targets': class_label }\n",
        "    \n",
        "  return ds.map(to_inputs_and_targets, \n",
        "                num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "#Create a new training task\n",
        "t5.data.TaskRegistry.remove('codeANDcomment_code_small_v2')\n",
        "t5.data.TaskRegistry.add(\n",
        "    \"codeANDcomment_code_small_v2\",\n",
        "    dataset_fn=nq_dataset_codeANDcomment_code_small_v2,\n",
        "    splits=[\"train\", \"validation\"],\n",
        "    text_preprocessor=[marked_code_preprocessing],\n",
        "    output_features = DEFAULT_OUTPUT_FEATURES,\n",
        "    metric_fns=[t5.evaluation.metrics.accuracy],\n",
        "    num_input_examples=num_nq_examples_codeANDcomment_code_small_v2\n",
        ")\n",
        "\n",
        "nq_task = t5.data.TaskRegistry.get(\"codeANDcomment_code_small_v2\")\n",
        "ds = nq_task.get_dataset(split=\"train\", sequence_length={\"inputs\": 512, \"targets\": 512})\n",
        "# print(\"A few preprocessed training examples...\")\n",
        "# for ex in tfds.as_numpy(ds.take(3)):\n",
        "#   print(ex)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A few raw validation examples...\n",
            "{'input': b'<code> public void startRuntime() { String tempDir = AppConstants.getInstance().getString(\"log.dir\", null); <START> v8 = V8.createV8Runtime(\"J2V8Javascript\", tempDir); <END> } </code><technical_language> Setting alias temp directory important. this optional argument? overwrite alias needed (i.e. improve performance). Right instances cached name </technical_language>', 'output': b'public void startRuntime() { v8 = V8.createV8Runtime(); }'}\n",
            "{'input': b'<code> public GWCConfig getConfig() { <START> if (gsEnvironment != null && gsEnvironment.isStale()) { <END> syncEnvironment(); } return gwcConfigPersister.getConfig(); } </code><technical_language> This bloc of code is repeated times, centralized in a single method </technical_language>', 'output': b'public GWCConfig getConfig() { return gwcConfigPersister.getConfig(); }'}\n",
            "A few raw training examples...\n",
            "{'input': b'<code> protected static String commentFormat(String comment) { <START> if (comment == null || comment.isEmpty()) return \"\"; <END> while (comment.getBytes(ENCODING).length > 255) { comment = comment.substring(0, comment.length() - 1); } return comment; } </code><technical_language> String.isEmpty() is avaible of JDK 1.6. Please if (comment == null || comment.length() == 0) return \"\"; this compilable JDK 1.5 </technical_language>', 'output': b'protected static String commentFormat(String comment) { if (comment == null || comment.length() == 0) return \"\"; while (comment.getBytes(ENCODING).length > 255) { comment = comment.substring(0, comment.length() - 1); } return comment; }'}\n",
            "{'input': b'<code> public PageTitle(final String namespace, final String text) { this.namespace = namespace; <START> this.text = text.replaceAll(\"\\\\\\\\s+\", \"_\"); <END> } </code><technical_language> \\\\s include zero-width non-joiner stuff? Oh, languages, fun. :D </technical_language>', 'output': b'public PageTitle(final String namespace, final String text) { this.namespace = namespace; this.text = text; }'}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seqio/preprocessors.py:90: UserWarning: Creating resources inside a function passed to Dataset.map() is not supported. Create each resource outside the function, and capture it inside the function to use it.\n",
            "  _tokenize, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4jj7R6x4UoE"
      },
      "source": [
        "# Setting Up fine tuning tasks and mixtures"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-SDyG3Z44elJ"
      },
      "source": [
        "def _rate_num_input_examples(task):\n",
        "  if \"train\" in task.splits:\n",
        "    return float(task.num_input_examples(\"train\"))\n",
        "  elif \"validation\" in task.splits:\n",
        "    return float(task.num_input_examples(\"validation\"))\n",
        "  else:\n",
        "    raise ValueError(\"Task %s does not have a train or validation split.\" % (task.name))"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MCvcX3uV4bZb",
        "outputId": "59f524bd-232e-4f66-f298-ffce0dff9c67"
      },
      "source": [
        "t5.data.MixtureRegistry.remove(\"code_code_large\")\n",
        "t5.data.MixtureRegistry.add(\n",
        "    \"code_code_large\",\n",
        "    [\"code_code\"],\n",
        "    default_rate=_rate_num_input_examples\n",
        ")\n",
        "t5.data.MixtureRegistry.remove(\"code_comment_large\")\n",
        "t5.data.MixtureRegistry.add(\n",
        "    \"code_comment_large\",\n",
        "    [\"code_comment\"],\n",
        "    default_rate=_rate_num_input_examples\n",
        ")\n",
        "\n",
        "t5.data.MixtureRegistry.remove(\"codeANDcomment_large\")\n",
        "t5.data.MixtureRegistry.add(\n",
        "    \"codeANDcomment_large\",\n",
        "    [\"codeANDcomment_code\"],\n",
        "    default_rate=_rate_num_input_examples\n",
        ")\n",
        "\n",
        "t5.data.MixtureRegistry.remove(\"marked_code_large\")\n",
        "t5.data.MixtureRegistry.add(\n",
        "    \"marked_code_large\",\n",
        "    [\"marked_code\"],\n",
        "    default_rate=_rate_num_input_examples\n",
        ")\n",
        "\n",
        "t5.data.MixtureRegistry.remove(\"all_large\")\n",
        "t5.data.MixtureRegistry.add(\n",
        "    \"all_large\",\n",
        "    [\"code_code\",\"code_comment\",\"codeANDcomment_code\"],\n",
        "    default_rate=_rate_num_input_examples\n",
        ")\n",
        "\n",
        "t5.data.MixtureRegistry.remove(\"code_code_small_dataset_v1\")\n",
        "t5.data.MixtureRegistry.add(\n",
        "    \"code_code_small_dataset_v1\",\n",
        "    [\"code_code_small_v1\"],\n",
        "    default_rate=_rate_num_input_examples\n",
        ")\n",
        "\n",
        "t5.data.MixtureRegistry.remove(\"codeANDcomment_code_small_dataset_v1\")\n",
        "t5.data.MixtureRegistry.add(\n",
        "    \"codeANDcomment_code_small_dataset_v1\",\n",
        "    [\"codeANDcomment_code_small_v1\"],\n",
        "    default_rate=_rate_num_input_examples\n",
        ")\n",
        "\n",
        "t5.data.MixtureRegistry.remove(\"all_small_v1\")\n",
        "t5.data.MixtureRegistry.add(\n",
        "    \"all_small_v1\",\n",
        "    [\"code_code_small_v1\", \"codeANDcomment_code_small_v1\"],\n",
        "    default_rate=_rate_num_input_examples\n",
        ") \n",
        "\n",
        "t5.data.MixtureRegistry.remove(\"code_code_small_dataset_v2\")\n",
        "t5.data.MixtureRegistry.add(\n",
        "    \"code_code_small_dataset_v2\",\n",
        "    [\"code_code_small_v2\"],\n",
        "    default_rate=_rate_num_input_examples\n",
        ")\n",
        "\n",
        "t5.data.MixtureRegistry.remove(\"codeANDcomment_code_small_dataset_v2\")\n",
        "t5.data.MixtureRegistry.add(\n",
        "    \"codeANDcomment_code_small_dataset_v2\",\n",
        "    [\"codeANDcomment_code_small_v2\"],\n",
        "    default_rate=_rate_num_input_examples\n",
        ")\n",
        "\n",
        "t5.data.MixtureRegistry.remove(\"all_small_v2\")\n",
        "t5.data.MixtureRegistry.add(\n",
        "    \"all_small_v2\",\n",
        "    [\"code_code_small_v2\", \"codeANDcomment_code_small_v2\"],\n",
        "    default_rate=_rate_num_input_examples\n",
        ")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<seqio.dataset_providers.Mixture at 0x7fd545fc5790>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n73E59co4M20"
      },
      "source": [
        "We specify the path of our pre-trained model, the model size (small), and the directory where we want to store our model checkpoints in the GCS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "diBUukTP8_4K"
      },
      "source": [
        "# Specify the pre-trained dir which must contain the pre-trained models, the operative_config.gin file and the checkpoint file as well\n",
        "PRETRAINED_DIR= 'gs://' + bucket_name + '/replication_package/model_dumps'\n",
        "\n",
        "# our T5 selected architecture\n",
        "MODEL_SIZE = \"small\"\n",
        "\n",
        "#@title Selecte the task or the mixture you want to train the model on\n",
        "Task_to_train = \"all_small_v1\" #@param [\"code_code_large\",\"code_comment_large\",\"codeANDcomment_large\",\"marked_code_large\",\"all_large\",\"code_code_small_dataset_v1\",\"codeANDcomment_code_small_dataset_v1\",\"all_small_v1\",\"code_code_small_dataset_v2\",\"codeANDcomment_code_small_dataset_v2\",\"all_small_v2\"]\n",
        "\n",
        "############ output path ############\n",
        "MODEL_DIR = 'gs://' + bucket_name + '/replication_package/fine_tuning_model_dumps/'+ Task_to_train \n",
        "\n",
        "\n",
        "model_parallelism, train_batch_size, keep_checkpoint_max = {\n",
        "    \"small\": (1, 128, 200),\n",
        "    \"base\": (2, 128, 8),\n",
        "    \"large\": (8, 64, 4),\n",
        "    \"3B\": (8, 16, 1),\n",
        "    \"11B\": (8, 16, 1)}[MODEL_SIZE]\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jm72wnoR4YP0"
      },
      "source": [
        "We set the selected learning rate scheduler"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WlsFvlRP6b_Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f545bed-7e8c-4b6d-99b7-7dea7fcc2bd8"
      },
      "source": [
        "from mesh_tensorflow.transformer.learning_rate_schedules import slanted_triangular \n",
        "\n",
        "from mesh_tensorflow.transformer.learning_rate_schedules import truncated_rsqrt\n",
        " \n",
        "from tensorflow.keras.optimizers.schedules import PolynomialDecay\n",
        "\n",
        "starter_learning_rate = 0.05\n",
        "end_learning_rate = 0.001\n",
        "decay_steps = 10000\n",
        "\n",
        "learning_rate_fn = PolynomialDecay(\n",
        "    starter_learning_rate,\n",
        "    decay_steps,\n",
        "    end_learning_rate,\n",
        "    power=0.5)\n",
        "\n",
        "#@title Select a learning rate scheduler\n",
        "learning_rate_scheduler_picker = \"slanted\" #@param [\"slanted\", \"isr\", \"polynomial\", \"constant\"]\n",
        "\n",
        "if learning_rate_scheduler_picker == \"slanted\":\n",
        "  selected_learning_rate_scheduler = slanted_triangular\n",
        "  PATH_GIN_FILE = 'gs://' + bucket_name + '/replication_package/utils/operative_config_slanted.gin'\n",
        "elif learning_rate_scheduler_picker == \"isr\":\n",
        "  selected_learning_rate_scheduler = truncated_rsqrt\n",
        "  PATH_GIN_FILE = 'gs://' + bucket_name + '/replication_package/utils/operative_config_isr.gin'\n",
        "elif learning_rate_scheduler_picker == \"polynomial\":\n",
        "  selected_learning_rate_scheduler = learning_rate_fn\n",
        "  PATH_GIN_FILE = 'gs://' + bucket_name + '/replication_package/utils/operative_config_polynomial.gin'\n",
        "elif learning_rate_scheduler_picker == \"constant\":\n",
        "  selected_learning_rate_scheduler = 0.001\n",
        "  PATH_GIN_FILE = 'gs://' + bucket_name + '/replication_package/utils/operative_config_constant.gin'\n",
        "\n",
        "#@title Select a learning rate scheduler\n",
        "number_of_steps = 1000 #@param {type:\"integer\"}\n",
        "\n",
        "tf.io.gfile.makedirs(MODEL_DIR)\n",
        "\n",
        "model = t5.models.MtfModel(\n",
        "    model_dir=MODEL_DIR,\n",
        "    tpu=TPU_ADDRESS,\n",
        "    tpu_topology=TPU_TOPOLOGY,\n",
        "    model_parallelism=model_parallelism,\n",
        "    batch_size=train_batch_size,\n",
        "    learning_rate_schedule = selected_learning_rate_scheduler,\n",
        "    sequence_length={\"inputs\": 512, \"targets\": 512},\n",
        "    save_checkpoints_steps=10000,\n",
        "    keep_checkpoint_max=keep_checkpoint_max if ON_CLOUD else None,\n",
        "    iterations_per_loop=100,\n",
        ")\n",
        "\n",
        "!gsutil cp {PATH_GIN_FILE}  ./config.gin\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying gs://code_review_automation/replication_package/utils/operative_config_slanted.gin...\n",
            "/ [1 files][ 11.6 KiB/ 11.6 KiB]                                                \n",
            "Operation completed over 1 objects/11.6 KiB.                                     \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFe0q8yMGxXf"
      },
      "source": [
        "If the selected learning rate scheduler is Slanted, we need to modify the gin file according to our settings:\n",
        "\n",
        "in particular, in this file, on line `197` we have to set the number of already done pre-training steps, in our original case was 200000\n",
        "\n",
        "then, in the next line we have to set the number of steps we want to fine tune the model, in our case we have different settings for each configurations:\n",
        "- small dataset single task 100K\n",
        "- small dataset mixture 100K\n",
        "- large dataset single task 300K\n",
        "- large dataset mixture 600K\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BsX8mjVrBV3D",
        "outputId": "d8c403bb-ad21-4080-9f90-746e49d3d41b"
      },
      "source": [
        "import gin\n",
        "\n",
        "# # PRETRAINED\n",
        "#with gin.unlock_config():\n",
        "#    gin.parse_config_file(\"./config.gin\")\n",
        "#    #RUN FINE-TUNING\n",
        "#    FINETUNE_STEPS = number_of_steps\n",
        "#    model.finetune(\n",
        "#        mixture_or_task_name=Task_to_train,\n",
        "#        pretrained_model_dir=PRETRAINED_DIR,\n",
        "#        finetune_steps=FINETUNE_STEPS\n",
        "#    )\n",
        "# NON PRETRAINED\n",
        "with gin.unlock_config():\n",
        "    gin.parse_config_file(\"./config.gin\")\n",
        "    TRAIN_STEPS = number_of_steps\n",
        "    model.train(Task_to_train, steps=TRAIN_STEPS)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:root:system_path_file_exists:gs://code_review_automation/replication_package/model_dumps/operative_config.gin\n",
            "ERROR:root:Path not found: gs://code_review_automation/replication_package/model_dumps/operative_config.gin\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using config: {'_model_dir': 'gs://code_review_automation/replication_package/fine_tuning_model_dumps/all_small_v1', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 10000, '_save_checkpoints_secs': None, '_session_config': graph_options {\n",
            "  rewrite_options {\n",
            "    disable_meta_optimizer: true\n",
            "  }\n",
            "}\n",
            "cluster_def {\n",
            "  job {\n",
            "    name: \"worker\"\n",
            "    tasks {\n",
            "      key: 0\n",
            "      value: \"10.43.199.66:8470\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({'worker': ['10.43.199.66:8470']}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': 'grpc://10.43.199.66:8470', '_evaluation_master': 'grpc://10.43.199.66:8470', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=100, num_shards=None, num_cores_per_replica=1, per_host_input_for_training=4, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2, experimental_host_call_every_n_steps=1, experimental_allow_per_host_v2_parallel_get_next=False, experimental_feed_hook=None), '_cluster': <tensorflow.python.distribute.cluster_resolver.tpu.tpu_cluster_resolver.TPUClusterResolver object at 0x7fd54615dd50>}\n",
            "INFO:tensorflow:_TPUContext: eval_on_tpu True\n",
            "INFO:tensorflow:Skipping training since max_steps has already saved.\n",
            "INFO:tensorflow:training_loop marked as finished\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLLt9ywacQy8"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Df3oaQ3_cPVx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eaab7ea9-80e9-453b-8c6b-f474738fadad"
      },
      "source": [
        "# Use a larger batch size for evaluation, which requires less memory.\n",
        "model.batch_size = 1024\n",
        "model.eval(\n",
        "    mixture_or_task_name=Task_to_train,\n",
        "    # -1 will evaluate the last checkpoint, you can also provide \n",
        "    # a list of checkpoints with the following format : [10, 20, 30]\n",
        "    checkpoint_steps=-1\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:root:system_path_file_exists:gs://code_review_automation/replication_package/fine_tuning_model_dumps/all_small_v1/operative_config.gin\n",
            "ERROR:root:Path not found: gs://code_review_automation/replication_package/fine_tuning_model_dumps/all_small_v1/operative_config.gin\n",
            "INFO:absl:Adding task 'codeANDcomment_code_small_v1' with predict metric_fn(s).\n",
            "INFO:absl:Adding task 'code_code_small_v1' with predict metric_fn(s).\n",
            "INFO:absl:Automatically caching small dataset in memory: 'codeANDcomment_code_small_v1:validation'\n",
            "/usr/local/lib/python3.7/dist-packages/seqio/preprocessors.py:90: UserWarning: Creating resources inside a function passed to Dataset.map() is not supported. Create each resource outside the function, and capture it inside the function to use it.\n",
            "  _tokenize, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
            "INFO:absl:Skipping packing/padding for 'codeANDcomment_code_small_v1' since sequence length is None.\n",
            "INFO:absl:Automatically caching small dataset in memory: 'code_code_small_v1:validation'\n",
            "INFO:absl:Skipping packing/padding for 'code_code_small_v1' since sequence length is None.\n",
            "INFO:absl:Setting sequence lengths to {'inputs': 913, 'targets': 179}\n",
            "INFO:absl:Evaluating checkpoint step: 700\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using config: {'_model_dir': 'gs://code_review_automation/replication_package/fine_tuning_model_dumps/all_small_v1', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 10000, '_save_checkpoints_secs': None, '_session_config': graph_options {\n",
            "  rewrite_options {\n",
            "    disable_meta_optimizer: true\n",
            "  }\n",
            "}\n",
            "cluster_def {\n",
            "  job {\n",
            "    name: \"worker\"\n",
            "    tasks {\n",
            "      key: 0\n",
            "      value: \"10.43.199.66:8470\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({'worker': ['10.43.199.66:8470']}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': 'grpc://10.43.199.66:8470', '_evaluation_master': 'grpc://10.43.199.66:8470', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=100, num_shards=None, num_cores_per_replica=1, per_host_input_for_training=4, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2, experimental_host_call_every_n_steps=1, experimental_allow_per_host_v2_parallel_get_next=False, experimental_feed_hook=None), '_cluster': <tensorflow.python.distribute.cluster_resolver.tpu.tpu_cluster_resolver.TPUClusterResolver object at 0x7fd54615dd50>}\n",
            "INFO:tensorflow:_TPUContext: eval_on_tpu True\n",
            "INFO:tensorflow:Querying Tensorflow master (grpc://10.43.199.66:8470) for TPU system metadata.\n",
            "INFO:tensorflow:Initializing TPU system (master: grpc://10.43.199.66:8470) to fetch topology for model parallelism. This might take a while.\n",
            "INFO:tensorflow:Found TPU system:\n",
            "INFO:tensorflow:*** Num TPU Cores: 8\n",
            "INFO:tensorflow:*** Num TPU Workers: 1\n",
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, -4284013198042617505)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 3908844065217960952)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -444752713214085535)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -777436793638836199)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -2876346740869343367)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 2721201093506797607)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 8886139630517533961)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, -3598464739761706947)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -8361496358606082574)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, -3465983448002879422)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 6437355313563668189)\n",
            "INFO:tensorflow:Calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:Automatically caching small dataset in memory: 'codeANDcomment_code_small_v1:validation'\n",
            "INFO:absl:Padding 'codeANDcomment_code_small_v1' with sequence lengths: {'inputs': 913, 'targets': 179}\n",
            "INFO:absl:Automatically caching small dataset in memory: 'code_code_small_v1:validation'\n",
            "INFO:absl:Padding 'code_code_small_v1' with sequence lengths: {'inputs': 913, 'targets': 179}\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:num_cores_per_replica: 1\n",
            "INFO:tensorflow:computation_shape: [1, 1, 1, 1]\n",
            "INFO:tensorflow:num_replicas: 8\n",
            "INFO:tensorflow:device_assignment.topology.device_coordinates: [[[0 0 0 0]\n",
            "  [0 0 0 1]\n",
            "  [1 0 0 0]\n",
            "  [1 0 0 1]\n",
            "  [0 1 0 0]\n",
            "  [0 1 0 1]\n",
            "  [1 1 0 0]\n",
            "  [1 1 0 1]]]\n",
            "INFO:tensorflow:device_assignment.core_assignment: [[[0 0 0 0]]\n",
            "\n",
            " [[0 0 0 1]]\n",
            "\n",
            " [[1 0 0 0]]\n",
            "\n",
            " [[1 0 0 1]]\n",
            "\n",
            " [[0 1 0 0]]\n",
            "\n",
            " [[0 1 0 1]]\n",
            "\n",
            " [[1 1 0 0]]\n",
            "\n",
            " [[1 1 0 1]]]\n",
            "INFO:tensorflow:auto_logical_to_physical_tpu logical_shape=[8] physical_shape=[2, 2, 2]\n",
            "INFO:tensorflow:auto_logical_to_physical_tpu logical_to_physical = [(0, 0, 0), (0, 0, 1), (0, 1, 0), (0, 1, 1), (1, 1, 0), (1, 1, 1), (1, 0, 0), (1, 0, 1)]\n",
            "WARNING:tensorflow:SimdMeshImpl ignoring devices ['', '', '', '', '', '', '', '']\n",
            "INFO:tensorflow:SimdMeshImpl init: Shape[batch=8] LayoutRules{('ensemble', 'ensemble'), ('experts', 'batch'), ('batch', 'batch'), ('d_ff', 'model'), ('vocab', 'model'), ('heads', 'model')}\n",
            "INFO:tensorflow:Device Assignment: <tensorflow.python.tpu.device_assignment.DeviceAssignment object at 0x7fd54817ff50>\n",
            "WARNING:tensorflow:Using default tf glorot_uniform_initializer for variable encoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n",
            "WARNING:tensorflow:Using default tf glorot_uniform_initializer for variable decoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n",
            "WARNING:tensorflow:Using default tf glorot_uniform_initializer for variable decoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n",
            "INFO:tensorflow:Create pnum_tensor\n",
            "INFO:tensorflow:Casting <dtype: 'int32'> to float32 for allreduce\n",
            "INFO:tensorflow:Casting <dtype: 'int32'> to float32 for allreduce\n",
            "INFO:tensorflow:Variable decoder/block_000/layer_000/SelfAttention/k                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_000/layer_000/SelfAttention/o                  size 262144       slice_size 262144       Shape[heads=512, d_model=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_000/layer_000/SelfAttention/q                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_000/layer_000/SelfAttention/v                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_000/layer_001/EncDecAttention/k                size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_000/layer_001/EncDecAttention/o                size 262144       slice_size 262144       Shape[heads=512, d_model=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_000/layer_001/EncDecAttention/q                size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_000/layer_001/EncDecAttention/v                size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_000/layer_002/DenseReluDense/wi/kernel         size 1048576      slice_size 1048576      Shape[d_model=512, d_ff=2048]                               \n",
            "INFO:tensorflow:Variable decoder/block_000/layer_002/DenseReluDense/wo/kernel         size 1048576      slice_size 1048576      Shape[d_ff=2048, d_model=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_001/layer_000/SelfAttention/k                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_001/layer_000/SelfAttention/o                  size 262144       slice_size 262144       Shape[heads=512, d_model=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_001/layer_000/SelfAttention/q                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_001/layer_000/SelfAttention/v                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_001/layer_001/EncDecAttention/k                size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_001/layer_001/EncDecAttention/o                size 262144       slice_size 262144       Shape[heads=512, d_model=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_001/layer_001/EncDecAttention/q                size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_001/layer_001/EncDecAttention/v                size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_001/layer_002/DenseReluDense/wi/kernel         size 1048576      slice_size 1048576      Shape[d_model=512, d_ff=2048]                               \n",
            "INFO:tensorflow:Variable decoder/block_001/layer_002/DenseReluDense/wo/kernel         size 1048576      slice_size 1048576      Shape[d_ff=2048, d_model=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_002/layer_000/SelfAttention/k                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_002/layer_000/SelfAttention/o                  size 262144       slice_size 262144       Shape[heads=512, d_model=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_002/layer_000/SelfAttention/q                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_002/layer_000/SelfAttention/v                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_002/layer_001/EncDecAttention/k                size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_002/layer_001/EncDecAttention/o                size 262144       slice_size 262144       Shape[heads=512, d_model=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_002/layer_001/EncDecAttention/q                size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_002/layer_001/EncDecAttention/v                size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_002/layer_002/DenseReluDense/wi/kernel         size 1048576      slice_size 1048576      Shape[d_model=512, d_ff=2048]                               \n",
            "INFO:tensorflow:Variable decoder/block_002/layer_002/DenseReluDense/wo/kernel         size 1048576      slice_size 1048576      Shape[d_ff=2048, d_model=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_003/layer_000/SelfAttention/k                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_003/layer_000/SelfAttention/o                  size 262144       slice_size 262144       Shape[heads=512, d_model=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_003/layer_000/SelfAttention/q                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_003/layer_000/SelfAttention/v                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_003/layer_001/EncDecAttention/k                size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_003/layer_001/EncDecAttention/o                size 262144       slice_size 262144       Shape[heads=512, d_model=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_003/layer_001/EncDecAttention/q                size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_003/layer_001/EncDecAttention/v                size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_003/layer_002/DenseReluDense/wi/kernel         size 1048576      slice_size 1048576      Shape[d_model=512, d_ff=2048]                               \n",
            "INFO:tensorflow:Variable decoder/block_003/layer_002/DenseReluDense/wo/kernel         size 1048576      slice_size 1048576      Shape[d_ff=2048, d_model=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_004/layer_000/SelfAttention/k                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_004/layer_000/SelfAttention/o                  size 262144       slice_size 262144       Shape[heads=512, d_model=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_004/layer_000/SelfAttention/q                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_004/layer_000/SelfAttention/v                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_004/layer_001/EncDecAttention/k                size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_004/layer_001/EncDecAttention/o                size 262144       slice_size 262144       Shape[heads=512, d_model=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_004/layer_001/EncDecAttention/q                size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_004/layer_001/EncDecAttention/v                size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_004/layer_002/DenseReluDense/wi/kernel         size 1048576      slice_size 1048576      Shape[d_model=512, d_ff=2048]                               \n",
            "INFO:tensorflow:Variable decoder/block_004/layer_002/DenseReluDense/wo/kernel         size 1048576      slice_size 1048576      Shape[d_ff=2048, d_model=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_005/layer_000/SelfAttention/k                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_005/layer_000/SelfAttention/o                  size 262144       slice_size 262144       Shape[heads=512, d_model=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_005/layer_000/SelfAttention/q                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_005/layer_000/SelfAttention/v                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_005/layer_001/EncDecAttention/k                size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_005/layer_001/EncDecAttention/o                size 262144       slice_size 262144       Shape[heads=512, d_model=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_005/layer_001/EncDecAttention/q                size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_005/layer_001/EncDecAttention/v                size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_005/layer_002/DenseReluDense/wi/kernel         size 1048576      slice_size 1048576      Shape[d_model=512, d_ff=2048]                               \n",
            "INFO:tensorflow:Variable decoder/block_005/layer_002/DenseReluDense/wo/kernel         size 1048576      slice_size 1048576      Shape[d_ff=2048, d_model=512]                               \n",
            "INFO:tensorflow:Variable encoder/block_000/layer_000/SelfAttention/k                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable encoder/block_000/layer_000/SelfAttention/o                  size 262144       slice_size 262144       Shape[heads=512, d_model=512]                               \n",
            "INFO:tensorflow:Variable encoder/block_000/layer_000/SelfAttention/q                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable encoder/block_000/layer_000/SelfAttention/v                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable encoder/block_000/layer_001/DenseReluDense/wi/kernel         size 1048576      slice_size 1048576      Shape[d_model=512, d_ff=2048]                               \n",
            "INFO:tensorflow:Variable encoder/block_000/layer_001/DenseReluDense/wo/kernel         size 1048576      slice_size 1048576      Shape[d_ff=2048, d_model=512]                               \n",
            "INFO:tensorflow:Variable encoder/block_001/layer_000/SelfAttention/k                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable encoder/block_001/layer_000/SelfAttention/o                  size 262144       slice_size 262144       Shape[heads=512, d_model=512]                               \n",
            "INFO:tensorflow:Variable encoder/block_001/layer_000/SelfAttention/q                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable encoder/block_001/layer_000/SelfAttention/v                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable encoder/block_001/layer_001/DenseReluDense/wi/kernel         size 1048576      slice_size 1048576      Shape[d_model=512, d_ff=2048]                               \n",
            "INFO:tensorflow:Variable encoder/block_001/layer_001/DenseReluDense/wo/kernel         size 1048576      slice_size 1048576      Shape[d_ff=2048, d_model=512]                               \n",
            "INFO:tensorflow:Variable encoder/block_002/layer_000/SelfAttention/k                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable encoder/block_002/layer_000/SelfAttention/o                  size 262144       slice_size 262144       Shape[heads=512, d_model=512]                               \n",
            "INFO:tensorflow:Variable encoder/block_002/layer_000/SelfAttention/q                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable encoder/block_002/layer_000/SelfAttention/v                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable encoder/block_002/layer_001/DenseReluDense/wi/kernel         size 1048576      slice_size 1048576      Shape[d_model=512, d_ff=2048]                               \n",
            "INFO:tensorflow:Variable encoder/block_002/layer_001/DenseReluDense/wo/kernel         size 1048576      slice_size 1048576      Shape[d_ff=2048, d_model=512]                               \n",
            "INFO:tensorflow:Variable encoder/block_003/layer_000/SelfAttention/k                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable encoder/block_003/layer_000/SelfAttention/o                  size 262144       slice_size 262144       Shape[heads=512, d_model=512]                               \n",
            "INFO:tensorflow:Variable encoder/block_003/layer_000/SelfAttention/q                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable encoder/block_003/layer_000/SelfAttention/v                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable encoder/block_003/layer_001/DenseReluDense/wi/kernel         size 1048576      slice_size 1048576      Shape[d_model=512, d_ff=2048]                               \n",
            "INFO:tensorflow:Variable encoder/block_003/layer_001/DenseReluDense/wo/kernel         size 1048576      slice_size 1048576      Shape[d_ff=2048, d_model=512]                               \n",
            "INFO:tensorflow:Variable encoder/block_004/layer_000/SelfAttention/k                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable encoder/block_004/layer_000/SelfAttention/o                  size 262144       slice_size 262144       Shape[heads=512, d_model=512]                               \n",
            "INFO:tensorflow:Variable encoder/block_004/layer_000/SelfAttention/q                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable encoder/block_004/layer_000/SelfAttention/v                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable encoder/block_004/layer_001/DenseReluDense/wi/kernel         size 1048576      slice_size 1048576      Shape[d_model=512, d_ff=2048]                               \n",
            "INFO:tensorflow:Variable encoder/block_004/layer_001/DenseReluDense/wo/kernel         size 1048576      slice_size 1048576      Shape[d_ff=2048, d_model=512]                               \n",
            "INFO:tensorflow:Variable encoder/block_005/layer_000/SelfAttention/k                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable encoder/block_005/layer_000/SelfAttention/o                  size 262144       slice_size 262144       Shape[heads=512, d_model=512]                               \n",
            "INFO:tensorflow:Variable encoder/block_005/layer_000/SelfAttention/q                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable encoder/block_005/layer_000/SelfAttention/v                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable encoder/block_005/layer_001/DenseReluDense/wi/kernel         size 1048576      slice_size 1048576      Shape[d_model=512, d_ff=2048]                               \n",
            "INFO:tensorflow:Variable encoder/block_005/layer_001/DenseReluDense/wo/kernel         size 1048576      slice_size 1048576      Shape[d_ff=2048, d_model=512]                               \n",
            "INFO:tensorflow:Variable shared/embedding                                             size 16449536     slice_size 16449536     Shape[vocab=32128, d_model=512]                             \n",
            "INFO:tensorflow:Variable stacked/encoder/block_000/layer_000/SelfAttention/relative_attention_bias size 512          slice_size 512          Shape[stacked=2, heads=8, buckets=32]                       \n",
            "INFO:tensorflow:    encoder/block_000/layer_000/SelfAttention/relative_attention_bias\n",
            "INFO:tensorflow:    decoder/block_000/layer_000/SelfAttention/relative_attention_bias\n",
            "INFO:tensorflow:Variable stacked/encoder/block_000/layer_000/layer_norm/scale         size 16384        slice_size 16384        Shape[stacked=32, d_model=512]                              \n",
            "INFO:tensorflow:    encoder/block_000/layer_000/layer_norm/scale\n",
            "INFO:tensorflow:    encoder/block_000/layer_001/layer_norm/scale\n",
            "INFO:tensorflow:    encoder/block_001/layer_000/layer_norm/scale\n",
            "INFO:tensorflow:    encoder/block_001/layer_001/layer_norm/scale\n",
            "INFO:tensorflow:    encoder/block_002/layer_000/layer_norm/scale\n",
            "INFO:tensorflow:    encoder/block_002/layer_001/layer_norm/scale\n",
            "INFO:tensorflow:    encoder/block_003/layer_000/layer_norm/scale\n",
            "INFO:tensorflow:    encoder/block_003/layer_001/layer_norm/scale\n",
            "INFO:tensorflow:    encoder/block_004/layer_000/layer_norm/scale\n",
            "INFO:tensorflow:    encoder/block_004/layer_001/layer_norm/scale\n",
            "INFO:tensorflow:    encoder/block_005/layer_000/layer_norm/scale\n",
            "INFO:tensorflow:    encoder/block_005/layer_001/layer_norm/scale\n",
            "INFO:tensorflow:    encoder/final_layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_000/layer_000/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_000/layer_001/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_000/layer_002/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_001/layer_000/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_001/layer_001/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_001/layer_002/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_002/layer_000/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_002/layer_001/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_002/layer_002/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_003/layer_000/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_003/layer_001/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_003/layer_002/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_004/layer_000/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_004/layer_001/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_004/layer_002/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_005/layer_000/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_005/layer_001/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_005/layer_002/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/final_layer_norm/scale\n",
            "INFO:tensorflow:Trainable Variables            count: 99      Total size: 60506624         Total slice_size: 60506624       \n",
            "INFO:tensorflow:All Variables                  count: 99      Total size: 60506624         Total slice_size: 60506624       \n",
            "INFO:tensorflow:Counters:\n",
            "allconcat: 1.5e+07\n",
            " allconcat/0: 1.5e+07\n",
            "  allconcat/0/reshape_op: 1.5e+07\n",
            "allreduce: 8\n",
            " allreduce/[0]: 8\n",
            "  allreduce/[0]/reduce_op: 8\n",
            "einsum: 1.03e+14\n",
            "einsum_unique: 1.03e+14\n",
            "output: 1.43e+12\n",
            " output/AddOperation: 4.25e+11\n",
            " output/BinaryOpWithBroadcasting: 5.3e+09\n",
            " output/Constant: 5.75e+09\n",
            " output/EinsumOperation: 2.72e+11\n",
            " output/ImportOperation: 8.95e+06\n",
            " output/MinMaxOperation: 1.21e+08\n",
            " output/OneHotOperation: 6.26e+10\n",
            " output/RangeOperation: 1.46e+04\n",
            " output/ReduceOperation: 2.99e+08\n",
            " output/ReshapeOperation: 4.93e+10\n",
            " output/ScalarAddOperation: 1.9e+08\n",
            " output/ScalarMultiplyOperation: 5.96e+09\n",
            " output/ShiftOperation: 9.35e+05\n",
            " output/SlicewiseOperation: 4.77e+11\n",
            " output/StackedVariable: 1.35e+05\n",
            " output/StopGradient: 1.23e+11\n",
            " output/UnstackOperation: 1.35e+05\n",
            " output/Variable: 4.84e+08\n",
            " output/WhileLoopOperation: 5.75e+09\n",
            "output_unique: 1.43e+12\n",
            " output_unique/AddOperation: 4.24e+11\n",
            " output_unique/BinaryOpWithBroadcasting: 5.16e+09\n",
            " output_unique/Constant: 5.75e+09\n",
            " output_unique/EinsumOperation: 2.71e+11\n",
            " output_unique/ImportOperation: 1.12e+06\n",
            " output_unique/MinMaxOperation: 1.59e+07\n",
            " output_unique/OneHotOperation: 6.04e+10\n",
            " output_unique/RangeOperation: 1.83e+03\n",
            " output_unique/ReduceOperation: 2.99e+08\n",
            " output_unique/ReshapeOperation: 4.93e+10\n",
            " output_unique/ScalarAddOperation: 4.99e+07\n",
            " output_unique/ScalarMultiplyOperation: 5.68e+09\n",
            " output_unique/ShiftOperation: 9.35e+05\n",
            " output_unique/SlicewiseOperation: 4.76e+11\n",
            " output_unique/StackedVariable: 1.69e+04\n",
            " output_unique/StopGradient: 1.23e+11\n",
            " output_unique/UnstackOperation: 1.69e+04\n",
            " output_unique/Variable: 6.05e+07\n",
            " output_unique/WhileLoopOperation: 5.75e+09\n",
            "variables: 6.05e+07\n",
            " variables/trainable: 6.05e+07\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:TPU job name worker\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from gs://code_review_automation/replication_package/fine_tuning_model_dumps/all_small_v1/model.ckpt-700\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py:840: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Prefer Variable.assign which has equivalent behavior in 2.X.\n",
            "INFO:tensorflow:Starting infeed thread controller.\n",
            "INFO:tensorflow:Starting outfeed thread controller.\n",
            "INFO:tensorflow:Initialized dataset iterators in 0 seconds\n",
            "INFO:tensorflow:Before copy master to slices.\n",
            "INFO:tensorflow:Done with copy master to slices.\n",
            "INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (0, 0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YAKKKClFdAZY"
      },
      "source": [
        "# Prediction "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RzGRjPFrdC_R",
        "outputId": "a1390bf0-66d9-4fb8-fc29-a4425a28ef4f"
      },
      "source": [
        "with open(\"./input.txt\",\"w\") as f:\n",
        "  ## change the task prefix with the one of your choice\n",
        "  ## see tasks above\n",
        "  f.write('code2code: \"your code here\"')\n",
        "\n",
        "model.predict(input_file='./input.txt', output_file='./output.txt', checkpoint_steps=-1,\n",
        "              beam_size=1, temperature=1.0, keep_top_k=-1, vocabulary=get_default_vocabulary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:root:system_path_file_exists:gs://code_review_automation/replication_package/fine_tuning_model_dumps/all_small_v1/operative_config.gin\n",
            "ERROR:root:Path not found: gs://code_review_automation/replication_package/fine_tuning_model_dumps/all_small_v1/operative_config.gin\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using config: {'_model_dir': 'gs://code_review_automation/replication_package/fine_tuning_model_dumps/all_small_v1', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 10000, '_save_checkpoints_secs': None, '_session_config': graph_options {\n",
            "  rewrite_options {\n",
            "    disable_meta_optimizer: true\n",
            "  }\n",
            "}\n",
            "cluster_def {\n",
            "  job {\n",
            "    name: \"worker\"\n",
            "    tasks {\n",
            "      key: 0\n",
            "      value: \"10.125.53.98:8470\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({'worker': ['10.125.53.98:8470']}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': 'grpc://10.125.53.98:8470', '_evaluation_master': 'grpc://10.125.53.98:8470', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=100, num_shards=None, num_cores_per_replica=1, per_host_input_for_training=4, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2, experimental_host_call_every_n_steps=1, experimental_allow_per_host_v2_parallel_get_next=False, experimental_feed_hook=None), '_cluster': <tensorflow.python.distribute.cluster_resolver.tpu.tpu_cluster_resolver.TPUClusterResolver object at 0x7fdba7b0d310>}\n",
            "INFO:tensorflow:_TPUContext: eval_on_tpu True\n",
            "INFO:tensorflow:Querying Tensorflow master (grpc://10.125.53.98:8470) for TPU system metadata.\n",
            "INFO:tensorflow:Initializing TPU system (master: grpc://10.125.53.98:8470) to fetch topology for model parallelism. This might take a while.\n",
            "INFO:tensorflow:Found TPU system:\n",
            "INFO:tensorflow:*** Num TPU Cores: 8\n",
            "INFO:tensorflow:*** Num TPU Workers: 1\n",
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 4025712427205452571)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 1322760993643496630)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -2576183401430103129)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -1590325048151439551)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 4799119043629808170)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, -3512083216256075292)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 5067868034616079750)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, -3857837328137784975)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -230555620278116676)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 6656035149174234586)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 128114689363834347)\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:num_cores_per_replica: 1\n",
            "INFO:tensorflow:computation_shape: [1, 1, 1, 1]\n",
            "INFO:tensorflow:num_replicas: 8\n",
            "INFO:tensorflow:device_assignment.topology.device_coordinates: [[[0 0 0 0]\n",
            "  [0 0 0 1]\n",
            "  [1 0 0 0]\n",
            "  [1 0 0 1]\n",
            "  [0 1 0 0]\n",
            "  [0 1 0 1]\n",
            "  [1 1 0 0]\n",
            "  [1 1 0 1]]]\n",
            "INFO:tensorflow:device_assignment.core_assignment: [[[0 0 0 0]]\n",
            "\n",
            " [[0 0 0 1]]\n",
            "\n",
            " [[1 0 0 0]]\n",
            "\n",
            " [[1 0 0 1]]\n",
            "\n",
            " [[0 1 0 0]]\n",
            "\n",
            " [[0 1 0 1]]\n",
            "\n",
            " [[1 1 0 0]]\n",
            "\n",
            " [[1 1 0 1]]]\n",
            "INFO:tensorflow:auto_logical_to_physical_tpu logical_shape=[8] physical_shape=[2, 2, 2]\n",
            "INFO:tensorflow:auto_logical_to_physical_tpu logical_to_physical = [(0, 0, 0), (0, 0, 1), (0, 1, 0), (0, 1, 1), (1, 1, 0), (1, 1, 1), (1, 0, 0), (1, 0, 1)]\n",
            "WARNING:tensorflow:SimdMeshImpl ignoring devices ['', '', '', '', '', '', '', '']\n",
            "INFO:tensorflow:SimdMeshImpl init: Shape[batch=8] LayoutRules{('vocab', 'model'), ('ensemble', 'ensemble'), ('batch', 'batch'), ('d_ff', 'model'), ('heads', 'model'), ('experts', 'batch')}\n",
            "INFO:tensorflow:Device Assignment: <tensorflow.python.tpu.device_assignment.DeviceAssignment object at 0x7fdba1c39790>\n",
            "WARNING:tensorflow:Using default tf glorot_uniform_initializer for variable encoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n",
            "WARNING:tensorflow:Using default tf glorot_uniform_initializer for variable decoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n",
            "WARNING:tensorflow:Using default tf glorot_uniform_initializer for variable decoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n",
            "INFO:tensorflow:Create pnum_tensor\n",
            "INFO:tensorflow:Casting <dtype: 'int32'> to float32 for allreduce\n",
            "INFO:tensorflow:Casting <dtype: 'int32'> to float32 for allreduce\n",
            "INFO:tensorflow:Variable decoder/block_000/layer_000/SelfAttention/k                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_000/layer_000/SelfAttention/o                  size 262144       slice_size 262144       Shape[heads=512, d_model=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_000/layer_000/SelfAttention/q                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_000/layer_000/SelfAttention/v                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_000/layer_001/EncDecAttention/k                size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_000/layer_001/EncDecAttention/o                size 262144       slice_size 262144       Shape[heads=512, d_model=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_000/layer_001/EncDecAttention/q                size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_000/layer_001/EncDecAttention/v                size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_000/layer_002/DenseReluDense/wi/kernel         size 1048576      slice_size 1048576      Shape[d_model=512, d_ff=2048]                               \n",
            "INFO:tensorflow:Variable decoder/block_000/layer_002/DenseReluDense/wo/kernel         size 1048576      slice_size 1048576      Shape[d_ff=2048, d_model=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_001/layer_000/SelfAttention/k                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_001/layer_000/SelfAttention/o                  size 262144       slice_size 262144       Shape[heads=512, d_model=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_001/layer_000/SelfAttention/q                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_001/layer_000/SelfAttention/v                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_001/layer_001/EncDecAttention/k                size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_001/layer_001/EncDecAttention/o                size 262144       slice_size 262144       Shape[heads=512, d_model=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_001/layer_001/EncDecAttention/q                size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_001/layer_001/EncDecAttention/v                size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_001/layer_002/DenseReluDense/wi/kernel         size 1048576      slice_size 1048576      Shape[d_model=512, d_ff=2048]                               \n",
            "INFO:tensorflow:Variable decoder/block_001/layer_002/DenseReluDense/wo/kernel         size 1048576      slice_size 1048576      Shape[d_ff=2048, d_model=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_002/layer_000/SelfAttention/k                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_002/layer_000/SelfAttention/o                  size 262144       slice_size 262144       Shape[heads=512, d_model=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_002/layer_000/SelfAttention/q                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_002/layer_000/SelfAttention/v                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_002/layer_001/EncDecAttention/k                size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_002/layer_001/EncDecAttention/o                size 262144       slice_size 262144       Shape[heads=512, d_model=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_002/layer_001/EncDecAttention/q                size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_002/layer_001/EncDecAttention/v                size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_002/layer_002/DenseReluDense/wi/kernel         size 1048576      slice_size 1048576      Shape[d_model=512, d_ff=2048]                               \n",
            "INFO:tensorflow:Variable decoder/block_002/layer_002/DenseReluDense/wo/kernel         size 1048576      slice_size 1048576      Shape[d_ff=2048, d_model=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_003/layer_000/SelfAttention/k                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_003/layer_000/SelfAttention/o                  size 262144       slice_size 262144       Shape[heads=512, d_model=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_003/layer_000/SelfAttention/q                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_003/layer_000/SelfAttention/v                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_003/layer_001/EncDecAttention/k                size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_003/layer_001/EncDecAttention/o                size 262144       slice_size 262144       Shape[heads=512, d_model=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_003/layer_001/EncDecAttention/q                size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_003/layer_001/EncDecAttention/v                size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_003/layer_002/DenseReluDense/wi/kernel         size 1048576      slice_size 1048576      Shape[d_model=512, d_ff=2048]                               \n",
            "INFO:tensorflow:Variable decoder/block_003/layer_002/DenseReluDense/wo/kernel         size 1048576      slice_size 1048576      Shape[d_ff=2048, d_model=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_004/layer_000/SelfAttention/k                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_004/layer_000/SelfAttention/o                  size 262144       slice_size 262144       Shape[heads=512, d_model=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_004/layer_000/SelfAttention/q                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_004/layer_000/SelfAttention/v                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_004/layer_001/EncDecAttention/k                size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_004/layer_001/EncDecAttention/o                size 262144       slice_size 262144       Shape[heads=512, d_model=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_004/layer_001/EncDecAttention/q                size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_004/layer_001/EncDecAttention/v                size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_004/layer_002/DenseReluDense/wi/kernel         size 1048576      slice_size 1048576      Shape[d_model=512, d_ff=2048]                               \n",
            "INFO:tensorflow:Variable decoder/block_004/layer_002/DenseReluDense/wo/kernel         size 1048576      slice_size 1048576      Shape[d_ff=2048, d_model=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_005/layer_000/SelfAttention/k                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_005/layer_000/SelfAttention/o                  size 262144       slice_size 262144       Shape[heads=512, d_model=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_005/layer_000/SelfAttention/q                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_005/layer_000/SelfAttention/v                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_005/layer_001/EncDecAttention/k                size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_005/layer_001/EncDecAttention/o                size 262144       slice_size 262144       Shape[heads=512, d_model=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_005/layer_001/EncDecAttention/q                size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_005/layer_001/EncDecAttention/v                size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_005/layer_002/DenseReluDense/wi/kernel         size 1048576      slice_size 1048576      Shape[d_model=512, d_ff=2048]                               \n",
            "INFO:tensorflow:Variable decoder/block_005/layer_002/DenseReluDense/wo/kernel         size 1048576      slice_size 1048576      Shape[d_ff=2048, d_model=512]                               \n",
            "INFO:tensorflow:Variable encoder/block_000/layer_000/SelfAttention/k                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable encoder/block_000/layer_000/SelfAttention/o                  size 262144       slice_size 262144       Shape[heads=512, d_model=512]                               \n",
            "INFO:tensorflow:Variable encoder/block_000/layer_000/SelfAttention/q                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable encoder/block_000/layer_000/SelfAttention/v                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable encoder/block_000/layer_001/DenseReluDense/wi/kernel         size 1048576      slice_size 1048576      Shape[d_model=512, d_ff=2048]                               \n",
            "INFO:tensorflow:Variable encoder/block_000/layer_001/DenseReluDense/wo/kernel         size 1048576      slice_size 1048576      Shape[d_ff=2048, d_model=512]                               \n",
            "INFO:tensorflow:Variable encoder/block_001/layer_000/SelfAttention/k                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable encoder/block_001/layer_000/SelfAttention/o                  size 262144       slice_size 262144       Shape[heads=512, d_model=512]                               \n",
            "INFO:tensorflow:Variable encoder/block_001/layer_000/SelfAttention/q                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable encoder/block_001/layer_000/SelfAttention/v                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable encoder/block_001/layer_001/DenseReluDense/wi/kernel         size 1048576      slice_size 1048576      Shape[d_model=512, d_ff=2048]                               \n",
            "INFO:tensorflow:Variable encoder/block_001/layer_001/DenseReluDense/wo/kernel         size 1048576      slice_size 1048576      Shape[d_ff=2048, d_model=512]                               \n",
            "INFO:tensorflow:Variable encoder/block_002/layer_000/SelfAttention/k                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable encoder/block_002/layer_000/SelfAttention/o                  size 262144       slice_size 262144       Shape[heads=512, d_model=512]                               \n",
            "INFO:tensorflow:Variable encoder/block_002/layer_000/SelfAttention/q                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable encoder/block_002/layer_000/SelfAttention/v                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable encoder/block_002/layer_001/DenseReluDense/wi/kernel         size 1048576      slice_size 1048576      Shape[d_model=512, d_ff=2048]                               \n",
            "INFO:tensorflow:Variable encoder/block_002/layer_001/DenseReluDense/wo/kernel         size 1048576      slice_size 1048576      Shape[d_ff=2048, d_model=512]                               \n",
            "INFO:tensorflow:Variable encoder/block_003/layer_000/SelfAttention/k                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable encoder/block_003/layer_000/SelfAttention/o                  size 262144       slice_size 262144       Shape[heads=512, d_model=512]                               \n",
            "INFO:tensorflow:Variable encoder/block_003/layer_000/SelfAttention/q                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable encoder/block_003/layer_000/SelfAttention/v                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable encoder/block_003/layer_001/DenseReluDense/wi/kernel         size 1048576      slice_size 1048576      Shape[d_model=512, d_ff=2048]                               \n",
            "INFO:tensorflow:Variable encoder/block_003/layer_001/DenseReluDense/wo/kernel         size 1048576      slice_size 1048576      Shape[d_ff=2048, d_model=512]                               \n",
            "INFO:tensorflow:Variable encoder/block_004/layer_000/SelfAttention/k                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable encoder/block_004/layer_000/SelfAttention/o                  size 262144       slice_size 262144       Shape[heads=512, d_model=512]                               \n",
            "INFO:tensorflow:Variable encoder/block_004/layer_000/SelfAttention/q                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable encoder/block_004/layer_000/SelfAttention/v                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable encoder/block_004/layer_001/DenseReluDense/wi/kernel         size 1048576      slice_size 1048576      Shape[d_model=512, d_ff=2048]                               \n",
            "INFO:tensorflow:Variable encoder/block_004/layer_001/DenseReluDense/wo/kernel         size 1048576      slice_size 1048576      Shape[d_ff=2048, d_model=512]                               \n",
            "INFO:tensorflow:Variable encoder/block_005/layer_000/SelfAttention/k                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable encoder/block_005/layer_000/SelfAttention/o                  size 262144       slice_size 262144       Shape[heads=512, d_model=512]                               \n",
            "INFO:tensorflow:Variable encoder/block_005/layer_000/SelfAttention/q                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable encoder/block_005/layer_000/SelfAttention/v                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable encoder/block_005/layer_001/DenseReluDense/wi/kernel         size 1048576      slice_size 1048576      Shape[d_model=512, d_ff=2048]                               \n",
            "INFO:tensorflow:Variable encoder/block_005/layer_001/DenseReluDense/wo/kernel         size 1048576      slice_size 1048576      Shape[d_ff=2048, d_model=512]                               \n",
            "INFO:tensorflow:Variable shared/embedding                                             size 16449536     slice_size 16449536     Shape[vocab=32128, d_model=512]                             \n",
            "INFO:tensorflow:Variable stacked/encoder/block_000/layer_000/SelfAttention/relative_attention_bias size 512          slice_size 512          Shape[stacked=2, heads=8, buckets=32]                       \n",
            "INFO:tensorflow:    encoder/block_000/layer_000/SelfAttention/relative_attention_bias\n",
            "INFO:tensorflow:    decoder/block_000/layer_000/SelfAttention/relative_attention_bias\n",
            "INFO:tensorflow:Variable stacked/encoder/block_000/layer_000/layer_norm/scale         size 16384        slice_size 16384        Shape[stacked=32, d_model=512]                              \n",
            "INFO:tensorflow:    encoder/block_000/layer_000/layer_norm/scale\n",
            "INFO:tensorflow:    encoder/block_000/layer_001/layer_norm/scale\n",
            "INFO:tensorflow:    encoder/block_001/layer_000/layer_norm/scale\n",
            "INFO:tensorflow:    encoder/block_001/layer_001/layer_norm/scale\n",
            "INFO:tensorflow:    encoder/block_002/layer_000/layer_norm/scale\n",
            "INFO:tensorflow:    encoder/block_002/layer_001/layer_norm/scale\n",
            "INFO:tensorflow:    encoder/block_003/layer_000/layer_norm/scale\n",
            "INFO:tensorflow:    encoder/block_003/layer_001/layer_norm/scale\n",
            "INFO:tensorflow:    encoder/block_004/layer_000/layer_norm/scale\n",
            "INFO:tensorflow:    encoder/block_004/layer_001/layer_norm/scale\n",
            "INFO:tensorflow:    encoder/block_005/layer_000/layer_norm/scale\n",
            "INFO:tensorflow:    encoder/block_005/layer_001/layer_norm/scale\n",
            "INFO:tensorflow:    encoder/final_layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_000/layer_000/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_000/layer_001/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_000/layer_002/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_001/layer_000/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_001/layer_001/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_001/layer_002/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_002/layer_000/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_002/layer_001/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_002/layer_002/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_003/layer_000/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_003/layer_001/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_003/layer_002/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_004/layer_000/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_004/layer_001/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_004/layer_002/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_005/layer_000/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_005/layer_001/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_005/layer_002/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/final_layer_norm/scale\n",
            "INFO:tensorflow:Trainable Variables            count: 99      Total size: 60506624         Total slice_size: 60506624       \n",
            "INFO:tensorflow:All Variables                  count: 99      Total size: 60506624         Total slice_size: 60506624       \n",
            "INFO:tensorflow:Counters:\n",
            "allconcat: 8.39e+06\n",
            " allconcat/0: 8.39e+06\n",
            "  allconcat/0/reshape_op: 8.39e+06\n",
            "allreduce: 8\n",
            " allreduce/[0]: 8\n",
            "  allreduce/[0]/reduce_op: 8\n",
            "einsum: 5.4e+13\n",
            "einsum_unique: 5.4e+13\n",
            "output: 5.35e+11\n",
            " output/AddOperation: 1.37e+11\n",
            " output/BinaryOpWithBroadcasting: 1.67e+09\n",
            " output/Constant: 3.22e+09\n",
            " output/EinsumOperation: 1.22e+11\n",
            " output/ImportOperation: 4.19e+06\n",
            " output/MinMaxOperation: 3.83e+07\n",
            " output/OneHotOperation: 3.45e+10\n",
            " output/RangeOperation: 8.19e+03\n",
            " output/ReduceOperation: 1.68e+08\n",
            " output/ReshapeOperation: 2.77e+10\n",
            " output/ScalarAddOperation: 6.71e+07\n",
            " output/ScalarMultiplyOperation: 2e+09\n",
            " output/ShiftOperation: 5.24e+05\n",
            " output/SlicewiseOperation: 1.64e+11\n",
            " output/StackedVariable: 1.35e+05\n",
            " output/StopGradient: 3.87e+10\n",
            " output/UnstackOperation: 1.35e+05\n",
            " output/Variable: 4.84e+08\n",
            " output/WhileLoopOperation: 3.22e+09\n",
            "output_unique: 5.33e+11\n",
            " output_unique/AddOperation: 1.37e+11\n",
            " output_unique/BinaryOpWithBroadcasting: 1.63e+09\n",
            " output_unique/Constant: 3.22e+09\n",
            " output_unique/EinsumOperation: 1.22e+11\n",
            " output_unique/ImportOperation: 5.24e+05\n",
            " output_unique/MinMaxOperation: 5.24e+06\n",
            " output_unique/OneHotOperation: 3.38e+10\n",
            " output_unique/RangeOperation: 1.02e+03\n",
            " output_unique/ReduceOperation: 1.68e+08\n",
            " output_unique/ReshapeOperation: 2.77e+10\n",
            " output_unique/ScalarAddOperation: 2.31e+07\n",
            " output_unique/ScalarMultiplyOperation: 1.91e+09\n",
            " output_unique/ShiftOperation: 5.24e+05\n",
            " output_unique/SlicewiseOperation: 1.64e+11\n",
            " output_unique/StackedVariable: 1.69e+04\n",
            " output_unique/StopGradient: 3.87e+10\n",
            " output_unique/UnstackOperation: 1.69e+04\n",
            " output_unique/Variable: 6.05e+07\n",
            " output_unique/WhileLoopOperation: 3.22e+09\n",
            "variables: 6.05e+07\n",
            " variables/trainable: 6.05e+07\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:TPU job name worker\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from gs://code_review_automation/replication_package/fine_tuning_model_dumps/all_small_v1/model.ckpt-700\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Starting infeed thread controller.\n",
            "INFO:tensorflow:Starting outfeed thread controller.\n",
            "INFO:tensorflow:Initialized dataset iterators in 0 seconds\n",
            "INFO:tensorflow:Before copy master to slices.\n",
            "INFO:tensorflow:Done with copy master to slices.\n",
            "INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (0, 0)\n",
            "INFO:tensorflow:decoded 0: code2code: \"Hello, World!\"\n",
            "INFO:tensorflow:            -> <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "INFO:tensorflow:decoded 1: code2code: \"Hello, World!\"\n",
            "INFO:tensorflow:            -> <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "INFO:tensorflow:decoded 2: code2code: \"Hello, World!\"\n",
            "INFO:tensorflow:            -> <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "INFO:tensorflow:decoded 4: code2code: \"Hello, World!\"\n",
            "INFO:tensorflow:            -> <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "INFO:tensorflow:decoded 8: code2code: \"Hello, World!\"\n",
            "INFO:tensorflow:            -> <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "INFO:tensorflow:decoded 16: code2code: \"Hello, World!\"\n",
            "INFO:tensorflow:            -> <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "INFO:tensorflow:decoded 32: code2code: \"Hello, World!\"\n",
            "INFO:tensorflow:            -> <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "INFO:tensorflow:decoded 64: code2code: \"Hello, World!\"\n",
            "INFO:tensorflow:            -> <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "INFO:tensorflow:decoded 128: code2code: \"Hello, World!\"\n",
            "INFO:tensorflow:            -> <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "INFO:tensorflow:decoded 256: code2code: \"Hello, World!\"\n",
            "INFO:tensorflow:            -> <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "INFO:tensorflow:decoded 512: code2code: \"Hello, World!\"\n",
            "INFO:tensorflow:            -> <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Stop infeed thread controller\n",
            "INFO:tensorflow:Shutting down InfeedController thread.\n",
            "INFO:tensorflow:InfeedController received shutdown signal, stopping.\n",
            "INFO:tensorflow:Infeed thread finished, shutting down.\n",
            "INFO:tensorflow:infeed marked as finished\n",
            "INFO:tensorflow:Stop output thread controller\n",
            "INFO:tensorflow:Shutting down OutfeedController thread.\n",
            "INFO:tensorflow:OutfeedController received shutdown signal, stopping.\n",
            "INFO:tensorflow:Outfeed thread finished, shutting down.\n",
            "INFO:tensorflow:outfeed marked as finished\n",
            "INFO:tensorflow:Shutdown TPU system.\n",
            "INFO:tensorflow:prediction_loop marked as finished\n",
            "INFO:tensorflow:prediction_loop marked as finished\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
