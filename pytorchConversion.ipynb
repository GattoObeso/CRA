{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pytorchConversion.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPtC/lQLsS531cWNPgwZSXz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/masies/CRA/blob/main/pytorchConversion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8i-CsM5qG8P4",
        "outputId": "2efe681c-f3f1-47c4-b37a-0d7661bd005e"
      },
      "source": [
        "import tensorflow as tf\n",
        "import torch\n",
        "!pip3 install transformers\n",
        "import transformers\n",
        "!pip install sentencepiece\n",
        "import sentencepiece as spm\n",
        "\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "project_id = 'helical-loop-303918'\n",
        "bucket_name = 'code_review_automation'\n",
        "\n",
        "!gcloud config set project {project_id}\n",
        "!gsutil ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.4.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.95)\n",
            "Updated property [core/project].\n",
            "gs://code_review_automation/\n",
            "gs://t5tutorial1/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-xKBubBHvGD",
        "outputId": "03ad44d9-bd81-451e-e642-9cf93a233713"
      },
      "source": [
        "!gsutil cp gs://{bucket_name}/pyTorch_coversion/code/tf_2_pytorch_T5.py ./tf_2_pytorch_T5.py\n",
        "\n",
        "!mkdir models\n",
        "!mkdir dumps\n",
        "\n",
        "!gsutil cp gs://{bucket_name}/model_dumps/model.ckpt-200000.index ./models/model.ckpt-200000.index\n",
        "!gsutil cp gs://{bucket_name}/model_dumps/model.ckpt-200000.meta ./models/model.ckpt-200000.meta\n",
        "!gsutil cp gs://{bucket_name}/model_dumps/model.ckpt-200000.data-00000-of-00002 ./models/model.ckpt-200000.data-00000-of-00002\n",
        "!gsutil cp gs://{bucket_name}/model_dumps/model.ckpt-200000.data-00001-of-00002 ./models/model.ckpt-200000.data-00001-of-00002\n",
        "\n",
        "!gsutil cp gs://{bucket_name}/pyTorch_coversion/config/config.json ./config.json"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying gs://code_review_automation/pyTorch_coversion/code/tf_2_pytorch_T5.py...\n",
            "/ [1 files][  2.3 KiB/  2.3 KiB]                                                \n",
            "Operation completed over 1 objects/2.3 KiB.                                      \n",
            "mkdir: cannot create directory ‘models’: File exists\n",
            "mkdir: cannot create directory ‘dumps’: File exists\n",
            "Copying gs://code_review_automation/model_dumps/model.ckpt-200000.index...\n",
            "/ [1 files][  5.5 KiB/  5.5 KiB]                                                \n",
            "Operation completed over 1 objects/5.5 KiB.                                      \n",
            "Copying gs://code_review_automation/model_dumps/model.ckpt-200000.meta...\n",
            "- [1 files][  9.5 MiB/  9.5 MiB]                                                \n",
            "Operation completed over 1 objects/9.5 MiB.                                      \n",
            "Copying gs://code_review_automation/model_dumps/model.ckpt-200000.data-00000-of-00002...\n",
            "/ [1 files][    8.0 B/    8.0 B]                                                \n",
            "Operation completed over 1 objects/8.0 B.                                        \n",
            "Copying gs://code_review_automation/model_dumps/model.ckpt-200000.data-00001-of-00002...\n",
            "/ [1 files][116.1 MiB/116.1 MiB]                                                \n",
            "Operation completed over 1 objects/116.1 MiB.                                    \n",
            "Copying gs://code_review_automation/pyTorch_coversion/config/config.json...\n",
            "/ [1 files][  463.0 B/  463.0 B]                                                \n",
            "Operation completed over 1 objects/463.0 B.                                      \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V3DnUlPWVyDa",
        "outputId": "b07eb08f-6a9a-450e-fc7a-6e792d760a5c"
      },
      "source": [
        "!python3 ./tf_2_pytorch_T5.py --tf_checkpoint_path ./models/model.ckpt-200000 --config_file ./config.json --pytorch_dump_path ./dumps"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building PyTorch model from configuration: T5Config {\n",
            "  \"architectures\": [\n",
            "    \"T5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 512,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"relu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"n_positions\": 512,\n",
            "  \"num_decoder_layers\": 6,\n",
            "  \"num_heads\": 8,\n",
            "  \"num_layers\": 6,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"transformers_version\": \"4.4.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "2021-03-17 11:22:50.309715: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "Converting TensorFlow checkpoint from /content/models/model.ckpt-200000\n",
            "Loading TF weight decoder/block_000/layer_000/SelfAttention/k with shape [512, 512]\n",
            "Loading TF weight decoder/block_000/layer_000/SelfAttention/k_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_000/layer_000/SelfAttention/k_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_000/layer_000/SelfAttention/o with shape [512, 512]\n",
            "Loading TF weight decoder/block_000/layer_000/SelfAttention/o_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_000/layer_000/SelfAttention/o_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_000/layer_000/SelfAttention/q with shape [512, 512]\n",
            "Loading TF weight decoder/block_000/layer_000/SelfAttention/q_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_000/layer_000/SelfAttention/q_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_000/layer_000/SelfAttention/relative_attention_bias with shape [8, 32]\n",
            "Loading TF weight decoder/block_000/layer_000/SelfAttention/relative_attention_bias_slot_v with shape [8, 32]\n",
            "Loading TF weight decoder/block_000/layer_000/SelfAttention/v with shape [512, 512]\n",
            "Loading TF weight decoder/block_000/layer_000/SelfAttention/v_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_000/layer_000/SelfAttention/v_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_000/layer_000/layer_norm/scale with shape [512]\n",
            "Loading TF weight decoder/block_000/layer_000/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight decoder/block_000/layer_001/EncDecAttention/k with shape [512, 512]\n",
            "Loading TF weight decoder/block_000/layer_001/EncDecAttention/k_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_000/layer_001/EncDecAttention/k_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_000/layer_001/EncDecAttention/o with shape [512, 512]\n",
            "Loading TF weight decoder/block_000/layer_001/EncDecAttention/o_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_000/layer_001/EncDecAttention/o_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_000/layer_001/EncDecAttention/q with shape [512, 512]\n",
            "Loading TF weight decoder/block_000/layer_001/EncDecAttention/q_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_000/layer_001/EncDecAttention/q_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_000/layer_001/EncDecAttention/v with shape [512, 512]\n",
            "Loading TF weight decoder/block_000/layer_001/EncDecAttention/v_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_000/layer_001/EncDecAttention/v_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_000/layer_001/layer_norm/scale with shape [512]\n",
            "Loading TF weight decoder/block_000/layer_001/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight decoder/block_000/layer_002/DenseReluDense/wi/kernel with shape [512, 2048]\n",
            "Loading TF weight decoder/block_000/layer_002/DenseReluDense/wi/kernel_slot_vc with shape [2048]\n",
            "Loading TF weight decoder/block_000/layer_002/DenseReluDense/wi/kernel_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_000/layer_002/DenseReluDense/wo/kernel with shape [2048, 512]\n",
            "Loading TF weight decoder/block_000/layer_002/DenseReluDense/wo/kernel_slot_vc with shape [2048]\n",
            "Loading TF weight decoder/block_000/layer_002/DenseReluDense/wo/kernel_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_000/layer_002/layer_norm/scale with shape [512]\n",
            "Loading TF weight decoder/block_000/layer_002/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight decoder/block_001/layer_000/SelfAttention/k with shape [512, 512]\n",
            "Loading TF weight decoder/block_001/layer_000/SelfAttention/k_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_001/layer_000/SelfAttention/k_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_001/layer_000/SelfAttention/o with shape [512, 512]\n",
            "Loading TF weight decoder/block_001/layer_000/SelfAttention/o_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_001/layer_000/SelfAttention/o_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_001/layer_000/SelfAttention/q with shape [512, 512]\n",
            "Loading TF weight decoder/block_001/layer_000/SelfAttention/q_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_001/layer_000/SelfAttention/q_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_001/layer_000/SelfAttention/v with shape [512, 512]\n",
            "Loading TF weight decoder/block_001/layer_000/SelfAttention/v_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_001/layer_000/SelfAttention/v_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_001/layer_000/layer_norm/scale with shape [512]\n",
            "Loading TF weight decoder/block_001/layer_000/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight decoder/block_001/layer_001/EncDecAttention/k with shape [512, 512]\n",
            "Loading TF weight decoder/block_001/layer_001/EncDecAttention/k_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_001/layer_001/EncDecAttention/k_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_001/layer_001/EncDecAttention/o with shape [512, 512]\n",
            "Loading TF weight decoder/block_001/layer_001/EncDecAttention/o_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_001/layer_001/EncDecAttention/o_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_001/layer_001/EncDecAttention/q with shape [512, 512]\n",
            "Loading TF weight decoder/block_001/layer_001/EncDecAttention/q_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_001/layer_001/EncDecAttention/q_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_001/layer_001/EncDecAttention/v with shape [512, 512]\n",
            "Loading TF weight decoder/block_001/layer_001/EncDecAttention/v_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_001/layer_001/EncDecAttention/v_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_001/layer_001/layer_norm/scale with shape [512]\n",
            "Loading TF weight decoder/block_001/layer_001/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight decoder/block_001/layer_002/DenseReluDense/wi/kernel with shape [512, 2048]\n",
            "Loading TF weight decoder/block_001/layer_002/DenseReluDense/wi/kernel_slot_vc with shape [2048]\n",
            "Loading TF weight decoder/block_001/layer_002/DenseReluDense/wi/kernel_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_001/layer_002/DenseReluDense/wo/kernel with shape [2048, 512]\n",
            "Loading TF weight decoder/block_001/layer_002/DenseReluDense/wo/kernel_slot_vc with shape [2048]\n",
            "Loading TF weight decoder/block_001/layer_002/DenseReluDense/wo/kernel_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_001/layer_002/layer_norm/scale with shape [512]\n",
            "Loading TF weight decoder/block_001/layer_002/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight decoder/block_002/layer_000/SelfAttention/k with shape [512, 512]\n",
            "Loading TF weight decoder/block_002/layer_000/SelfAttention/k_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_002/layer_000/SelfAttention/k_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_002/layer_000/SelfAttention/o with shape [512, 512]\n",
            "Loading TF weight decoder/block_002/layer_000/SelfAttention/o_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_002/layer_000/SelfAttention/o_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_002/layer_000/SelfAttention/q with shape [512, 512]\n",
            "Loading TF weight decoder/block_002/layer_000/SelfAttention/q_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_002/layer_000/SelfAttention/q_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_002/layer_000/SelfAttention/v with shape [512, 512]\n",
            "Loading TF weight decoder/block_002/layer_000/SelfAttention/v_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_002/layer_000/SelfAttention/v_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_002/layer_000/layer_norm/scale with shape [512]\n",
            "Loading TF weight decoder/block_002/layer_000/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight decoder/block_002/layer_001/EncDecAttention/k with shape [512, 512]\n",
            "Loading TF weight decoder/block_002/layer_001/EncDecAttention/k_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_002/layer_001/EncDecAttention/k_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_002/layer_001/EncDecAttention/o with shape [512, 512]\n",
            "Loading TF weight decoder/block_002/layer_001/EncDecAttention/o_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_002/layer_001/EncDecAttention/o_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_002/layer_001/EncDecAttention/q with shape [512, 512]\n",
            "Loading TF weight decoder/block_002/layer_001/EncDecAttention/q_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_002/layer_001/EncDecAttention/q_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_002/layer_001/EncDecAttention/v with shape [512, 512]\n",
            "Loading TF weight decoder/block_002/layer_001/EncDecAttention/v_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_002/layer_001/EncDecAttention/v_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_002/layer_001/layer_norm/scale with shape [512]\n",
            "Loading TF weight decoder/block_002/layer_001/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight decoder/block_002/layer_002/DenseReluDense/wi/kernel with shape [512, 2048]\n",
            "Loading TF weight decoder/block_002/layer_002/DenseReluDense/wi/kernel_slot_vc with shape [2048]\n",
            "Loading TF weight decoder/block_002/layer_002/DenseReluDense/wi/kernel_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_002/layer_002/DenseReluDense/wo/kernel with shape [2048, 512]\n",
            "Loading TF weight decoder/block_002/layer_002/DenseReluDense/wo/kernel_slot_vc with shape [2048]\n",
            "Loading TF weight decoder/block_002/layer_002/DenseReluDense/wo/kernel_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_002/layer_002/layer_norm/scale with shape [512]\n",
            "Loading TF weight decoder/block_002/layer_002/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight decoder/block_003/layer_000/SelfAttention/k with shape [512, 512]\n",
            "Loading TF weight decoder/block_003/layer_000/SelfAttention/k_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_003/layer_000/SelfAttention/k_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_003/layer_000/SelfAttention/o with shape [512, 512]\n",
            "Loading TF weight decoder/block_003/layer_000/SelfAttention/o_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_003/layer_000/SelfAttention/o_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_003/layer_000/SelfAttention/q with shape [512, 512]\n",
            "Loading TF weight decoder/block_003/layer_000/SelfAttention/q_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_003/layer_000/SelfAttention/q_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_003/layer_000/SelfAttention/v with shape [512, 512]\n",
            "Loading TF weight decoder/block_003/layer_000/SelfAttention/v_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_003/layer_000/SelfAttention/v_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_003/layer_000/layer_norm/scale with shape [512]\n",
            "Loading TF weight decoder/block_003/layer_000/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight decoder/block_003/layer_001/EncDecAttention/k with shape [512, 512]\n",
            "Loading TF weight decoder/block_003/layer_001/EncDecAttention/k_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_003/layer_001/EncDecAttention/k_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_003/layer_001/EncDecAttention/o with shape [512, 512]\n",
            "Loading TF weight decoder/block_003/layer_001/EncDecAttention/o_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_003/layer_001/EncDecAttention/o_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_003/layer_001/EncDecAttention/q with shape [512, 512]\n",
            "Loading TF weight decoder/block_003/layer_001/EncDecAttention/q_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_003/layer_001/EncDecAttention/q_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_003/layer_001/EncDecAttention/v with shape [512, 512]\n",
            "Loading TF weight decoder/block_003/layer_001/EncDecAttention/v_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_003/layer_001/EncDecAttention/v_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_003/layer_001/layer_norm/scale with shape [512]\n",
            "Loading TF weight decoder/block_003/layer_001/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight decoder/block_003/layer_002/DenseReluDense/wi/kernel with shape [512, 2048]\n",
            "Loading TF weight decoder/block_003/layer_002/DenseReluDense/wi/kernel_slot_vc with shape [2048]\n",
            "Loading TF weight decoder/block_003/layer_002/DenseReluDense/wi/kernel_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_003/layer_002/DenseReluDense/wo/kernel with shape [2048, 512]\n",
            "Loading TF weight decoder/block_003/layer_002/DenseReluDense/wo/kernel_slot_vc with shape [2048]\n",
            "Loading TF weight decoder/block_003/layer_002/DenseReluDense/wo/kernel_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_003/layer_002/layer_norm/scale with shape [512]\n",
            "Loading TF weight decoder/block_003/layer_002/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight decoder/block_004/layer_000/SelfAttention/k with shape [512, 512]\n",
            "Loading TF weight decoder/block_004/layer_000/SelfAttention/k_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_004/layer_000/SelfAttention/k_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_004/layer_000/SelfAttention/o with shape [512, 512]\n",
            "Loading TF weight decoder/block_004/layer_000/SelfAttention/o_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_004/layer_000/SelfAttention/o_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_004/layer_000/SelfAttention/q with shape [512, 512]\n",
            "Loading TF weight decoder/block_004/layer_000/SelfAttention/q_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_004/layer_000/SelfAttention/q_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_004/layer_000/SelfAttention/v with shape [512, 512]\n",
            "Loading TF weight decoder/block_004/layer_000/SelfAttention/v_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_004/layer_000/SelfAttention/v_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_004/layer_000/layer_norm/scale with shape [512]\n",
            "Loading TF weight decoder/block_004/layer_000/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight decoder/block_004/layer_001/EncDecAttention/k with shape [512, 512]\n",
            "Loading TF weight decoder/block_004/layer_001/EncDecAttention/k_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_004/layer_001/EncDecAttention/k_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_004/layer_001/EncDecAttention/o with shape [512, 512]\n",
            "Loading TF weight decoder/block_004/layer_001/EncDecAttention/o_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_004/layer_001/EncDecAttention/o_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_004/layer_001/EncDecAttention/q with shape [512, 512]\n",
            "Loading TF weight decoder/block_004/layer_001/EncDecAttention/q_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_004/layer_001/EncDecAttention/q_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_004/layer_001/EncDecAttention/v with shape [512, 512]\n",
            "Loading TF weight decoder/block_004/layer_001/EncDecAttention/v_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_004/layer_001/EncDecAttention/v_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_004/layer_001/layer_norm/scale with shape [512]\n",
            "Loading TF weight decoder/block_004/layer_001/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight decoder/block_004/layer_002/DenseReluDense/wi/kernel with shape [512, 2048]\n",
            "Loading TF weight decoder/block_004/layer_002/DenseReluDense/wi/kernel_slot_vc with shape [2048]\n",
            "Loading TF weight decoder/block_004/layer_002/DenseReluDense/wi/kernel_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_004/layer_002/DenseReluDense/wo/kernel with shape [2048, 512]\n",
            "Loading TF weight decoder/block_004/layer_002/DenseReluDense/wo/kernel_slot_vc with shape [2048]\n",
            "Loading TF weight decoder/block_004/layer_002/DenseReluDense/wo/kernel_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_004/layer_002/layer_norm/scale with shape [512]\n",
            "Loading TF weight decoder/block_004/layer_002/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight decoder/block_005/layer_000/SelfAttention/k with shape [512, 512]\n",
            "Loading TF weight decoder/block_005/layer_000/SelfAttention/k_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_005/layer_000/SelfAttention/k_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_005/layer_000/SelfAttention/o with shape [512, 512]\n",
            "Loading TF weight decoder/block_005/layer_000/SelfAttention/o_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_005/layer_000/SelfAttention/o_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_005/layer_000/SelfAttention/q with shape [512, 512]\n",
            "Loading TF weight decoder/block_005/layer_000/SelfAttention/q_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_005/layer_000/SelfAttention/q_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_005/layer_000/SelfAttention/v with shape [512, 512]\n",
            "Loading TF weight decoder/block_005/layer_000/SelfAttention/v_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_005/layer_000/SelfAttention/v_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_005/layer_000/layer_norm/scale with shape [512]\n",
            "Loading TF weight decoder/block_005/layer_000/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight decoder/block_005/layer_001/EncDecAttention/k with shape [512, 512]\n",
            "Loading TF weight decoder/block_005/layer_001/EncDecAttention/k_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_005/layer_001/EncDecAttention/k_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_005/layer_001/EncDecAttention/o with shape [512, 512]\n",
            "Loading TF weight decoder/block_005/layer_001/EncDecAttention/o_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_005/layer_001/EncDecAttention/o_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_005/layer_001/EncDecAttention/q with shape [512, 512]\n",
            "Loading TF weight decoder/block_005/layer_001/EncDecAttention/q_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_005/layer_001/EncDecAttention/q_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_005/layer_001/EncDecAttention/v with shape [512, 512]\n",
            "Loading TF weight decoder/block_005/layer_001/EncDecAttention/v_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_005/layer_001/EncDecAttention/v_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_005/layer_001/layer_norm/scale with shape [512]\n",
            "Loading TF weight decoder/block_005/layer_001/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight decoder/block_005/layer_002/DenseReluDense/wi/kernel with shape [512, 2048]\n",
            "Loading TF weight decoder/block_005/layer_002/DenseReluDense/wi/kernel_slot_vc with shape [2048]\n",
            "Loading TF weight decoder/block_005/layer_002/DenseReluDense/wi/kernel_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_005/layer_002/DenseReluDense/wo/kernel with shape [2048, 512]\n",
            "Loading TF weight decoder/block_005/layer_002/DenseReluDense/wo/kernel_slot_vc with shape [2048]\n",
            "Loading TF weight decoder/block_005/layer_002/DenseReluDense/wo/kernel_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_005/layer_002/layer_norm/scale with shape [512]\n",
            "Loading TF weight decoder/block_005/layer_002/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight decoder/final_layer_norm/scale with shape [512]\n",
            "Loading TF weight decoder/final_layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight encoder/block_000/layer_000/SelfAttention/k with shape [512, 512]\n",
            "Loading TF weight encoder/block_000/layer_000/SelfAttention/k_slot_vc with shape [512]\n",
            "Loading TF weight encoder/block_000/layer_000/SelfAttention/k_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_000/layer_000/SelfAttention/o with shape [512, 512]\n",
            "Loading TF weight encoder/block_000/layer_000/SelfAttention/o_slot_vc with shape [512]\n",
            "Loading TF weight encoder/block_000/layer_000/SelfAttention/o_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_000/layer_000/SelfAttention/q with shape [512, 512]\n",
            "Loading TF weight encoder/block_000/layer_000/SelfAttention/q_slot_vc with shape [512]\n",
            "Loading TF weight encoder/block_000/layer_000/SelfAttention/q_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_000/layer_000/SelfAttention/relative_attention_bias with shape [8, 32]\n",
            "Loading TF weight encoder/block_000/layer_000/SelfAttention/relative_attention_bias_slot_v with shape [8, 32]\n",
            "Loading TF weight encoder/block_000/layer_000/SelfAttention/v with shape [512, 512]\n",
            "Loading TF weight encoder/block_000/layer_000/SelfAttention/v_slot_vc with shape [512]\n",
            "Loading TF weight encoder/block_000/layer_000/SelfAttention/v_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_000/layer_000/layer_norm/scale with shape [512]\n",
            "Loading TF weight encoder/block_000/layer_000/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight encoder/block_000/layer_001/DenseReluDense/wi/kernel with shape [512, 2048]\n",
            "Loading TF weight encoder/block_000/layer_001/DenseReluDense/wi/kernel_slot_vc with shape [2048]\n",
            "Loading TF weight encoder/block_000/layer_001/DenseReluDense/wi/kernel_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_000/layer_001/DenseReluDense/wo/kernel with shape [2048, 512]\n",
            "Loading TF weight encoder/block_000/layer_001/DenseReluDense/wo/kernel_slot_vc with shape [2048]\n",
            "Loading TF weight encoder/block_000/layer_001/DenseReluDense/wo/kernel_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_000/layer_001/layer_norm/scale with shape [512]\n",
            "Loading TF weight encoder/block_000/layer_001/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight encoder/block_001/layer_000/SelfAttention/k with shape [512, 512]\n",
            "Loading TF weight encoder/block_001/layer_000/SelfAttention/k_slot_vc with shape [512]\n",
            "Loading TF weight encoder/block_001/layer_000/SelfAttention/k_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_001/layer_000/SelfAttention/o with shape [512, 512]\n",
            "Loading TF weight encoder/block_001/layer_000/SelfAttention/o_slot_vc with shape [512]\n",
            "Loading TF weight encoder/block_001/layer_000/SelfAttention/o_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_001/layer_000/SelfAttention/q with shape [512, 512]\n",
            "Loading TF weight encoder/block_001/layer_000/SelfAttention/q_slot_vc with shape [512]\n",
            "Loading TF weight encoder/block_001/layer_000/SelfAttention/q_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_001/layer_000/SelfAttention/v with shape [512, 512]\n",
            "Loading TF weight encoder/block_001/layer_000/SelfAttention/v_slot_vc with shape [512]\n",
            "Loading TF weight encoder/block_001/layer_000/SelfAttention/v_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_001/layer_000/layer_norm/scale with shape [512]\n",
            "Loading TF weight encoder/block_001/layer_000/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight encoder/block_001/layer_001/DenseReluDense/wi/kernel with shape [512, 2048]\n",
            "Loading TF weight encoder/block_001/layer_001/DenseReluDense/wi/kernel_slot_vc with shape [2048]\n",
            "Loading TF weight encoder/block_001/layer_001/DenseReluDense/wi/kernel_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_001/layer_001/DenseReluDense/wo/kernel with shape [2048, 512]\n",
            "Loading TF weight encoder/block_001/layer_001/DenseReluDense/wo/kernel_slot_vc with shape [2048]\n",
            "Loading TF weight encoder/block_001/layer_001/DenseReluDense/wo/kernel_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_001/layer_001/layer_norm/scale with shape [512]\n",
            "Loading TF weight encoder/block_001/layer_001/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight encoder/block_002/layer_000/SelfAttention/k with shape [512, 512]\n",
            "Loading TF weight encoder/block_002/layer_000/SelfAttention/k_slot_vc with shape [512]\n",
            "Loading TF weight encoder/block_002/layer_000/SelfAttention/k_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_002/layer_000/SelfAttention/o with shape [512, 512]\n",
            "Loading TF weight encoder/block_002/layer_000/SelfAttention/o_slot_vc with shape [512]\n",
            "Loading TF weight encoder/block_002/layer_000/SelfAttention/o_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_002/layer_000/SelfAttention/q with shape [512, 512]\n",
            "Loading TF weight encoder/block_002/layer_000/SelfAttention/q_slot_vc with shape [512]\n",
            "Loading TF weight encoder/block_002/layer_000/SelfAttention/q_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_002/layer_000/SelfAttention/v with shape [512, 512]\n",
            "Loading TF weight encoder/block_002/layer_000/SelfAttention/v_slot_vc with shape [512]\n",
            "Loading TF weight encoder/block_002/layer_000/SelfAttention/v_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_002/layer_000/layer_norm/scale with shape [512]\n",
            "Loading TF weight encoder/block_002/layer_000/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight encoder/block_002/layer_001/DenseReluDense/wi/kernel with shape [512, 2048]\n",
            "Loading TF weight encoder/block_002/layer_001/DenseReluDense/wi/kernel_slot_vc with shape [2048]\n",
            "Loading TF weight encoder/block_002/layer_001/DenseReluDense/wi/kernel_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_002/layer_001/DenseReluDense/wo/kernel with shape [2048, 512]\n",
            "Loading TF weight encoder/block_002/layer_001/DenseReluDense/wo/kernel_slot_vc with shape [2048]\n",
            "Loading TF weight encoder/block_002/layer_001/DenseReluDense/wo/kernel_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_002/layer_001/layer_norm/scale with shape [512]\n",
            "Loading TF weight encoder/block_002/layer_001/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight encoder/block_003/layer_000/SelfAttention/k with shape [512, 512]\n",
            "Loading TF weight encoder/block_003/layer_000/SelfAttention/k_slot_vc with shape [512]\n",
            "Loading TF weight encoder/block_003/layer_000/SelfAttention/k_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_003/layer_000/SelfAttention/o with shape [512, 512]\n",
            "Loading TF weight encoder/block_003/layer_000/SelfAttention/o_slot_vc with shape [512]\n",
            "Loading TF weight encoder/block_003/layer_000/SelfAttention/o_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_003/layer_000/SelfAttention/q with shape [512, 512]\n",
            "Loading TF weight encoder/block_003/layer_000/SelfAttention/q_slot_vc with shape [512]\n",
            "Loading TF weight encoder/block_003/layer_000/SelfAttention/q_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_003/layer_000/SelfAttention/v with shape [512, 512]\n",
            "Loading TF weight encoder/block_003/layer_000/SelfAttention/v_slot_vc with shape [512]\n",
            "Loading TF weight encoder/block_003/layer_000/SelfAttention/v_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_003/layer_000/layer_norm/scale with shape [512]\n",
            "Loading TF weight encoder/block_003/layer_000/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight encoder/block_003/layer_001/DenseReluDense/wi/kernel with shape [512, 2048]\n",
            "Loading TF weight encoder/block_003/layer_001/DenseReluDense/wi/kernel_slot_vc with shape [2048]\n",
            "Loading TF weight encoder/block_003/layer_001/DenseReluDense/wi/kernel_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_003/layer_001/DenseReluDense/wo/kernel with shape [2048, 512]\n",
            "Loading TF weight encoder/block_003/layer_001/DenseReluDense/wo/kernel_slot_vc with shape [2048]\n",
            "Loading TF weight encoder/block_003/layer_001/DenseReluDense/wo/kernel_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_003/layer_001/layer_norm/scale with shape [512]\n",
            "Loading TF weight encoder/block_003/layer_001/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight encoder/block_004/layer_000/SelfAttention/k with shape [512, 512]\n",
            "Loading TF weight encoder/block_004/layer_000/SelfAttention/k_slot_vc with shape [512]\n",
            "Loading TF weight encoder/block_004/layer_000/SelfAttention/k_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_004/layer_000/SelfAttention/o with shape [512, 512]\n",
            "Loading TF weight encoder/block_004/layer_000/SelfAttention/o_slot_vc with shape [512]\n",
            "Loading TF weight encoder/block_004/layer_000/SelfAttention/o_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_004/layer_000/SelfAttention/q with shape [512, 512]\n",
            "Loading TF weight encoder/block_004/layer_000/SelfAttention/q_slot_vc with shape [512]\n",
            "Loading TF weight encoder/block_004/layer_000/SelfAttention/q_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_004/layer_000/SelfAttention/v with shape [512, 512]\n",
            "Loading TF weight encoder/block_004/layer_000/SelfAttention/v_slot_vc with shape [512]\n",
            "Loading TF weight encoder/block_004/layer_000/SelfAttention/v_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_004/layer_000/layer_norm/scale with shape [512]\n",
            "Loading TF weight encoder/block_004/layer_000/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight encoder/block_004/layer_001/DenseReluDense/wi/kernel with shape [512, 2048]\n",
            "Loading TF weight encoder/block_004/layer_001/DenseReluDense/wi/kernel_slot_vc with shape [2048]\n",
            "Loading TF weight encoder/block_004/layer_001/DenseReluDense/wi/kernel_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_004/layer_001/DenseReluDense/wo/kernel with shape [2048, 512]\n",
            "Loading TF weight encoder/block_004/layer_001/DenseReluDense/wo/kernel_slot_vc with shape [2048]\n",
            "Loading TF weight encoder/block_004/layer_001/DenseReluDense/wo/kernel_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_004/layer_001/layer_norm/scale with shape [512]\n",
            "Loading TF weight encoder/block_004/layer_001/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight encoder/block_005/layer_000/SelfAttention/k with shape [512, 512]\n",
            "Loading TF weight encoder/block_005/layer_000/SelfAttention/k_slot_vc with shape [512]\n",
            "Loading TF weight encoder/block_005/layer_000/SelfAttention/k_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_005/layer_000/SelfAttention/o with shape [512, 512]\n",
            "Loading TF weight encoder/block_005/layer_000/SelfAttention/o_slot_vc with shape [512]\n",
            "Loading TF weight encoder/block_005/layer_000/SelfAttention/o_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_005/layer_000/SelfAttention/q with shape [512, 512]\n",
            "Loading TF weight encoder/block_005/layer_000/SelfAttention/q_slot_vc with shape [512]\n",
            "Loading TF weight encoder/block_005/layer_000/SelfAttention/q_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_005/layer_000/SelfAttention/v with shape [512, 512]\n",
            "Loading TF weight encoder/block_005/layer_000/SelfAttention/v_slot_vc with shape [512]\n",
            "Loading TF weight encoder/block_005/layer_000/SelfAttention/v_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_005/layer_000/layer_norm/scale with shape [512]\n",
            "Loading TF weight encoder/block_005/layer_000/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight encoder/block_005/layer_001/DenseReluDense/wi/kernel with shape [512, 2048]\n",
            "Loading TF weight encoder/block_005/layer_001/DenseReluDense/wi/kernel_slot_vc with shape [2048]\n",
            "Loading TF weight encoder/block_005/layer_001/DenseReluDense/wi/kernel_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_005/layer_001/DenseReluDense/wo/kernel with shape [2048, 512]\n",
            "Loading TF weight encoder/block_005/layer_001/DenseReluDense/wo/kernel_slot_vc with shape [2048]\n",
            "Loading TF weight encoder/block_005/layer_001/DenseReluDense/wo/kernel_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_005/layer_001/layer_norm/scale with shape [512]\n",
            "Loading TF weight encoder/block_005/layer_001/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight encoder/final_layer_norm/scale with shape [512]\n",
            "Loading TF weight encoder/final_layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight global_step with shape []\n",
            "Loading TF weight shared/embedding with shape [32128, 512]\n",
            "Loading TF weight shared/embedding_slot_vc with shape [32128]\n",
            "Loading TF weight shared/embedding_slot_vr with shape [512]\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_000', 'layer_000', 'SelfAttention', 'k']\n",
            "Initialize PyTorch weight ['decoder', 'block_000', 'layer_000', 'SelfAttention', 'k']\n",
            "Skipping decoder/block_000/layer_000/SelfAttention/k_slot_vc\n",
            "Skipping decoder/block_000/layer_000/SelfAttention/k_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_000', 'layer_000', 'SelfAttention', 'o']\n",
            "Initialize PyTorch weight ['decoder', 'block_000', 'layer_000', 'SelfAttention', 'o']\n",
            "Skipping decoder/block_000/layer_000/SelfAttention/o_slot_vc\n",
            "Skipping decoder/block_000/layer_000/SelfAttention/o_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_000', 'layer_000', 'SelfAttention', 'q']\n",
            "Initialize PyTorch weight ['decoder', 'block_000', 'layer_000', 'SelfAttention', 'q']\n",
            "Skipping decoder/block_000/layer_000/SelfAttention/q_slot_vc\n",
            "Skipping decoder/block_000/layer_000/SelfAttention/q_slot_vr\n",
            "Transposing numpy weight of shape (8, 32) for ['decoder', 'block_000', 'layer_000', 'SelfAttention', 'relative_attention_bias']\n",
            "Initialize PyTorch weight ['decoder', 'block_000', 'layer_000', 'SelfAttention', 'relative_attention_bias']\n",
            "Skipping decoder/block_000/layer_000/SelfAttention/relative_attention_bias_slot_v\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_000', 'layer_000', 'SelfAttention', 'v']\n",
            "Initialize PyTorch weight ['decoder', 'block_000', 'layer_000', 'SelfAttention', 'v']\n",
            "Skipping decoder/block_000/layer_000/SelfAttention/v_slot_vc\n",
            "Skipping decoder/block_000/layer_000/SelfAttention/v_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['decoder', 'block_000', 'layer_000', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['decoder', 'block_000', 'layer_000', 'layer_norm', 'scale']\n",
            "Skipping decoder/block_000/layer_000/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_000', 'layer_001', 'EncDecAttention', 'k']\n",
            "Initialize PyTorch weight ['decoder', 'block_000', 'layer_001', 'EncDecAttention', 'k']\n",
            "Skipping decoder/block_000/layer_001/EncDecAttention/k_slot_vc\n",
            "Skipping decoder/block_000/layer_001/EncDecAttention/k_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_000', 'layer_001', 'EncDecAttention', 'o']\n",
            "Initialize PyTorch weight ['decoder', 'block_000', 'layer_001', 'EncDecAttention', 'o']\n",
            "Skipping decoder/block_000/layer_001/EncDecAttention/o_slot_vc\n",
            "Skipping decoder/block_000/layer_001/EncDecAttention/o_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_000', 'layer_001', 'EncDecAttention', 'q']\n",
            "Initialize PyTorch weight ['decoder', 'block_000', 'layer_001', 'EncDecAttention', 'q']\n",
            "Skipping decoder/block_000/layer_001/EncDecAttention/q_slot_vc\n",
            "Skipping decoder/block_000/layer_001/EncDecAttention/q_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_000', 'layer_001', 'EncDecAttention', 'v']\n",
            "Initialize PyTorch weight ['decoder', 'block_000', 'layer_001', 'EncDecAttention', 'v']\n",
            "Skipping decoder/block_000/layer_001/EncDecAttention/v_slot_vc\n",
            "Skipping decoder/block_000/layer_001/EncDecAttention/v_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['decoder', 'block_000', 'layer_001', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['decoder', 'block_000', 'layer_001', 'layer_norm', 'scale']\n",
            "Skipping decoder/block_000/layer_001/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 2048) for ['decoder', 'block_000', 'layer_002', 'DenseReluDense', 'wi', 'kernel']\n",
            "Initialize PyTorch weight ['decoder', 'block_000', 'layer_002', 'DenseReluDense', 'wi', 'kernel']\n",
            "Skipping decoder/block_000/layer_002/DenseReluDense/wi/kernel_slot_vc\n",
            "Skipping decoder/block_000/layer_002/DenseReluDense/wi/kernel_slot_vr\n",
            "Transposing numpy weight of shape (2048, 512) for ['decoder', 'block_000', 'layer_002', 'DenseReluDense', 'wo', 'kernel']\n",
            "Initialize PyTorch weight ['decoder', 'block_000', 'layer_002', 'DenseReluDense', 'wo', 'kernel']\n",
            "Skipping decoder/block_000/layer_002/DenseReluDense/wo/kernel_slot_vc\n",
            "Skipping decoder/block_000/layer_002/DenseReluDense/wo/kernel_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['decoder', 'block_000', 'layer_002', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['decoder', 'block_000', 'layer_002', 'layer_norm', 'scale']\n",
            "Skipping decoder/block_000/layer_002/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_001', 'layer_000', 'SelfAttention', 'k']\n",
            "Initialize PyTorch weight ['decoder', 'block_001', 'layer_000', 'SelfAttention', 'k']\n",
            "Skipping decoder/block_001/layer_000/SelfAttention/k_slot_vc\n",
            "Skipping decoder/block_001/layer_000/SelfAttention/k_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_001', 'layer_000', 'SelfAttention', 'o']\n",
            "Initialize PyTorch weight ['decoder', 'block_001', 'layer_000', 'SelfAttention', 'o']\n",
            "Skipping decoder/block_001/layer_000/SelfAttention/o_slot_vc\n",
            "Skipping decoder/block_001/layer_000/SelfAttention/o_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_001', 'layer_000', 'SelfAttention', 'q']\n",
            "Initialize PyTorch weight ['decoder', 'block_001', 'layer_000', 'SelfAttention', 'q']\n",
            "Skipping decoder/block_001/layer_000/SelfAttention/q_slot_vc\n",
            "Skipping decoder/block_001/layer_000/SelfAttention/q_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_001', 'layer_000', 'SelfAttention', 'v']\n",
            "Initialize PyTorch weight ['decoder', 'block_001', 'layer_000', 'SelfAttention', 'v']\n",
            "Skipping decoder/block_001/layer_000/SelfAttention/v_slot_vc\n",
            "Skipping decoder/block_001/layer_000/SelfAttention/v_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['decoder', 'block_001', 'layer_000', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['decoder', 'block_001', 'layer_000', 'layer_norm', 'scale']\n",
            "Skipping decoder/block_001/layer_000/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_001', 'layer_001', 'EncDecAttention', 'k']\n",
            "Initialize PyTorch weight ['decoder', 'block_001', 'layer_001', 'EncDecAttention', 'k']\n",
            "Skipping decoder/block_001/layer_001/EncDecAttention/k_slot_vc\n",
            "Skipping decoder/block_001/layer_001/EncDecAttention/k_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_001', 'layer_001', 'EncDecAttention', 'o']\n",
            "Initialize PyTorch weight ['decoder', 'block_001', 'layer_001', 'EncDecAttention', 'o']\n",
            "Skipping decoder/block_001/layer_001/EncDecAttention/o_slot_vc\n",
            "Skipping decoder/block_001/layer_001/EncDecAttention/o_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_001', 'layer_001', 'EncDecAttention', 'q']\n",
            "Initialize PyTorch weight ['decoder', 'block_001', 'layer_001', 'EncDecAttention', 'q']\n",
            "Skipping decoder/block_001/layer_001/EncDecAttention/q_slot_vc\n",
            "Skipping decoder/block_001/layer_001/EncDecAttention/q_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_001', 'layer_001', 'EncDecAttention', 'v']\n",
            "Initialize PyTorch weight ['decoder', 'block_001', 'layer_001', 'EncDecAttention', 'v']\n",
            "Skipping decoder/block_001/layer_001/EncDecAttention/v_slot_vc\n",
            "Skipping decoder/block_001/layer_001/EncDecAttention/v_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['decoder', 'block_001', 'layer_001', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['decoder', 'block_001', 'layer_001', 'layer_norm', 'scale']\n",
            "Skipping decoder/block_001/layer_001/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 2048) for ['decoder', 'block_001', 'layer_002', 'DenseReluDense', 'wi', 'kernel']\n",
            "Initialize PyTorch weight ['decoder', 'block_001', 'layer_002', 'DenseReluDense', 'wi', 'kernel']\n",
            "Skipping decoder/block_001/layer_002/DenseReluDense/wi/kernel_slot_vc\n",
            "Skipping decoder/block_001/layer_002/DenseReluDense/wi/kernel_slot_vr\n",
            "Transposing numpy weight of shape (2048, 512) for ['decoder', 'block_001', 'layer_002', 'DenseReluDense', 'wo', 'kernel']\n",
            "Initialize PyTorch weight ['decoder', 'block_001', 'layer_002', 'DenseReluDense', 'wo', 'kernel']\n",
            "Skipping decoder/block_001/layer_002/DenseReluDense/wo/kernel_slot_vc\n",
            "Skipping decoder/block_001/layer_002/DenseReluDense/wo/kernel_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['decoder', 'block_001', 'layer_002', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['decoder', 'block_001', 'layer_002', 'layer_norm', 'scale']\n",
            "Skipping decoder/block_001/layer_002/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_002', 'layer_000', 'SelfAttention', 'k']\n",
            "Initialize PyTorch weight ['decoder', 'block_002', 'layer_000', 'SelfAttention', 'k']\n",
            "Skipping decoder/block_002/layer_000/SelfAttention/k_slot_vc\n",
            "Skipping decoder/block_002/layer_000/SelfAttention/k_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_002', 'layer_000', 'SelfAttention', 'o']\n",
            "Initialize PyTorch weight ['decoder', 'block_002', 'layer_000', 'SelfAttention', 'o']\n",
            "Skipping decoder/block_002/layer_000/SelfAttention/o_slot_vc\n",
            "Skipping decoder/block_002/layer_000/SelfAttention/o_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_002', 'layer_000', 'SelfAttention', 'q']\n",
            "Initialize PyTorch weight ['decoder', 'block_002', 'layer_000', 'SelfAttention', 'q']\n",
            "Skipping decoder/block_002/layer_000/SelfAttention/q_slot_vc\n",
            "Skipping decoder/block_002/layer_000/SelfAttention/q_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_002', 'layer_000', 'SelfAttention', 'v']\n",
            "Initialize PyTorch weight ['decoder', 'block_002', 'layer_000', 'SelfAttention', 'v']\n",
            "Skipping decoder/block_002/layer_000/SelfAttention/v_slot_vc\n",
            "Skipping decoder/block_002/layer_000/SelfAttention/v_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['decoder', 'block_002', 'layer_000', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['decoder', 'block_002', 'layer_000', 'layer_norm', 'scale']\n",
            "Skipping decoder/block_002/layer_000/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_002', 'layer_001', 'EncDecAttention', 'k']\n",
            "Initialize PyTorch weight ['decoder', 'block_002', 'layer_001', 'EncDecAttention', 'k']\n",
            "Skipping decoder/block_002/layer_001/EncDecAttention/k_slot_vc\n",
            "Skipping decoder/block_002/layer_001/EncDecAttention/k_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_002', 'layer_001', 'EncDecAttention', 'o']\n",
            "Initialize PyTorch weight ['decoder', 'block_002', 'layer_001', 'EncDecAttention', 'o']\n",
            "Skipping decoder/block_002/layer_001/EncDecAttention/o_slot_vc\n",
            "Skipping decoder/block_002/layer_001/EncDecAttention/o_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_002', 'layer_001', 'EncDecAttention', 'q']\n",
            "Initialize PyTorch weight ['decoder', 'block_002', 'layer_001', 'EncDecAttention', 'q']\n",
            "Skipping decoder/block_002/layer_001/EncDecAttention/q_slot_vc\n",
            "Skipping decoder/block_002/layer_001/EncDecAttention/q_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_002', 'layer_001', 'EncDecAttention', 'v']\n",
            "Initialize PyTorch weight ['decoder', 'block_002', 'layer_001', 'EncDecAttention', 'v']\n",
            "Skipping decoder/block_002/layer_001/EncDecAttention/v_slot_vc\n",
            "Skipping decoder/block_002/layer_001/EncDecAttention/v_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['decoder', 'block_002', 'layer_001', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['decoder', 'block_002', 'layer_001', 'layer_norm', 'scale']\n",
            "Skipping decoder/block_002/layer_001/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 2048) for ['decoder', 'block_002', 'layer_002', 'DenseReluDense', 'wi', 'kernel']\n",
            "Initialize PyTorch weight ['decoder', 'block_002', 'layer_002', 'DenseReluDense', 'wi', 'kernel']\n",
            "Skipping decoder/block_002/layer_002/DenseReluDense/wi/kernel_slot_vc\n",
            "Skipping decoder/block_002/layer_002/DenseReluDense/wi/kernel_slot_vr\n",
            "Transposing numpy weight of shape (2048, 512) for ['decoder', 'block_002', 'layer_002', 'DenseReluDense', 'wo', 'kernel']\n",
            "Initialize PyTorch weight ['decoder', 'block_002', 'layer_002', 'DenseReluDense', 'wo', 'kernel']\n",
            "Skipping decoder/block_002/layer_002/DenseReluDense/wo/kernel_slot_vc\n",
            "Skipping decoder/block_002/layer_002/DenseReluDense/wo/kernel_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['decoder', 'block_002', 'layer_002', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['decoder', 'block_002', 'layer_002', 'layer_norm', 'scale']\n",
            "Skipping decoder/block_002/layer_002/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_003', 'layer_000', 'SelfAttention', 'k']\n",
            "Initialize PyTorch weight ['decoder', 'block_003', 'layer_000', 'SelfAttention', 'k']\n",
            "Skipping decoder/block_003/layer_000/SelfAttention/k_slot_vc\n",
            "Skipping decoder/block_003/layer_000/SelfAttention/k_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_003', 'layer_000', 'SelfAttention', 'o']\n",
            "Initialize PyTorch weight ['decoder', 'block_003', 'layer_000', 'SelfAttention', 'o']\n",
            "Skipping decoder/block_003/layer_000/SelfAttention/o_slot_vc\n",
            "Skipping decoder/block_003/layer_000/SelfAttention/o_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_003', 'layer_000', 'SelfAttention', 'q']\n",
            "Initialize PyTorch weight ['decoder', 'block_003', 'layer_000', 'SelfAttention', 'q']\n",
            "Skipping decoder/block_003/layer_000/SelfAttention/q_slot_vc\n",
            "Skipping decoder/block_003/layer_000/SelfAttention/q_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_003', 'layer_000', 'SelfAttention', 'v']\n",
            "Initialize PyTorch weight ['decoder', 'block_003', 'layer_000', 'SelfAttention', 'v']\n",
            "Skipping decoder/block_003/layer_000/SelfAttention/v_slot_vc\n",
            "Skipping decoder/block_003/layer_000/SelfAttention/v_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['decoder', 'block_003', 'layer_000', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['decoder', 'block_003', 'layer_000', 'layer_norm', 'scale']\n",
            "Skipping decoder/block_003/layer_000/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_003', 'layer_001', 'EncDecAttention', 'k']\n",
            "Initialize PyTorch weight ['decoder', 'block_003', 'layer_001', 'EncDecAttention', 'k']\n",
            "Skipping decoder/block_003/layer_001/EncDecAttention/k_slot_vc\n",
            "Skipping decoder/block_003/layer_001/EncDecAttention/k_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_003', 'layer_001', 'EncDecAttention', 'o']\n",
            "Initialize PyTorch weight ['decoder', 'block_003', 'layer_001', 'EncDecAttention', 'o']\n",
            "Skipping decoder/block_003/layer_001/EncDecAttention/o_slot_vc\n",
            "Skipping decoder/block_003/layer_001/EncDecAttention/o_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_003', 'layer_001', 'EncDecAttention', 'q']\n",
            "Initialize PyTorch weight ['decoder', 'block_003', 'layer_001', 'EncDecAttention', 'q']\n",
            "Skipping decoder/block_003/layer_001/EncDecAttention/q_slot_vc\n",
            "Skipping decoder/block_003/layer_001/EncDecAttention/q_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_003', 'layer_001', 'EncDecAttention', 'v']\n",
            "Initialize PyTorch weight ['decoder', 'block_003', 'layer_001', 'EncDecAttention', 'v']\n",
            "Skipping decoder/block_003/layer_001/EncDecAttention/v_slot_vc\n",
            "Skipping decoder/block_003/layer_001/EncDecAttention/v_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['decoder', 'block_003', 'layer_001', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['decoder', 'block_003', 'layer_001', 'layer_norm', 'scale']\n",
            "Skipping decoder/block_003/layer_001/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 2048) for ['decoder', 'block_003', 'layer_002', 'DenseReluDense', 'wi', 'kernel']\n",
            "Initialize PyTorch weight ['decoder', 'block_003', 'layer_002', 'DenseReluDense', 'wi', 'kernel']\n",
            "Skipping decoder/block_003/layer_002/DenseReluDense/wi/kernel_slot_vc\n",
            "Skipping decoder/block_003/layer_002/DenseReluDense/wi/kernel_slot_vr\n",
            "Transposing numpy weight of shape (2048, 512) for ['decoder', 'block_003', 'layer_002', 'DenseReluDense', 'wo', 'kernel']\n",
            "Initialize PyTorch weight ['decoder', 'block_003', 'layer_002', 'DenseReluDense', 'wo', 'kernel']\n",
            "Skipping decoder/block_003/layer_002/DenseReluDense/wo/kernel_slot_vc\n",
            "Skipping decoder/block_003/layer_002/DenseReluDense/wo/kernel_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['decoder', 'block_003', 'layer_002', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['decoder', 'block_003', 'layer_002', 'layer_norm', 'scale']\n",
            "Skipping decoder/block_003/layer_002/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_004', 'layer_000', 'SelfAttention', 'k']\n",
            "Initialize PyTorch weight ['decoder', 'block_004', 'layer_000', 'SelfAttention', 'k']\n",
            "Skipping decoder/block_004/layer_000/SelfAttention/k_slot_vc\n",
            "Skipping decoder/block_004/layer_000/SelfAttention/k_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_004', 'layer_000', 'SelfAttention', 'o']\n",
            "Initialize PyTorch weight ['decoder', 'block_004', 'layer_000', 'SelfAttention', 'o']\n",
            "Skipping decoder/block_004/layer_000/SelfAttention/o_slot_vc\n",
            "Skipping decoder/block_004/layer_000/SelfAttention/o_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_004', 'layer_000', 'SelfAttention', 'q']\n",
            "Initialize PyTorch weight ['decoder', 'block_004', 'layer_000', 'SelfAttention', 'q']\n",
            "Skipping decoder/block_004/layer_000/SelfAttention/q_slot_vc\n",
            "Skipping decoder/block_004/layer_000/SelfAttention/q_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_004', 'layer_000', 'SelfAttention', 'v']\n",
            "Initialize PyTorch weight ['decoder', 'block_004', 'layer_000', 'SelfAttention', 'v']\n",
            "Skipping decoder/block_004/layer_000/SelfAttention/v_slot_vc\n",
            "Skipping decoder/block_004/layer_000/SelfAttention/v_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['decoder', 'block_004', 'layer_000', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['decoder', 'block_004', 'layer_000', 'layer_norm', 'scale']\n",
            "Skipping decoder/block_004/layer_000/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_004', 'layer_001', 'EncDecAttention', 'k']\n",
            "Initialize PyTorch weight ['decoder', 'block_004', 'layer_001', 'EncDecAttention', 'k']\n",
            "Skipping decoder/block_004/layer_001/EncDecAttention/k_slot_vc\n",
            "Skipping decoder/block_004/layer_001/EncDecAttention/k_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_004', 'layer_001', 'EncDecAttention', 'o']\n",
            "Initialize PyTorch weight ['decoder', 'block_004', 'layer_001', 'EncDecAttention', 'o']\n",
            "Skipping decoder/block_004/layer_001/EncDecAttention/o_slot_vc\n",
            "Skipping decoder/block_004/layer_001/EncDecAttention/o_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_004', 'layer_001', 'EncDecAttention', 'q']\n",
            "Initialize PyTorch weight ['decoder', 'block_004', 'layer_001', 'EncDecAttention', 'q']\n",
            "Skipping decoder/block_004/layer_001/EncDecAttention/q_slot_vc\n",
            "Skipping decoder/block_004/layer_001/EncDecAttention/q_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_004', 'layer_001', 'EncDecAttention', 'v']\n",
            "Initialize PyTorch weight ['decoder', 'block_004', 'layer_001', 'EncDecAttention', 'v']\n",
            "Skipping decoder/block_004/layer_001/EncDecAttention/v_slot_vc\n",
            "Skipping decoder/block_004/layer_001/EncDecAttention/v_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['decoder', 'block_004', 'layer_001', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['decoder', 'block_004', 'layer_001', 'layer_norm', 'scale']\n",
            "Skipping decoder/block_004/layer_001/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 2048) for ['decoder', 'block_004', 'layer_002', 'DenseReluDense', 'wi', 'kernel']\n",
            "Initialize PyTorch weight ['decoder', 'block_004', 'layer_002', 'DenseReluDense', 'wi', 'kernel']\n",
            "Skipping decoder/block_004/layer_002/DenseReluDense/wi/kernel_slot_vc\n",
            "Skipping decoder/block_004/layer_002/DenseReluDense/wi/kernel_slot_vr\n",
            "Transposing numpy weight of shape (2048, 512) for ['decoder', 'block_004', 'layer_002', 'DenseReluDense', 'wo', 'kernel']\n",
            "Initialize PyTorch weight ['decoder', 'block_004', 'layer_002', 'DenseReluDense', 'wo', 'kernel']\n",
            "Skipping decoder/block_004/layer_002/DenseReluDense/wo/kernel_slot_vc\n",
            "Skipping decoder/block_004/layer_002/DenseReluDense/wo/kernel_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['decoder', 'block_004', 'layer_002', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['decoder', 'block_004', 'layer_002', 'layer_norm', 'scale']\n",
            "Skipping decoder/block_004/layer_002/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_005', 'layer_000', 'SelfAttention', 'k']\n",
            "Initialize PyTorch weight ['decoder', 'block_005', 'layer_000', 'SelfAttention', 'k']\n",
            "Skipping decoder/block_005/layer_000/SelfAttention/k_slot_vc\n",
            "Skipping decoder/block_005/layer_000/SelfAttention/k_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_005', 'layer_000', 'SelfAttention', 'o']\n",
            "Initialize PyTorch weight ['decoder', 'block_005', 'layer_000', 'SelfAttention', 'o']\n",
            "Skipping decoder/block_005/layer_000/SelfAttention/o_slot_vc\n",
            "Skipping decoder/block_005/layer_000/SelfAttention/o_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_005', 'layer_000', 'SelfAttention', 'q']\n",
            "Initialize PyTorch weight ['decoder', 'block_005', 'layer_000', 'SelfAttention', 'q']\n",
            "Skipping decoder/block_005/layer_000/SelfAttention/q_slot_vc\n",
            "Skipping decoder/block_005/layer_000/SelfAttention/q_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_005', 'layer_000', 'SelfAttention', 'v']\n",
            "Initialize PyTorch weight ['decoder', 'block_005', 'layer_000', 'SelfAttention', 'v']\n",
            "Skipping decoder/block_005/layer_000/SelfAttention/v_slot_vc\n",
            "Skipping decoder/block_005/layer_000/SelfAttention/v_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['decoder', 'block_005', 'layer_000', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['decoder', 'block_005', 'layer_000', 'layer_norm', 'scale']\n",
            "Skipping decoder/block_005/layer_000/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_005', 'layer_001', 'EncDecAttention', 'k']\n",
            "Initialize PyTorch weight ['decoder', 'block_005', 'layer_001', 'EncDecAttention', 'k']\n",
            "Skipping decoder/block_005/layer_001/EncDecAttention/k_slot_vc\n",
            "Skipping decoder/block_005/layer_001/EncDecAttention/k_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_005', 'layer_001', 'EncDecAttention', 'o']\n",
            "Initialize PyTorch weight ['decoder', 'block_005', 'layer_001', 'EncDecAttention', 'o']\n",
            "Skipping decoder/block_005/layer_001/EncDecAttention/o_slot_vc\n",
            "Skipping decoder/block_005/layer_001/EncDecAttention/o_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_005', 'layer_001', 'EncDecAttention', 'q']\n",
            "Initialize PyTorch weight ['decoder', 'block_005', 'layer_001', 'EncDecAttention', 'q']\n",
            "Skipping decoder/block_005/layer_001/EncDecAttention/q_slot_vc\n",
            "Skipping decoder/block_005/layer_001/EncDecAttention/q_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_005', 'layer_001', 'EncDecAttention', 'v']\n",
            "Initialize PyTorch weight ['decoder', 'block_005', 'layer_001', 'EncDecAttention', 'v']\n",
            "Skipping decoder/block_005/layer_001/EncDecAttention/v_slot_vc\n",
            "Skipping decoder/block_005/layer_001/EncDecAttention/v_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['decoder', 'block_005', 'layer_001', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['decoder', 'block_005', 'layer_001', 'layer_norm', 'scale']\n",
            "Skipping decoder/block_005/layer_001/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 2048) for ['decoder', 'block_005', 'layer_002', 'DenseReluDense', 'wi', 'kernel']\n",
            "Initialize PyTorch weight ['decoder', 'block_005', 'layer_002', 'DenseReluDense', 'wi', 'kernel']\n",
            "Skipping decoder/block_005/layer_002/DenseReluDense/wi/kernel_slot_vc\n",
            "Skipping decoder/block_005/layer_002/DenseReluDense/wi/kernel_slot_vr\n",
            "Transposing numpy weight of shape (2048, 512) for ['decoder', 'block_005', 'layer_002', 'DenseReluDense', 'wo', 'kernel']\n",
            "Initialize PyTorch weight ['decoder', 'block_005', 'layer_002', 'DenseReluDense', 'wo', 'kernel']\n",
            "Skipping decoder/block_005/layer_002/DenseReluDense/wo/kernel_slot_vc\n",
            "Skipping decoder/block_005/layer_002/DenseReluDense/wo/kernel_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['decoder', 'block_005', 'layer_002', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['decoder', 'block_005', 'layer_002', 'layer_norm', 'scale']\n",
            "Skipping decoder/block_005/layer_002/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512,) for ['decoder', 'final_layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['decoder', 'final_layer_norm', 'scale']\n",
            "Skipping decoder/final_layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 512) for ['encoder', 'block_000', 'layer_000', 'SelfAttention', 'k']\n",
            "Initialize PyTorch weight ['encoder', 'block_000', 'layer_000', 'SelfAttention', 'k']\n",
            "Skipping encoder/block_000/layer_000/SelfAttention/k_slot_vc\n",
            "Skipping encoder/block_000/layer_000/SelfAttention/k_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['encoder', 'block_000', 'layer_000', 'SelfAttention', 'o']\n",
            "Initialize PyTorch weight ['encoder', 'block_000', 'layer_000', 'SelfAttention', 'o']\n",
            "Skipping encoder/block_000/layer_000/SelfAttention/o_slot_vc\n",
            "Skipping encoder/block_000/layer_000/SelfAttention/o_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['encoder', 'block_000', 'layer_000', 'SelfAttention', 'q']\n",
            "Initialize PyTorch weight ['encoder', 'block_000', 'layer_000', 'SelfAttention', 'q']\n",
            "Skipping encoder/block_000/layer_000/SelfAttention/q_slot_vc\n",
            "Skipping encoder/block_000/layer_000/SelfAttention/q_slot_vr\n",
            "Transposing numpy weight of shape (8, 32) for ['encoder', 'block_000', 'layer_000', 'SelfAttention', 'relative_attention_bias']\n",
            "Initialize PyTorch weight ['encoder', 'block_000', 'layer_000', 'SelfAttention', 'relative_attention_bias']\n",
            "Skipping encoder/block_000/layer_000/SelfAttention/relative_attention_bias_slot_v\n",
            "Transposing numpy weight of shape (512, 512) for ['encoder', 'block_000', 'layer_000', 'SelfAttention', 'v']\n",
            "Initialize PyTorch weight ['encoder', 'block_000', 'layer_000', 'SelfAttention', 'v']\n",
            "Skipping encoder/block_000/layer_000/SelfAttention/v_slot_vc\n",
            "Skipping encoder/block_000/layer_000/SelfAttention/v_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['encoder', 'block_000', 'layer_000', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['encoder', 'block_000', 'layer_000', 'layer_norm', 'scale']\n",
            "Skipping encoder/block_000/layer_000/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 2048) for ['encoder', 'block_000', 'layer_001', 'DenseReluDense', 'wi', 'kernel']\n",
            "Initialize PyTorch weight ['encoder', 'block_000', 'layer_001', 'DenseReluDense', 'wi', 'kernel']\n",
            "Skipping encoder/block_000/layer_001/DenseReluDense/wi/kernel_slot_vc\n",
            "Skipping encoder/block_000/layer_001/DenseReluDense/wi/kernel_slot_vr\n",
            "Transposing numpy weight of shape (2048, 512) for ['encoder', 'block_000', 'layer_001', 'DenseReluDense', 'wo', 'kernel']\n",
            "Initialize PyTorch weight ['encoder', 'block_000', 'layer_001', 'DenseReluDense', 'wo', 'kernel']\n",
            "Skipping encoder/block_000/layer_001/DenseReluDense/wo/kernel_slot_vc\n",
            "Skipping encoder/block_000/layer_001/DenseReluDense/wo/kernel_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['encoder', 'block_000', 'layer_001', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['encoder', 'block_000', 'layer_001', 'layer_norm', 'scale']\n",
            "Skipping encoder/block_000/layer_001/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 512) for ['encoder', 'block_001', 'layer_000', 'SelfAttention', 'k']\n",
            "Initialize PyTorch weight ['encoder', 'block_001', 'layer_000', 'SelfAttention', 'k']\n",
            "Skipping encoder/block_001/layer_000/SelfAttention/k_slot_vc\n",
            "Skipping encoder/block_001/layer_000/SelfAttention/k_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['encoder', 'block_001', 'layer_000', 'SelfAttention', 'o']\n",
            "Initialize PyTorch weight ['encoder', 'block_001', 'layer_000', 'SelfAttention', 'o']\n",
            "Skipping encoder/block_001/layer_000/SelfAttention/o_slot_vc\n",
            "Skipping encoder/block_001/layer_000/SelfAttention/o_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['encoder', 'block_001', 'layer_000', 'SelfAttention', 'q']\n",
            "Initialize PyTorch weight ['encoder', 'block_001', 'layer_000', 'SelfAttention', 'q']\n",
            "Skipping encoder/block_001/layer_000/SelfAttention/q_slot_vc\n",
            "Skipping encoder/block_001/layer_000/SelfAttention/q_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['encoder', 'block_001', 'layer_000', 'SelfAttention', 'v']\n",
            "Initialize PyTorch weight ['encoder', 'block_001', 'layer_000', 'SelfAttention', 'v']\n",
            "Skipping encoder/block_001/layer_000/SelfAttention/v_slot_vc\n",
            "Skipping encoder/block_001/layer_000/SelfAttention/v_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['encoder', 'block_001', 'layer_000', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['encoder', 'block_001', 'layer_000', 'layer_norm', 'scale']\n",
            "Skipping encoder/block_001/layer_000/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 2048) for ['encoder', 'block_001', 'layer_001', 'DenseReluDense', 'wi', 'kernel']\n",
            "Initialize PyTorch weight ['encoder', 'block_001', 'layer_001', 'DenseReluDense', 'wi', 'kernel']\n",
            "Skipping encoder/block_001/layer_001/DenseReluDense/wi/kernel_slot_vc\n",
            "Skipping encoder/block_001/layer_001/DenseReluDense/wi/kernel_slot_vr\n",
            "Transposing numpy weight of shape (2048, 512) for ['encoder', 'block_001', 'layer_001', 'DenseReluDense', 'wo', 'kernel']\n",
            "Initialize PyTorch weight ['encoder', 'block_001', 'layer_001', 'DenseReluDense', 'wo', 'kernel']\n",
            "Skipping encoder/block_001/layer_001/DenseReluDense/wo/kernel_slot_vc\n",
            "Skipping encoder/block_001/layer_001/DenseReluDense/wo/kernel_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['encoder', 'block_001', 'layer_001', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['encoder', 'block_001', 'layer_001', 'layer_norm', 'scale']\n",
            "Skipping encoder/block_001/layer_001/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 512) for ['encoder', 'block_002', 'layer_000', 'SelfAttention', 'k']\n",
            "Initialize PyTorch weight ['encoder', 'block_002', 'layer_000', 'SelfAttention', 'k']\n",
            "Skipping encoder/block_002/layer_000/SelfAttention/k_slot_vc\n",
            "Skipping encoder/block_002/layer_000/SelfAttention/k_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['encoder', 'block_002', 'layer_000', 'SelfAttention', 'o']\n",
            "Initialize PyTorch weight ['encoder', 'block_002', 'layer_000', 'SelfAttention', 'o']\n",
            "Skipping encoder/block_002/layer_000/SelfAttention/o_slot_vc\n",
            "Skipping encoder/block_002/layer_000/SelfAttention/o_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['encoder', 'block_002', 'layer_000', 'SelfAttention', 'q']\n",
            "Initialize PyTorch weight ['encoder', 'block_002', 'layer_000', 'SelfAttention', 'q']\n",
            "Skipping encoder/block_002/layer_000/SelfAttention/q_slot_vc\n",
            "Skipping encoder/block_002/layer_000/SelfAttention/q_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['encoder', 'block_002', 'layer_000', 'SelfAttention', 'v']\n",
            "Initialize PyTorch weight ['encoder', 'block_002', 'layer_000', 'SelfAttention', 'v']\n",
            "Skipping encoder/block_002/layer_000/SelfAttention/v_slot_vc\n",
            "Skipping encoder/block_002/layer_000/SelfAttention/v_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['encoder', 'block_002', 'layer_000', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['encoder', 'block_002', 'layer_000', 'layer_norm', 'scale']\n",
            "Skipping encoder/block_002/layer_000/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 2048) for ['encoder', 'block_002', 'layer_001', 'DenseReluDense', 'wi', 'kernel']\n",
            "Initialize PyTorch weight ['encoder', 'block_002', 'layer_001', 'DenseReluDense', 'wi', 'kernel']\n",
            "Skipping encoder/block_002/layer_001/DenseReluDense/wi/kernel_slot_vc\n",
            "Skipping encoder/block_002/layer_001/DenseReluDense/wi/kernel_slot_vr\n",
            "Transposing numpy weight of shape (2048, 512) for ['encoder', 'block_002', 'layer_001', 'DenseReluDense', 'wo', 'kernel']\n",
            "Initialize PyTorch weight ['encoder', 'block_002', 'layer_001', 'DenseReluDense', 'wo', 'kernel']\n",
            "Skipping encoder/block_002/layer_001/DenseReluDense/wo/kernel_slot_vc\n",
            "Skipping encoder/block_002/layer_001/DenseReluDense/wo/kernel_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['encoder', 'block_002', 'layer_001', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['encoder', 'block_002', 'layer_001', 'layer_norm', 'scale']\n",
            "Skipping encoder/block_002/layer_001/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 512) for ['encoder', 'block_003', 'layer_000', 'SelfAttention', 'k']\n",
            "Initialize PyTorch weight ['encoder', 'block_003', 'layer_000', 'SelfAttention', 'k']\n",
            "Skipping encoder/block_003/layer_000/SelfAttention/k_slot_vc\n",
            "Skipping encoder/block_003/layer_000/SelfAttention/k_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['encoder', 'block_003', 'layer_000', 'SelfAttention', 'o']\n",
            "Initialize PyTorch weight ['encoder', 'block_003', 'layer_000', 'SelfAttention', 'o']\n",
            "Skipping encoder/block_003/layer_000/SelfAttention/o_slot_vc\n",
            "Skipping encoder/block_003/layer_000/SelfAttention/o_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['encoder', 'block_003', 'layer_000', 'SelfAttention', 'q']\n",
            "Initialize PyTorch weight ['encoder', 'block_003', 'layer_000', 'SelfAttention', 'q']\n",
            "Skipping encoder/block_003/layer_000/SelfAttention/q_slot_vc\n",
            "Skipping encoder/block_003/layer_000/SelfAttention/q_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['encoder', 'block_003', 'layer_000', 'SelfAttention', 'v']\n",
            "Initialize PyTorch weight ['encoder', 'block_003', 'layer_000', 'SelfAttention', 'v']\n",
            "Skipping encoder/block_003/layer_000/SelfAttention/v_slot_vc\n",
            "Skipping encoder/block_003/layer_000/SelfAttention/v_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['encoder', 'block_003', 'layer_000', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['encoder', 'block_003', 'layer_000', 'layer_norm', 'scale']\n",
            "Skipping encoder/block_003/layer_000/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 2048) for ['encoder', 'block_003', 'layer_001', 'DenseReluDense', 'wi', 'kernel']\n",
            "Initialize PyTorch weight ['encoder', 'block_003', 'layer_001', 'DenseReluDense', 'wi', 'kernel']\n",
            "Skipping encoder/block_003/layer_001/DenseReluDense/wi/kernel_slot_vc\n",
            "Skipping encoder/block_003/layer_001/DenseReluDense/wi/kernel_slot_vr\n",
            "Transposing numpy weight of shape (2048, 512) for ['encoder', 'block_003', 'layer_001', 'DenseReluDense', 'wo', 'kernel']\n",
            "Initialize PyTorch weight ['encoder', 'block_003', 'layer_001', 'DenseReluDense', 'wo', 'kernel']\n",
            "Skipping encoder/block_003/layer_001/DenseReluDense/wo/kernel_slot_vc\n",
            "Skipping encoder/block_003/layer_001/DenseReluDense/wo/kernel_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['encoder', 'block_003', 'layer_001', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['encoder', 'block_003', 'layer_001', 'layer_norm', 'scale']\n",
            "Skipping encoder/block_003/layer_001/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 512) for ['encoder', 'block_004', 'layer_000', 'SelfAttention', 'k']\n",
            "Initialize PyTorch weight ['encoder', 'block_004', 'layer_000', 'SelfAttention', 'k']\n",
            "Skipping encoder/block_004/layer_000/SelfAttention/k_slot_vc\n",
            "Skipping encoder/block_004/layer_000/SelfAttention/k_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['encoder', 'block_004', 'layer_000', 'SelfAttention', 'o']\n",
            "Initialize PyTorch weight ['encoder', 'block_004', 'layer_000', 'SelfAttention', 'o']\n",
            "Skipping encoder/block_004/layer_000/SelfAttention/o_slot_vc\n",
            "Skipping encoder/block_004/layer_000/SelfAttention/o_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['encoder', 'block_004', 'layer_000', 'SelfAttention', 'q']\n",
            "Initialize PyTorch weight ['encoder', 'block_004', 'layer_000', 'SelfAttention', 'q']\n",
            "Skipping encoder/block_004/layer_000/SelfAttention/q_slot_vc\n",
            "Skipping encoder/block_004/layer_000/SelfAttention/q_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['encoder', 'block_004', 'layer_000', 'SelfAttention', 'v']\n",
            "Initialize PyTorch weight ['encoder', 'block_004', 'layer_000', 'SelfAttention', 'v']\n",
            "Skipping encoder/block_004/layer_000/SelfAttention/v_slot_vc\n",
            "Skipping encoder/block_004/layer_000/SelfAttention/v_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['encoder', 'block_004', 'layer_000', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['encoder', 'block_004', 'layer_000', 'layer_norm', 'scale']\n",
            "Skipping encoder/block_004/layer_000/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 2048) for ['encoder', 'block_004', 'layer_001', 'DenseReluDense', 'wi', 'kernel']\n",
            "Initialize PyTorch weight ['encoder', 'block_004', 'layer_001', 'DenseReluDense', 'wi', 'kernel']\n",
            "Skipping encoder/block_004/layer_001/DenseReluDense/wi/kernel_slot_vc\n",
            "Skipping encoder/block_004/layer_001/DenseReluDense/wi/kernel_slot_vr\n",
            "Transposing numpy weight of shape (2048, 512) for ['encoder', 'block_004', 'layer_001', 'DenseReluDense', 'wo', 'kernel']\n",
            "Initialize PyTorch weight ['encoder', 'block_004', 'layer_001', 'DenseReluDense', 'wo', 'kernel']\n",
            "Skipping encoder/block_004/layer_001/DenseReluDense/wo/kernel_slot_vc\n",
            "Skipping encoder/block_004/layer_001/DenseReluDense/wo/kernel_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['encoder', 'block_004', 'layer_001', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['encoder', 'block_004', 'layer_001', 'layer_norm', 'scale']\n",
            "Skipping encoder/block_004/layer_001/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 512) for ['encoder', 'block_005', 'layer_000', 'SelfAttention', 'k']\n",
            "Initialize PyTorch weight ['encoder', 'block_005', 'layer_000', 'SelfAttention', 'k']\n",
            "Skipping encoder/block_005/layer_000/SelfAttention/k_slot_vc\n",
            "Skipping encoder/block_005/layer_000/SelfAttention/k_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['encoder', 'block_005', 'layer_000', 'SelfAttention', 'o']\n",
            "Initialize PyTorch weight ['encoder', 'block_005', 'layer_000', 'SelfAttention', 'o']\n",
            "Skipping encoder/block_005/layer_000/SelfAttention/o_slot_vc\n",
            "Skipping encoder/block_005/layer_000/SelfAttention/o_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['encoder', 'block_005', 'layer_000', 'SelfAttention', 'q']\n",
            "Initialize PyTorch weight ['encoder', 'block_005', 'layer_000', 'SelfAttention', 'q']\n",
            "Skipping encoder/block_005/layer_000/SelfAttention/q_slot_vc\n",
            "Skipping encoder/block_005/layer_000/SelfAttention/q_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['encoder', 'block_005', 'layer_000', 'SelfAttention', 'v']\n",
            "Initialize PyTorch weight ['encoder', 'block_005', 'layer_000', 'SelfAttention', 'v']\n",
            "Skipping encoder/block_005/layer_000/SelfAttention/v_slot_vc\n",
            "Skipping encoder/block_005/layer_000/SelfAttention/v_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['encoder', 'block_005', 'layer_000', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['encoder', 'block_005', 'layer_000', 'layer_norm', 'scale']\n",
            "Skipping encoder/block_005/layer_000/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 2048) for ['encoder', 'block_005', 'layer_001', 'DenseReluDense', 'wi', 'kernel']\n",
            "Initialize PyTorch weight ['encoder', 'block_005', 'layer_001', 'DenseReluDense', 'wi', 'kernel']\n",
            "Skipping encoder/block_005/layer_001/DenseReluDense/wi/kernel_slot_vc\n",
            "Skipping encoder/block_005/layer_001/DenseReluDense/wi/kernel_slot_vr\n",
            "Transposing numpy weight of shape (2048, 512) for ['encoder', 'block_005', 'layer_001', 'DenseReluDense', 'wo', 'kernel']\n",
            "Initialize PyTorch weight ['encoder', 'block_005', 'layer_001', 'DenseReluDense', 'wo', 'kernel']\n",
            "Skipping encoder/block_005/layer_001/DenseReluDense/wo/kernel_slot_vc\n",
            "Skipping encoder/block_005/layer_001/DenseReluDense/wo/kernel_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['encoder', 'block_005', 'layer_001', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['encoder', 'block_005', 'layer_001', 'layer_norm', 'scale']\n",
            "Skipping encoder/block_005/layer_001/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512,) for ['encoder', 'final_layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['encoder', 'final_layer_norm', 'scale']\n",
            "Skipping encoder/final_layer_norm/scale_slot_v\n",
            "Skipping global_step\n",
            "Initialize PyTorch weight ['shared', 'embedding']\n",
            "Skipping shared/embedding_slot_vc\n",
            "Skipping shared/embedding_slot_vr\n",
            "Weights not copied to PyTorch model: \n",
            "Save PyTorch model to ./dumps\n",
            "Configuration saved in ./dumps/config.json\n",
            "Model weights saved in ./dumps/pytorch_model.bin\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UDFMVaOHVl3u",
        "outputId": "528f518f-46a7-40c7-f814-3c96e9d4fbcc"
      },
      "source": [
        "# !gsutil cp gs://{bucket_name}/pyTorch_coversion/code/validation.py ./validation.py\n",
        "!mkdir core\n",
        "!mkdir core/__pycache__\n",
        "\n",
        "!gsutil -m cp -r \\\n",
        "  \"gs://code_review_automation/pyTorch_coversion/core/Logger.py\" \\\n",
        "  \"gs://code_review_automation/pyTorch_coversion/core/__init__.py\" \\\n",
        "  \"gs://code_review_automation/pyTorch_coversion/core/custom_dataset.py\" \\\n",
        "  ./core/\n",
        "\n",
        "!gsutil -m cp \\\n",
        "  \"gs://code_review_automation/pyTorch_coversion/core/__pycache__/Logger.cpython-38.pyc\" \\\n",
        "  \"gs://code_review_automation/pyTorch_coversion/core/__pycache__/__init__.cpython-38.pyc\" \\\n",
        "  ./core/__pycache__\n",
        "\n",
        "!gsutil cp gs://{bucket_name}/CodeReviewModel/TestModel.model ./TestModel.model\n",
        "!gsutil cp gs://{bucket_name}/CodeReviewModel/TestModel.vocab ./TestModel.vocab\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘core’: File exists\n",
            "mkdir: cannot create directory ‘core/__pycache__’: File exists\n",
            "Copying gs://code_review_automation/pyTorch_coversion/core/Logger.py...\n",
            "Copying gs://code_review_automation/pyTorch_coversion/core/__init__.py...\n",
            "Copying gs://code_review_automation/pyTorch_coversion/core/custom_dataset.py...\n",
            "/ [3/3 files][  2.6 KiB/  2.6 KiB] 100% Done                                    \n",
            "Operation completed over 3 objects/2.6 KiB.                                      \n",
            "Copying gs://code_review_automation/pyTorch_coversion/core/__pycache__/Logger.cpython-38.pyc...\n",
            "Copying gs://code_review_automation/pyTorch_coversion/core/__pycache__/__init__.cpython-38.pyc...\n",
            "- [2/2 files][  1.4 KiB/  1.4 KiB] 100% Done                                    \n",
            "Operation completed over 2 objects/1.4 KiB.                                      \n",
            "Copying gs://code_review_automation/CodeReviewModel/TestModel.model...\n",
            "/ [1 files][762.0 KiB/762.0 KiB]                                                \n",
            "Operation completed over 1 objects/762.0 KiB.                                    \n",
            "Copying gs://code_review_automation/CodeReviewModel/TestModel.vocab...\n",
            "/ [1 files][557.4 KiB/557.4 KiB]                                                \n",
            "Operation completed over 1 objects/557.4 KiB.                                    \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOMwy8f0xpd5"
      },
      "source": [
        "# !gsutil cp ./dumps/pytorch_model.bin gs://code_review_automation/pyTorch_coversion/pyTorch_dumps/pytorch_model.bin"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1kX6-yPb5ZW"
      },
      "source": [
        "# model = torch.load(\"./dumps/pytorch_model.bin\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oVTNncoEcG1E",
        "outputId": "224f9765-f767-4f31-d67a-c858e4172532"
      },
      "source": [
        "!python3 newVal.py --beam_size 1 --batch_size 5 --data_dir ./data/ --tokenizer_name ./TestModel.model --model_name_or_path ./dumps/pytorch_model.bin --config_name ./config.json"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: Calling T5Tokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
            "  FutureWarning,\n",
            "['<extra_id_1>', '<extra_id_2>', '<extra_id_3>', '<extra_id_4>', '<extra_id_5>', '<extra_id_6>', '<extra_id_7>', '<extra_id_8>', '<extra_id_9>', '<extra_id_10>', '<extra_id_11>', '<extra_id_12>', '<extra_id_13>', '<extra_id_14>', '<extra_id_15>', '<extra_id_16>', '<extra_id_17>', '<extra_id_18>', '<extra_id_19>', '<extra_id_20>', '<extra_id_21>', '<extra_id_22>', '<extra_id_23>', '<extra_id_24>', '<extra_id_25>', '<extra_id_26>', '<extra_id_27>', '<extra_id_28>', '<extra_id_29>', '<extra_id_30>', '<extra_id_31>', '<extra_id_32>', '<extra_id_33>', '<extra_id_34>', '<extra_id_35>', '<extra_id_36>', '<extra_id_37>', '<extra_id_38>', '<extra_id_39>', '<extra_id_40>', '<extra_id_41>', '<extra_id_42>', '<extra_id_43>', '<extra_id_44>', '<extra_id_45>', '<extra_id_46>', '<extra_id_47>', '<extra_id_48>', '<extra_id_49>', '<extra_id_50>', '<extra_id_51>', '<extra_id_52>', '<extra_id_53>', '<extra_id_54>', '<extra_id_55>', '<extra_id_56>', '<extra_id_57>', '<extra_id_58>', '<extra_id_59>', '<extra_id_60>', '<extra_id_61>', '<extra_id_62>', '<extra_id_63>', '<extra_id_64>', '<extra_id_65>', '<extra_id_66>', '<extra_id_67>', '<extra_id_68>', '<extra_id_69>', '<extra_id_70>', '<extra_id_71>', '<extra_id_72>', '<extra_id_73>', '<extra_id_74>', '<extra_id_75>', '<extra_id_76>', '<extra_id_77>', '<extra_id_78>', '<extra_id_79>', '<extra_id_80>', '<extra_id_81>', '<extra_id_82>', '<extra_id_83>', '<extra_id_84>', '<extra_id_85>', '<extra_id_86>', '<extra_id_87>', '<extra_id_88>', '<extra_id_89>', '<extra_id_90>', '<extra_id_91>', '<extra_id_92>', '<extra_id_93>', '<extra_id_94>', '<extra_id_95>', '<extra_id_96>', '<extra_id_97>', '<extra_id_98>', '<extra_id_99>']\n",
            "  0% 0/2 [00:00<?, ?it/s]2021-03-17 11:23:06.875413: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "100% 2/2 [00:03<00:00,  1.95s/it]\n",
            "#perfect prediction:  0\n",
            "Perfect prediction 0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fctCapzljjVd",
        "outputId": "7e054a9d-ae31-48b8-fda4-d4ac04d1749d"
      },
      "source": [
        "!cat ./results.txt\n",
        "!rm ./results.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[+] Input: public static void <extra_id_0>(String[] args) {System.Out.println(\"hello world\")}\n",
            "[+] Target: main\n",
            "\t [-] pred: hello world<< . (\"hello world\")<< . (\"hello world\")<< . (\"\n",
            "[+] Input: public static void main(String[] <extra_id_0> {System.Out.println(\"hello world\")}\n",
            "[+] Target: args)\n",
            "\t [-] pred: args<< .<<< (\"hello world\")<<<<<<<<<<<<<<<<\n",
            "[+] Input: public static void main(String[] args) {System.Out.<extra_id_0>(\"hello world\")}\n",
            "[+] Target: println\n",
            "\t [-] pred: println(\"hello world\")<< . (\"hello world\")<< . (\"hello world\")<< .\n",
            "[+] Input: public <extra_id_0> void main(String[] args) {System.Out.println(\"hello world\")}\n",
            "[+] Target: static\n",
            "\t [-] pred: .out.println(\"hello world\")<<< world world\")<<<<<<<<<<<<<<<<\n",
            "[+] Input: public static void main(String[] args) {System.<extra_id_0>.println(\"hello world\")}\n",
            "[+] Target: Out\n",
            "\t [-] pred: println(\"hello world\")< static void _<<  _<extra_.<<extra_ println(\"hello world\")\n",
            "[+] Input: Bind<extra_id_0> to the supplied collection.@param name\n",
            "[+] Target: indexed elements\n",
            "\t [-] pred: param collection Name param collection Collection Name Collection collection collection<<extra_id .<extra_id param<extra_\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}