{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pytorchConversion.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPb+kBjcppweNrBu5ICQ1AK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/masies/CRA/blob/main/pytorchConversion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8i-CsM5qG8P4",
        "outputId": "61febb37-4543-4f1c-9124-1cc559af1807"
      },
      "source": [
        "# !pip3 install transformers\n",
        "!pip install sentencepiece==0.1.94\n",
        "!pip install -q torch==1.4.0 -f https://download.pytorch.org/whl/cu101/torch_stable.html\n",
        "!pip install -q transformers==3.5.0 fast-trees\n",
        "!git clone -q https://github.com/microsoft/CodeXGLUE.git\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "from transformers import T5Tokenizer, T5Config, T5ForConditionalGeneration\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import logging\n",
        "import shutil\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import json\n",
        "import sys\n",
        "\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "project_id = 'helical-loop-303918'\n",
        "bucket_name = 'code_review_automation'\n",
        "\n",
        "!gcloud config set project {project_id}\n",
        "\n",
        "!mkdir core\n",
        "\n",
        "!gsutil -m cp -r \\\n",
        "  \"gs://code_review_automation/pyTorch_coversion/core/Logger.py\" \\\n",
        "  ./core\n",
        "\n",
        "from core.Logger import Logger\n",
        "\n",
        "# script for conversion in pythorch\n",
        "!gsutil cp gs://{bucket_name}/pyTorch_coversion/code/tf_2_pytorch_T5.py ./tf_2_pytorch_T5.py\n",
        "\n",
        "# Download the configuration file\n",
        "!gsutil cp gs://{bucket_name}/pyTorch_coversion/config/config.json ./current_model/config.json\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentencepiece==0.1.94\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6e/f0/7614029138ec9422f1a3ed3cd82c3bfc0821157e8032ca1828cee6b198bb/sentencepiece-0.1.94-cp37-cp37m-manylinux2014_x86_64.whl (1.1MB)\n",
            "\r\u001b[K     |▎                               | 10kB 21.7MB/s eta 0:00:01\r\u001b[K     |▋                               | 20kB 29.4MB/s eta 0:00:01\r\u001b[K     |▉                               | 30kB 23.8MB/s eta 0:00:01\r\u001b[K     |█▏                              | 40kB 18.0MB/s eta 0:00:01\r\u001b[K     |█▌                              | 51kB 15.5MB/s eta 0:00:01\r\u001b[K     |█▊                              | 61kB 14.0MB/s eta 0:00:01\r\u001b[K     |██                              | 71kB 15.5MB/s eta 0:00:01\r\u001b[K     |██▍                             | 81kB 15.5MB/s eta 0:00:01\r\u001b[K     |██▋                             | 92kB 14.1MB/s eta 0:00:01\r\u001b[K     |███                             | 102kB 15.1MB/s eta 0:00:01\r\u001b[K     |███▎                            | 112kB 15.1MB/s eta 0:00:01\r\u001b[K     |███▌                            | 122kB 15.1MB/s eta 0:00:01\r\u001b[K     |███▉                            | 133kB 15.1MB/s eta 0:00:01\r\u001b[K     |████▏                           | 143kB 15.1MB/s eta 0:00:01\r\u001b[K     |████▍                           | 153kB 15.1MB/s eta 0:00:01\r\u001b[K     |████▊                           | 163kB 15.1MB/s eta 0:00:01\r\u001b[K     |█████                           | 174kB 15.1MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 184kB 15.1MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 194kB 15.1MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 204kB 15.1MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 215kB 15.1MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 225kB 15.1MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 235kB 15.1MB/s eta 0:00:01\r\u001b[K     |███████                         | 245kB 15.1MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 256kB 15.1MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 266kB 15.1MB/s eta 0:00:01\r\u001b[K     |████████                        | 276kB 15.1MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 286kB 15.1MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 296kB 15.1MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 307kB 15.1MB/s eta 0:00:01\r\u001b[K     |█████████                       | 317kB 15.1MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 327kB 15.1MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 337kB 15.1MB/s eta 0:00:01\r\u001b[K     |██████████                      | 348kB 15.1MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 358kB 15.1MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 368kB 15.1MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 378kB 15.1MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 389kB 15.1MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 399kB 15.1MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 409kB 15.1MB/s eta 0:00:01\r\u001b[K     |████████████                    | 419kB 15.1MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 430kB 15.1MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 440kB 15.1MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 450kB 15.1MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 460kB 15.1MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 471kB 15.1MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 481kB 15.1MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 491kB 15.1MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 501kB 15.1MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 512kB 15.1MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 522kB 15.1MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 532kB 15.1MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 542kB 15.1MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 552kB 15.1MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 563kB 15.1MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 573kB 15.1MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 583kB 15.1MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 593kB 15.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 604kB 15.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 614kB 15.1MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 624kB 15.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 634kB 15.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 645kB 15.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 655kB 15.1MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 665kB 15.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 675kB 15.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 686kB 15.1MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 696kB 15.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 706kB 15.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 716kB 15.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 727kB 15.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 737kB 15.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 747kB 15.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 757kB 15.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 768kB 15.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 778kB 15.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 788kB 15.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 798kB 15.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 808kB 15.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 819kB 15.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 829kB 15.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 839kB 15.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 849kB 15.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 860kB 15.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 870kB 15.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 880kB 15.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 890kB 15.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 901kB 15.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 911kB 15.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 921kB 15.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 931kB 15.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 942kB 15.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 952kB 15.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 962kB 15.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 972kB 15.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 983kB 15.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 993kB 15.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.0MB 15.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.0MB 15.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.0MB 15.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.0MB 15.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.0MB 15.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.1MB 15.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 1.1MB 15.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 1.1MB 15.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.1MB 15.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 1.1MB 15.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 1.1MB 15.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.1MB 15.1MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.94\n",
            "\u001b[K     |████████████████████████████████| 753.4MB 21kB/s \n",
            "\u001b[31mERROR: torchvision 0.9.1+cu101 has requirement torch==1.8.1, but you'll have torch 1.4.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: torchtext 0.9.1 has requirement torch==1.8.1, but you'll have torch 1.4.0 which is incompatible.\u001b[0m\n",
            "\u001b[K     |████████████████████████████████| 1.3MB 14.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.9MB 67.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 870kB 59.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1MB 54.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 112kB 66.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 163kB 59.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 8.6MB/s \n",
            "\u001b[?25h  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for tree-sitter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Updated property [core/project].\n",
            "Copying gs://code_review_automation/pyTorch_coversion/core/Logger.py...\n",
            "/ [1/1 files][   1016 B/   1016 B] 100% Done                                    \n",
            "Operation completed over 1 objects/1016.0 B.                                     \n",
            "Copying gs://code_review_automation/pyTorch_coversion/code/tf_2_pytorch_T5.py...\n",
            "/ [1 files][  2.3 KiB/  2.3 KiB]                                                \n",
            "Operation completed over 1 objects/2.3 KiB.                                      \n",
            "Copying gs://code_review_automation/pyTorch_coversion/config/config.json...\n",
            "/ [1 files][  463.0 B/  463.0 B]                                                \n",
            "Operation completed over 1 objects/463.0 B.                                      \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-xKBubBHvGD",
        "outputId": "9513ccb8-1fe5-4469-fdaf-021e6636d90c"
      },
      "source": [
        "model_number = 240000\n",
        "\n",
        "# Download the selected best model\n",
        "!mkdir dumps\n",
        "!mkdir current_model\n",
        "!gsutil -m cp \\\n",
        "  \"gs://code_review_automation/fine_tuning/HP_tuning/final_version/comment_v1/codeANDcomment_code/pretrained/model.ckpt-{model_number}.data-00000-of-00002\" \\\n",
        "  \"gs://code_review_automation/fine_tuning/HP_tuning/final_version/comment_v1/codeANDcomment_code/pretrained/model.ckpt-{model_number}.data-00001-of-00002\" \\\n",
        "  \"gs://code_review_automation/fine_tuning/HP_tuning/final_version/comment_v1/codeANDcomment_code/pretrained/model.ckpt-{model_number}.index\" \\\n",
        "  \"gs://code_review_automation/fine_tuning/HP_tuning/final_version/comment_v1/codeANDcomment_code/pretrained/model.ckpt-{model_number}.meta\" \\\n",
        "  ./current_model/\n",
        "\n",
        "# Convert the model\n",
        "!python3 ./tf_2_pytorch_T5.py --tf_checkpoint_path ./current_model/model.ckpt-{model_number} --config_file ./current_model/config.json --pytorch_dump_path ./dumps"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘current_model’: File exists\n",
            "Copying gs://code_review_automation/fine_tuning/HP_tuning/final_version/comment_v1/codeANDcomment_code/pretrained/model.ckpt-240000.data-00000-of-00002...\n",
            "Copying gs://code_review_automation/fine_tuning/HP_tuning/final_version/comment_v1/codeANDcomment_code/pretrained/model.ckpt-240000.data-00001-of-00002...\n",
            "Copying gs://code_review_automation/fine_tuning/HP_tuning/final_version/comment_v1/codeANDcomment_code/pretrained/model.ckpt-240000.index...\n",
            "Copying gs://code_review_automation/fine_tuning/HP_tuning/final_version/comment_v1/codeANDcomment_code/pretrained/model.ckpt-240000.meta...\n",
            "/ [4/4 files][126.0 MiB/126.0 MiB] 100% Done                                    \n",
            "Operation completed over 4 objects/126.0 MiB.                                    \n",
            "2021-04-09 17:19:20.249001: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "Building PyTorch model from configuration: T5Config {\n",
            "  \"architectures\": [\n",
            "    \"T5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 512,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"n_positions\": 512,\n",
            "  \"num_decoder_layers\": 6,\n",
            "  \"num_heads\": 8,\n",
            "  \"num_layers\": 6,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "Converting TensorFlow checkpoint from /content/current_model/model.ckpt-240000\n",
            "Loading TF weight decoder/block_000/layer_000/SelfAttention/k with shape [512, 512]\n",
            "Loading TF weight decoder/block_000/layer_000/SelfAttention/k_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_000/layer_000/SelfAttention/k_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_000/layer_000/SelfAttention/o with shape [512, 512]\n",
            "Loading TF weight decoder/block_000/layer_000/SelfAttention/o_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_000/layer_000/SelfAttention/o_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_000/layer_000/SelfAttention/q with shape [512, 512]\n",
            "Loading TF weight decoder/block_000/layer_000/SelfAttention/q_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_000/layer_000/SelfAttention/q_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_000/layer_000/SelfAttention/relative_attention_bias with shape [8, 32]\n",
            "Loading TF weight decoder/block_000/layer_000/SelfAttention/relative_attention_bias_slot_v with shape [8, 32]\n",
            "Loading TF weight decoder/block_000/layer_000/SelfAttention/v with shape [512, 512]\n",
            "Loading TF weight decoder/block_000/layer_000/SelfAttention/v_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_000/layer_000/SelfAttention/v_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_000/layer_000/layer_norm/scale with shape [512]\n",
            "Loading TF weight decoder/block_000/layer_000/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight decoder/block_000/layer_001/EncDecAttention/k with shape [512, 512]\n",
            "Loading TF weight decoder/block_000/layer_001/EncDecAttention/k_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_000/layer_001/EncDecAttention/k_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_000/layer_001/EncDecAttention/o with shape [512, 512]\n",
            "Loading TF weight decoder/block_000/layer_001/EncDecAttention/o_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_000/layer_001/EncDecAttention/o_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_000/layer_001/EncDecAttention/q with shape [512, 512]\n",
            "Loading TF weight decoder/block_000/layer_001/EncDecAttention/q_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_000/layer_001/EncDecAttention/q_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_000/layer_001/EncDecAttention/v with shape [512, 512]\n",
            "Loading TF weight decoder/block_000/layer_001/EncDecAttention/v_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_000/layer_001/EncDecAttention/v_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_000/layer_001/layer_norm/scale with shape [512]\n",
            "Loading TF weight decoder/block_000/layer_001/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight decoder/block_000/layer_002/DenseReluDense/wi/kernel with shape [512, 2048]\n",
            "Loading TF weight decoder/block_000/layer_002/DenseReluDense/wi/kernel_slot_vc with shape [2048]\n",
            "Loading TF weight decoder/block_000/layer_002/DenseReluDense/wi/kernel_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_000/layer_002/DenseReluDense/wo/kernel with shape [2048, 512]\n",
            "Loading TF weight decoder/block_000/layer_002/DenseReluDense/wo/kernel_slot_vc with shape [2048]\n",
            "Loading TF weight decoder/block_000/layer_002/DenseReluDense/wo/kernel_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_000/layer_002/layer_norm/scale with shape [512]\n",
            "Loading TF weight decoder/block_000/layer_002/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight decoder/block_001/layer_000/SelfAttention/k with shape [512, 512]\n",
            "Loading TF weight decoder/block_001/layer_000/SelfAttention/k_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_001/layer_000/SelfAttention/k_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_001/layer_000/SelfAttention/o with shape [512, 512]\n",
            "Loading TF weight decoder/block_001/layer_000/SelfAttention/o_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_001/layer_000/SelfAttention/o_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_001/layer_000/SelfAttention/q with shape [512, 512]\n",
            "Loading TF weight decoder/block_001/layer_000/SelfAttention/q_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_001/layer_000/SelfAttention/q_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_001/layer_000/SelfAttention/v with shape [512, 512]\n",
            "Loading TF weight decoder/block_001/layer_000/SelfAttention/v_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_001/layer_000/SelfAttention/v_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_001/layer_000/layer_norm/scale with shape [512]\n",
            "Loading TF weight decoder/block_001/layer_000/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight decoder/block_001/layer_001/EncDecAttention/k with shape [512, 512]\n",
            "Loading TF weight decoder/block_001/layer_001/EncDecAttention/k_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_001/layer_001/EncDecAttention/k_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_001/layer_001/EncDecAttention/o with shape [512, 512]\n",
            "Loading TF weight decoder/block_001/layer_001/EncDecAttention/o_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_001/layer_001/EncDecAttention/o_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_001/layer_001/EncDecAttention/q with shape [512, 512]\n",
            "Loading TF weight decoder/block_001/layer_001/EncDecAttention/q_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_001/layer_001/EncDecAttention/q_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_001/layer_001/EncDecAttention/v with shape [512, 512]\n",
            "Loading TF weight decoder/block_001/layer_001/EncDecAttention/v_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_001/layer_001/EncDecAttention/v_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_001/layer_001/layer_norm/scale with shape [512]\n",
            "Loading TF weight decoder/block_001/layer_001/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight decoder/block_001/layer_002/DenseReluDense/wi/kernel with shape [512, 2048]\n",
            "Loading TF weight decoder/block_001/layer_002/DenseReluDense/wi/kernel_slot_vc with shape [2048]\n",
            "Loading TF weight decoder/block_001/layer_002/DenseReluDense/wi/kernel_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_001/layer_002/DenseReluDense/wo/kernel with shape [2048, 512]\n",
            "Loading TF weight decoder/block_001/layer_002/DenseReluDense/wo/kernel_slot_vc with shape [2048]\n",
            "Loading TF weight decoder/block_001/layer_002/DenseReluDense/wo/kernel_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_001/layer_002/layer_norm/scale with shape [512]\n",
            "Loading TF weight decoder/block_001/layer_002/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight decoder/block_002/layer_000/SelfAttention/k with shape [512, 512]\n",
            "Loading TF weight decoder/block_002/layer_000/SelfAttention/k_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_002/layer_000/SelfAttention/k_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_002/layer_000/SelfAttention/o with shape [512, 512]\n",
            "Loading TF weight decoder/block_002/layer_000/SelfAttention/o_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_002/layer_000/SelfAttention/o_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_002/layer_000/SelfAttention/q with shape [512, 512]\n",
            "Loading TF weight decoder/block_002/layer_000/SelfAttention/q_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_002/layer_000/SelfAttention/q_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_002/layer_000/SelfAttention/v with shape [512, 512]\n",
            "Loading TF weight decoder/block_002/layer_000/SelfAttention/v_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_002/layer_000/SelfAttention/v_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_002/layer_000/layer_norm/scale with shape [512]\n",
            "Loading TF weight decoder/block_002/layer_000/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight decoder/block_002/layer_001/EncDecAttention/k with shape [512, 512]\n",
            "Loading TF weight decoder/block_002/layer_001/EncDecAttention/k_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_002/layer_001/EncDecAttention/k_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_002/layer_001/EncDecAttention/o with shape [512, 512]\n",
            "Loading TF weight decoder/block_002/layer_001/EncDecAttention/o_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_002/layer_001/EncDecAttention/o_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_002/layer_001/EncDecAttention/q with shape [512, 512]\n",
            "Loading TF weight decoder/block_002/layer_001/EncDecAttention/q_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_002/layer_001/EncDecAttention/q_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_002/layer_001/EncDecAttention/v with shape [512, 512]\n",
            "Loading TF weight decoder/block_002/layer_001/EncDecAttention/v_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_002/layer_001/EncDecAttention/v_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_002/layer_001/layer_norm/scale with shape [512]\n",
            "Loading TF weight decoder/block_002/layer_001/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight decoder/block_002/layer_002/DenseReluDense/wi/kernel with shape [512, 2048]\n",
            "Loading TF weight decoder/block_002/layer_002/DenseReluDense/wi/kernel_slot_vc with shape [2048]\n",
            "Loading TF weight decoder/block_002/layer_002/DenseReluDense/wi/kernel_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_002/layer_002/DenseReluDense/wo/kernel with shape [2048, 512]\n",
            "Loading TF weight decoder/block_002/layer_002/DenseReluDense/wo/kernel_slot_vc with shape [2048]\n",
            "Loading TF weight decoder/block_002/layer_002/DenseReluDense/wo/kernel_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_002/layer_002/layer_norm/scale with shape [512]\n",
            "Loading TF weight decoder/block_002/layer_002/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight decoder/block_003/layer_000/SelfAttention/k with shape [512, 512]\n",
            "Loading TF weight decoder/block_003/layer_000/SelfAttention/k_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_003/layer_000/SelfAttention/k_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_003/layer_000/SelfAttention/o with shape [512, 512]\n",
            "Loading TF weight decoder/block_003/layer_000/SelfAttention/o_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_003/layer_000/SelfAttention/o_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_003/layer_000/SelfAttention/q with shape [512, 512]\n",
            "Loading TF weight decoder/block_003/layer_000/SelfAttention/q_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_003/layer_000/SelfAttention/q_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_003/layer_000/SelfAttention/v with shape [512, 512]\n",
            "Loading TF weight decoder/block_003/layer_000/SelfAttention/v_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_003/layer_000/SelfAttention/v_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_003/layer_000/layer_norm/scale with shape [512]\n",
            "Loading TF weight decoder/block_003/layer_000/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight decoder/block_003/layer_001/EncDecAttention/k with shape [512, 512]\n",
            "Loading TF weight decoder/block_003/layer_001/EncDecAttention/k_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_003/layer_001/EncDecAttention/k_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_003/layer_001/EncDecAttention/o with shape [512, 512]\n",
            "Loading TF weight decoder/block_003/layer_001/EncDecAttention/o_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_003/layer_001/EncDecAttention/o_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_003/layer_001/EncDecAttention/q with shape [512, 512]\n",
            "Loading TF weight decoder/block_003/layer_001/EncDecAttention/q_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_003/layer_001/EncDecAttention/q_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_003/layer_001/EncDecAttention/v with shape [512, 512]\n",
            "Loading TF weight decoder/block_003/layer_001/EncDecAttention/v_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_003/layer_001/EncDecAttention/v_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_003/layer_001/layer_norm/scale with shape [512]\n",
            "Loading TF weight decoder/block_003/layer_001/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight decoder/block_003/layer_002/DenseReluDense/wi/kernel with shape [512, 2048]\n",
            "Loading TF weight decoder/block_003/layer_002/DenseReluDense/wi/kernel_slot_vc with shape [2048]\n",
            "Loading TF weight decoder/block_003/layer_002/DenseReluDense/wi/kernel_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_003/layer_002/DenseReluDense/wo/kernel with shape [2048, 512]\n",
            "Loading TF weight decoder/block_003/layer_002/DenseReluDense/wo/kernel_slot_vc with shape [2048]\n",
            "Loading TF weight decoder/block_003/layer_002/DenseReluDense/wo/kernel_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_003/layer_002/layer_norm/scale with shape [512]\n",
            "Loading TF weight decoder/block_003/layer_002/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight decoder/block_004/layer_000/SelfAttention/k with shape [512, 512]\n",
            "Loading TF weight decoder/block_004/layer_000/SelfAttention/k_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_004/layer_000/SelfAttention/k_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_004/layer_000/SelfAttention/o with shape [512, 512]\n",
            "Loading TF weight decoder/block_004/layer_000/SelfAttention/o_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_004/layer_000/SelfAttention/o_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_004/layer_000/SelfAttention/q with shape [512, 512]\n",
            "Loading TF weight decoder/block_004/layer_000/SelfAttention/q_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_004/layer_000/SelfAttention/q_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_004/layer_000/SelfAttention/v with shape [512, 512]\n",
            "Loading TF weight decoder/block_004/layer_000/SelfAttention/v_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_004/layer_000/SelfAttention/v_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_004/layer_000/layer_norm/scale with shape [512]\n",
            "Loading TF weight decoder/block_004/layer_000/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight decoder/block_004/layer_001/EncDecAttention/k with shape [512, 512]\n",
            "Loading TF weight decoder/block_004/layer_001/EncDecAttention/k_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_004/layer_001/EncDecAttention/k_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_004/layer_001/EncDecAttention/o with shape [512, 512]\n",
            "Loading TF weight decoder/block_004/layer_001/EncDecAttention/o_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_004/layer_001/EncDecAttention/o_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_004/layer_001/EncDecAttention/q with shape [512, 512]\n",
            "Loading TF weight decoder/block_004/layer_001/EncDecAttention/q_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_004/layer_001/EncDecAttention/q_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_004/layer_001/EncDecAttention/v with shape [512, 512]\n",
            "Loading TF weight decoder/block_004/layer_001/EncDecAttention/v_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_004/layer_001/EncDecAttention/v_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_004/layer_001/layer_norm/scale with shape [512]\n",
            "Loading TF weight decoder/block_004/layer_001/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight decoder/block_004/layer_002/DenseReluDense/wi/kernel with shape [512, 2048]\n",
            "Loading TF weight decoder/block_004/layer_002/DenseReluDense/wi/kernel_slot_vc with shape [2048]\n",
            "Loading TF weight decoder/block_004/layer_002/DenseReluDense/wi/kernel_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_004/layer_002/DenseReluDense/wo/kernel with shape [2048, 512]\n",
            "Loading TF weight decoder/block_004/layer_002/DenseReluDense/wo/kernel_slot_vc with shape [2048]\n",
            "Loading TF weight decoder/block_004/layer_002/DenseReluDense/wo/kernel_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_004/layer_002/layer_norm/scale with shape [512]\n",
            "Loading TF weight decoder/block_004/layer_002/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight decoder/block_005/layer_000/SelfAttention/k with shape [512, 512]\n",
            "Loading TF weight decoder/block_005/layer_000/SelfAttention/k_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_005/layer_000/SelfAttention/k_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_005/layer_000/SelfAttention/o with shape [512, 512]\n",
            "Loading TF weight decoder/block_005/layer_000/SelfAttention/o_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_005/layer_000/SelfAttention/o_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_005/layer_000/SelfAttention/q with shape [512, 512]\n",
            "Loading TF weight decoder/block_005/layer_000/SelfAttention/q_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_005/layer_000/SelfAttention/q_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_005/layer_000/SelfAttention/v with shape [512, 512]\n",
            "Loading TF weight decoder/block_005/layer_000/SelfAttention/v_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_005/layer_000/SelfAttention/v_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_005/layer_000/layer_norm/scale with shape [512]\n",
            "Loading TF weight decoder/block_005/layer_000/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight decoder/block_005/layer_001/EncDecAttention/k with shape [512, 512]\n",
            "Loading TF weight decoder/block_005/layer_001/EncDecAttention/k_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_005/layer_001/EncDecAttention/k_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_005/layer_001/EncDecAttention/o with shape [512, 512]\n",
            "Loading TF weight decoder/block_005/layer_001/EncDecAttention/o_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_005/layer_001/EncDecAttention/o_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_005/layer_001/EncDecAttention/q with shape [512, 512]\n",
            "Loading TF weight decoder/block_005/layer_001/EncDecAttention/q_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_005/layer_001/EncDecAttention/q_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_005/layer_001/EncDecAttention/v with shape [512, 512]\n",
            "Loading TF weight decoder/block_005/layer_001/EncDecAttention/v_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_005/layer_001/EncDecAttention/v_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_005/layer_001/layer_norm/scale with shape [512]\n",
            "Loading TF weight decoder/block_005/layer_001/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight decoder/block_005/layer_002/DenseReluDense/wi/kernel with shape [512, 2048]\n",
            "Loading TF weight decoder/block_005/layer_002/DenseReluDense/wi/kernel_slot_vc with shape [2048]\n",
            "Loading TF weight decoder/block_005/layer_002/DenseReluDense/wi/kernel_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_005/layer_002/DenseReluDense/wo/kernel with shape [2048, 512]\n",
            "Loading TF weight decoder/block_005/layer_002/DenseReluDense/wo/kernel_slot_vc with shape [2048]\n",
            "Loading TF weight decoder/block_005/layer_002/DenseReluDense/wo/kernel_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_005/layer_002/layer_norm/scale with shape [512]\n",
            "Loading TF weight decoder/block_005/layer_002/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight decoder/final_layer_norm/scale with shape [512]\n",
            "Loading TF weight decoder/final_layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight encoder/block_000/layer_000/SelfAttention/k with shape [512, 512]\n",
            "Loading TF weight encoder/block_000/layer_000/SelfAttention/k_slot_vc with shape [512]\n",
            "Loading TF weight encoder/block_000/layer_000/SelfAttention/k_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_000/layer_000/SelfAttention/o with shape [512, 512]\n",
            "Loading TF weight encoder/block_000/layer_000/SelfAttention/o_slot_vc with shape [512]\n",
            "Loading TF weight encoder/block_000/layer_000/SelfAttention/o_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_000/layer_000/SelfAttention/q with shape [512, 512]\n",
            "Loading TF weight encoder/block_000/layer_000/SelfAttention/q_slot_vc with shape [512]\n",
            "Loading TF weight encoder/block_000/layer_000/SelfAttention/q_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_000/layer_000/SelfAttention/relative_attention_bias with shape [8, 32]\n",
            "Loading TF weight encoder/block_000/layer_000/SelfAttention/relative_attention_bias_slot_v with shape [8, 32]\n",
            "Loading TF weight encoder/block_000/layer_000/SelfAttention/v with shape [512, 512]\n",
            "Loading TF weight encoder/block_000/layer_000/SelfAttention/v_slot_vc with shape [512]\n",
            "Loading TF weight encoder/block_000/layer_000/SelfAttention/v_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_000/layer_000/layer_norm/scale with shape [512]\n",
            "Loading TF weight encoder/block_000/layer_000/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight encoder/block_000/layer_001/DenseReluDense/wi/kernel with shape [512, 2048]\n",
            "Loading TF weight encoder/block_000/layer_001/DenseReluDense/wi/kernel_slot_vc with shape [2048]\n",
            "Loading TF weight encoder/block_000/layer_001/DenseReluDense/wi/kernel_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_000/layer_001/DenseReluDense/wo/kernel with shape [2048, 512]\n",
            "Loading TF weight encoder/block_000/layer_001/DenseReluDense/wo/kernel_slot_vc with shape [2048]\n",
            "Loading TF weight encoder/block_000/layer_001/DenseReluDense/wo/kernel_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_000/layer_001/layer_norm/scale with shape [512]\n",
            "Loading TF weight encoder/block_000/layer_001/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight encoder/block_001/layer_000/SelfAttention/k with shape [512, 512]\n",
            "Loading TF weight encoder/block_001/layer_000/SelfAttention/k_slot_vc with shape [512]\n",
            "Loading TF weight encoder/block_001/layer_000/SelfAttention/k_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_001/layer_000/SelfAttention/o with shape [512, 512]\n",
            "Loading TF weight encoder/block_001/layer_000/SelfAttention/o_slot_vc with shape [512]\n",
            "Loading TF weight encoder/block_001/layer_000/SelfAttention/o_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_001/layer_000/SelfAttention/q with shape [512, 512]\n",
            "Loading TF weight encoder/block_001/layer_000/SelfAttention/q_slot_vc with shape [512]\n",
            "Loading TF weight encoder/block_001/layer_000/SelfAttention/q_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_001/layer_000/SelfAttention/v with shape [512, 512]\n",
            "Loading TF weight encoder/block_001/layer_000/SelfAttention/v_slot_vc with shape [512]\n",
            "Loading TF weight encoder/block_001/layer_000/SelfAttention/v_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_001/layer_000/layer_norm/scale with shape [512]\n",
            "Loading TF weight encoder/block_001/layer_000/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight encoder/block_001/layer_001/DenseReluDense/wi/kernel with shape [512, 2048]\n",
            "Loading TF weight encoder/block_001/layer_001/DenseReluDense/wi/kernel_slot_vc with shape [2048]\n",
            "Loading TF weight encoder/block_001/layer_001/DenseReluDense/wi/kernel_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_001/layer_001/DenseReluDense/wo/kernel with shape [2048, 512]\n",
            "Loading TF weight encoder/block_001/layer_001/DenseReluDense/wo/kernel_slot_vc with shape [2048]\n",
            "Loading TF weight encoder/block_001/layer_001/DenseReluDense/wo/kernel_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_001/layer_001/layer_norm/scale with shape [512]\n",
            "Loading TF weight encoder/block_001/layer_001/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight encoder/block_002/layer_000/SelfAttention/k with shape [512, 512]\n",
            "Loading TF weight encoder/block_002/layer_000/SelfAttention/k_slot_vc with shape [512]\n",
            "Loading TF weight encoder/block_002/layer_000/SelfAttention/k_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_002/layer_000/SelfAttention/o with shape [512, 512]\n",
            "Loading TF weight encoder/block_002/layer_000/SelfAttention/o_slot_vc with shape [512]\n",
            "Loading TF weight encoder/block_002/layer_000/SelfAttention/o_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_002/layer_000/SelfAttention/q with shape [512, 512]\n",
            "Loading TF weight encoder/block_002/layer_000/SelfAttention/q_slot_vc with shape [512]\n",
            "Loading TF weight encoder/block_002/layer_000/SelfAttention/q_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_002/layer_000/SelfAttention/v with shape [512, 512]\n",
            "Loading TF weight encoder/block_002/layer_000/SelfAttention/v_slot_vc with shape [512]\n",
            "Loading TF weight encoder/block_002/layer_000/SelfAttention/v_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_002/layer_000/layer_norm/scale with shape [512]\n",
            "Loading TF weight encoder/block_002/layer_000/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight encoder/block_002/layer_001/DenseReluDense/wi/kernel with shape [512, 2048]\n",
            "Loading TF weight encoder/block_002/layer_001/DenseReluDense/wi/kernel_slot_vc with shape [2048]\n",
            "Loading TF weight encoder/block_002/layer_001/DenseReluDense/wi/kernel_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_002/layer_001/DenseReluDense/wo/kernel with shape [2048, 512]\n",
            "Loading TF weight encoder/block_002/layer_001/DenseReluDense/wo/kernel_slot_vc with shape [2048]\n",
            "Loading TF weight encoder/block_002/layer_001/DenseReluDense/wo/kernel_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_002/layer_001/layer_norm/scale with shape [512]\n",
            "Loading TF weight encoder/block_002/layer_001/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight encoder/block_003/layer_000/SelfAttention/k with shape [512, 512]\n",
            "Loading TF weight encoder/block_003/layer_000/SelfAttention/k_slot_vc with shape [512]\n",
            "Loading TF weight encoder/block_003/layer_000/SelfAttention/k_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_003/layer_000/SelfAttention/o with shape [512, 512]\n",
            "Loading TF weight encoder/block_003/layer_000/SelfAttention/o_slot_vc with shape [512]\n",
            "Loading TF weight encoder/block_003/layer_000/SelfAttention/o_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_003/layer_000/SelfAttention/q with shape [512, 512]\n",
            "Loading TF weight encoder/block_003/layer_000/SelfAttention/q_slot_vc with shape [512]\n",
            "Loading TF weight encoder/block_003/layer_000/SelfAttention/q_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_003/layer_000/SelfAttention/v with shape [512, 512]\n",
            "Loading TF weight encoder/block_003/layer_000/SelfAttention/v_slot_vc with shape [512]\n",
            "Loading TF weight encoder/block_003/layer_000/SelfAttention/v_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_003/layer_000/layer_norm/scale with shape [512]\n",
            "Loading TF weight encoder/block_003/layer_000/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight encoder/block_003/layer_001/DenseReluDense/wi/kernel with shape [512, 2048]\n",
            "Loading TF weight encoder/block_003/layer_001/DenseReluDense/wi/kernel_slot_vc with shape [2048]\n",
            "Loading TF weight encoder/block_003/layer_001/DenseReluDense/wi/kernel_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_003/layer_001/DenseReluDense/wo/kernel with shape [2048, 512]\n",
            "Loading TF weight encoder/block_003/layer_001/DenseReluDense/wo/kernel_slot_vc with shape [2048]\n",
            "Loading TF weight encoder/block_003/layer_001/DenseReluDense/wo/kernel_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_003/layer_001/layer_norm/scale with shape [512]\n",
            "Loading TF weight encoder/block_003/layer_001/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight encoder/block_004/layer_000/SelfAttention/k with shape [512, 512]\n",
            "Loading TF weight encoder/block_004/layer_000/SelfAttention/k_slot_vc with shape [512]\n",
            "Loading TF weight encoder/block_004/layer_000/SelfAttention/k_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_004/layer_000/SelfAttention/o with shape [512, 512]\n",
            "Loading TF weight encoder/block_004/layer_000/SelfAttention/o_slot_vc with shape [512]\n",
            "Loading TF weight encoder/block_004/layer_000/SelfAttention/o_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_004/layer_000/SelfAttention/q with shape [512, 512]\n",
            "Loading TF weight encoder/block_004/layer_000/SelfAttention/q_slot_vc with shape [512]\n",
            "Loading TF weight encoder/block_004/layer_000/SelfAttention/q_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_004/layer_000/SelfAttention/v with shape [512, 512]\n",
            "Loading TF weight encoder/block_004/layer_000/SelfAttention/v_slot_vc with shape [512]\n",
            "Loading TF weight encoder/block_004/layer_000/SelfAttention/v_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_004/layer_000/layer_norm/scale with shape [512]\n",
            "Loading TF weight encoder/block_004/layer_000/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight encoder/block_004/layer_001/DenseReluDense/wi/kernel with shape [512, 2048]\n",
            "Loading TF weight encoder/block_004/layer_001/DenseReluDense/wi/kernel_slot_vc with shape [2048]\n",
            "Loading TF weight encoder/block_004/layer_001/DenseReluDense/wi/kernel_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_004/layer_001/DenseReluDense/wo/kernel with shape [2048, 512]\n",
            "Loading TF weight encoder/block_004/layer_001/DenseReluDense/wo/kernel_slot_vc with shape [2048]\n",
            "Loading TF weight encoder/block_004/layer_001/DenseReluDense/wo/kernel_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_004/layer_001/layer_norm/scale with shape [512]\n",
            "Loading TF weight encoder/block_004/layer_001/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight encoder/block_005/layer_000/SelfAttention/k with shape [512, 512]\n",
            "Loading TF weight encoder/block_005/layer_000/SelfAttention/k_slot_vc with shape [512]\n",
            "Loading TF weight encoder/block_005/layer_000/SelfAttention/k_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_005/layer_000/SelfAttention/o with shape [512, 512]\n",
            "Loading TF weight encoder/block_005/layer_000/SelfAttention/o_slot_vc with shape [512]\n",
            "Loading TF weight encoder/block_005/layer_000/SelfAttention/o_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_005/layer_000/SelfAttention/q with shape [512, 512]\n",
            "Loading TF weight encoder/block_005/layer_000/SelfAttention/q_slot_vc with shape [512]\n",
            "Loading TF weight encoder/block_005/layer_000/SelfAttention/q_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_005/layer_000/SelfAttention/v with shape [512, 512]\n",
            "Loading TF weight encoder/block_005/layer_000/SelfAttention/v_slot_vc with shape [512]\n",
            "Loading TF weight encoder/block_005/layer_000/SelfAttention/v_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_005/layer_000/layer_norm/scale with shape [512]\n",
            "Loading TF weight encoder/block_005/layer_000/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight encoder/block_005/layer_001/DenseReluDense/wi/kernel with shape [512, 2048]\n",
            "Loading TF weight encoder/block_005/layer_001/DenseReluDense/wi/kernel_slot_vc with shape [2048]\n",
            "Loading TF weight encoder/block_005/layer_001/DenseReluDense/wi/kernel_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_005/layer_001/DenseReluDense/wo/kernel with shape [2048, 512]\n",
            "Loading TF weight encoder/block_005/layer_001/DenseReluDense/wo/kernel_slot_vc with shape [2048]\n",
            "Loading TF weight encoder/block_005/layer_001/DenseReluDense/wo/kernel_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_005/layer_001/layer_norm/scale with shape [512]\n",
            "Loading TF weight encoder/block_005/layer_001/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight encoder/final_layer_norm/scale with shape [512]\n",
            "Loading TF weight encoder/final_layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight global_step with shape []\n",
            "Loading TF weight shared/embedding with shape [32128, 512]\n",
            "Loading TF weight shared/embedding_slot_vc with shape [32128]\n",
            "Loading TF weight shared/embedding_slot_vr with shape [512]\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_000', 'layer_000', 'SelfAttention', 'k']\n",
            "Initialize PyTorch weight ['decoder', 'block_000', 'layer_000', 'SelfAttention', 'k']\n",
            "Skipping decoder/block_000/layer_000/SelfAttention/k_slot_vc\n",
            "Skipping decoder/block_000/layer_000/SelfAttention/k_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_000', 'layer_000', 'SelfAttention', 'o']\n",
            "Initialize PyTorch weight ['decoder', 'block_000', 'layer_000', 'SelfAttention', 'o']\n",
            "Skipping decoder/block_000/layer_000/SelfAttention/o_slot_vc\n",
            "Skipping decoder/block_000/layer_000/SelfAttention/o_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_000', 'layer_000', 'SelfAttention', 'q']\n",
            "Initialize PyTorch weight ['decoder', 'block_000', 'layer_000', 'SelfAttention', 'q']\n",
            "Skipping decoder/block_000/layer_000/SelfAttention/q_slot_vc\n",
            "Skipping decoder/block_000/layer_000/SelfAttention/q_slot_vr\n",
            "Transposing numpy weight of shape (8, 32) for ['decoder', 'block_000', 'layer_000', 'SelfAttention', 'relative_attention_bias']\n",
            "Initialize PyTorch weight ['decoder', 'block_000', 'layer_000', 'SelfAttention', 'relative_attention_bias']\n",
            "Skipping decoder/block_000/layer_000/SelfAttention/relative_attention_bias_slot_v\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_000', 'layer_000', 'SelfAttention', 'v']\n",
            "Initialize PyTorch weight ['decoder', 'block_000', 'layer_000', 'SelfAttention', 'v']\n",
            "Skipping decoder/block_000/layer_000/SelfAttention/v_slot_vc\n",
            "Skipping decoder/block_000/layer_000/SelfAttention/v_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['decoder', 'block_000', 'layer_000', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['decoder', 'block_000', 'layer_000', 'layer_norm', 'scale']\n",
            "Skipping decoder/block_000/layer_000/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_000', 'layer_001', 'EncDecAttention', 'k']\n",
            "Initialize PyTorch weight ['decoder', 'block_000', 'layer_001', 'EncDecAttention', 'k']\n",
            "Skipping decoder/block_000/layer_001/EncDecAttention/k_slot_vc\n",
            "Skipping decoder/block_000/layer_001/EncDecAttention/k_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_000', 'layer_001', 'EncDecAttention', 'o']\n",
            "Initialize PyTorch weight ['decoder', 'block_000', 'layer_001', 'EncDecAttention', 'o']\n",
            "Skipping decoder/block_000/layer_001/EncDecAttention/o_slot_vc\n",
            "Skipping decoder/block_000/layer_001/EncDecAttention/o_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_000', 'layer_001', 'EncDecAttention', 'q']\n",
            "Initialize PyTorch weight ['decoder', 'block_000', 'layer_001', 'EncDecAttention', 'q']\n",
            "Skipping decoder/block_000/layer_001/EncDecAttention/q_slot_vc\n",
            "Skipping decoder/block_000/layer_001/EncDecAttention/q_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_000', 'layer_001', 'EncDecAttention', 'v']\n",
            "Initialize PyTorch weight ['decoder', 'block_000', 'layer_001', 'EncDecAttention', 'v']\n",
            "Skipping decoder/block_000/layer_001/EncDecAttention/v_slot_vc\n",
            "Skipping decoder/block_000/layer_001/EncDecAttention/v_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['decoder', 'block_000', 'layer_001', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['decoder', 'block_000', 'layer_001', 'layer_norm', 'scale']\n",
            "Skipping decoder/block_000/layer_001/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 2048) for ['decoder', 'block_000', 'layer_002', 'DenseReluDense', 'wi', 'kernel']\n",
            "Initialize PyTorch weight ['decoder', 'block_000', 'layer_002', 'DenseReluDense', 'wi', 'kernel']\n",
            "Skipping decoder/block_000/layer_002/DenseReluDense/wi/kernel_slot_vc\n",
            "Skipping decoder/block_000/layer_002/DenseReluDense/wi/kernel_slot_vr\n",
            "Transposing numpy weight of shape (2048, 512) for ['decoder', 'block_000', 'layer_002', 'DenseReluDense', 'wo', 'kernel']\n",
            "Initialize PyTorch weight ['decoder', 'block_000', 'layer_002', 'DenseReluDense', 'wo', 'kernel']\n",
            "Skipping decoder/block_000/layer_002/DenseReluDense/wo/kernel_slot_vc\n",
            "Skipping decoder/block_000/layer_002/DenseReluDense/wo/kernel_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['decoder', 'block_000', 'layer_002', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['decoder', 'block_000', 'layer_002', 'layer_norm', 'scale']\n",
            "Skipping decoder/block_000/layer_002/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_001', 'layer_000', 'SelfAttention', 'k']\n",
            "Initialize PyTorch weight ['decoder', 'block_001', 'layer_000', 'SelfAttention', 'k']\n",
            "Skipping decoder/block_001/layer_000/SelfAttention/k_slot_vc\n",
            "Skipping decoder/block_001/layer_000/SelfAttention/k_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_001', 'layer_000', 'SelfAttention', 'o']\n",
            "Initialize PyTorch weight ['decoder', 'block_001', 'layer_000', 'SelfAttention', 'o']\n",
            "Skipping decoder/block_001/layer_000/SelfAttention/o_slot_vc\n",
            "Skipping decoder/block_001/layer_000/SelfAttention/o_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_001', 'layer_000', 'SelfAttention', 'q']\n",
            "Initialize PyTorch weight ['decoder', 'block_001', 'layer_000', 'SelfAttention', 'q']\n",
            "Skipping decoder/block_001/layer_000/SelfAttention/q_slot_vc\n",
            "Skipping decoder/block_001/layer_000/SelfAttention/q_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_001', 'layer_000', 'SelfAttention', 'v']\n",
            "Initialize PyTorch weight ['decoder', 'block_001', 'layer_000', 'SelfAttention', 'v']\n",
            "Skipping decoder/block_001/layer_000/SelfAttention/v_slot_vc\n",
            "Skipping decoder/block_001/layer_000/SelfAttention/v_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['decoder', 'block_001', 'layer_000', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['decoder', 'block_001', 'layer_000', 'layer_norm', 'scale']\n",
            "Skipping decoder/block_001/layer_000/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_001', 'layer_001', 'EncDecAttention', 'k']\n",
            "Initialize PyTorch weight ['decoder', 'block_001', 'layer_001', 'EncDecAttention', 'k']\n",
            "Skipping decoder/block_001/layer_001/EncDecAttention/k_slot_vc\n",
            "Skipping decoder/block_001/layer_001/EncDecAttention/k_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_001', 'layer_001', 'EncDecAttention', 'o']\n",
            "Initialize PyTorch weight ['decoder', 'block_001', 'layer_001', 'EncDecAttention', 'o']\n",
            "Skipping decoder/block_001/layer_001/EncDecAttention/o_slot_vc\n",
            "Skipping decoder/block_001/layer_001/EncDecAttention/o_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_001', 'layer_001', 'EncDecAttention', 'q']\n",
            "Initialize PyTorch weight ['decoder', 'block_001', 'layer_001', 'EncDecAttention', 'q']\n",
            "Skipping decoder/block_001/layer_001/EncDecAttention/q_slot_vc\n",
            "Skipping decoder/block_001/layer_001/EncDecAttention/q_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_001', 'layer_001', 'EncDecAttention', 'v']\n",
            "Initialize PyTorch weight ['decoder', 'block_001', 'layer_001', 'EncDecAttention', 'v']\n",
            "Skipping decoder/block_001/layer_001/EncDecAttention/v_slot_vc\n",
            "Skipping decoder/block_001/layer_001/EncDecAttention/v_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['decoder', 'block_001', 'layer_001', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['decoder', 'block_001', 'layer_001', 'layer_norm', 'scale']\n",
            "Skipping decoder/block_001/layer_001/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 2048) for ['decoder', 'block_001', 'layer_002', 'DenseReluDense', 'wi', 'kernel']\n",
            "Initialize PyTorch weight ['decoder', 'block_001', 'layer_002', 'DenseReluDense', 'wi', 'kernel']\n",
            "Skipping decoder/block_001/layer_002/DenseReluDense/wi/kernel_slot_vc\n",
            "Skipping decoder/block_001/layer_002/DenseReluDense/wi/kernel_slot_vr\n",
            "Transposing numpy weight of shape (2048, 512) for ['decoder', 'block_001', 'layer_002', 'DenseReluDense', 'wo', 'kernel']\n",
            "Initialize PyTorch weight ['decoder', 'block_001', 'layer_002', 'DenseReluDense', 'wo', 'kernel']\n",
            "Skipping decoder/block_001/layer_002/DenseReluDense/wo/kernel_slot_vc\n",
            "Skipping decoder/block_001/layer_002/DenseReluDense/wo/kernel_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['decoder', 'block_001', 'layer_002', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['decoder', 'block_001', 'layer_002', 'layer_norm', 'scale']\n",
            "Skipping decoder/block_001/layer_002/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_002', 'layer_000', 'SelfAttention', 'k']\n",
            "Initialize PyTorch weight ['decoder', 'block_002', 'layer_000', 'SelfAttention', 'k']\n",
            "Skipping decoder/block_002/layer_000/SelfAttention/k_slot_vc\n",
            "Skipping decoder/block_002/layer_000/SelfAttention/k_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_002', 'layer_000', 'SelfAttention', 'o']\n",
            "Initialize PyTorch weight ['decoder', 'block_002', 'layer_000', 'SelfAttention', 'o']\n",
            "Skipping decoder/block_002/layer_000/SelfAttention/o_slot_vc\n",
            "Skipping decoder/block_002/layer_000/SelfAttention/o_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_002', 'layer_000', 'SelfAttention', 'q']\n",
            "Initialize PyTorch weight ['decoder', 'block_002', 'layer_000', 'SelfAttention', 'q']\n",
            "Skipping decoder/block_002/layer_000/SelfAttention/q_slot_vc\n",
            "Skipping decoder/block_002/layer_000/SelfAttention/q_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_002', 'layer_000', 'SelfAttention', 'v']\n",
            "Initialize PyTorch weight ['decoder', 'block_002', 'layer_000', 'SelfAttention', 'v']\n",
            "Skipping decoder/block_002/layer_000/SelfAttention/v_slot_vc\n",
            "Skipping decoder/block_002/layer_000/SelfAttention/v_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['decoder', 'block_002', 'layer_000', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['decoder', 'block_002', 'layer_000', 'layer_norm', 'scale']\n",
            "Skipping decoder/block_002/layer_000/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_002', 'layer_001', 'EncDecAttention', 'k']\n",
            "Initialize PyTorch weight ['decoder', 'block_002', 'layer_001', 'EncDecAttention', 'k']\n",
            "Skipping decoder/block_002/layer_001/EncDecAttention/k_slot_vc\n",
            "Skipping decoder/block_002/layer_001/EncDecAttention/k_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_002', 'layer_001', 'EncDecAttention', 'o']\n",
            "Initialize PyTorch weight ['decoder', 'block_002', 'layer_001', 'EncDecAttention', 'o']\n",
            "Skipping decoder/block_002/layer_001/EncDecAttention/o_slot_vc\n",
            "Skipping decoder/block_002/layer_001/EncDecAttention/o_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_002', 'layer_001', 'EncDecAttention', 'q']\n",
            "Initialize PyTorch weight ['decoder', 'block_002', 'layer_001', 'EncDecAttention', 'q']\n",
            "Skipping decoder/block_002/layer_001/EncDecAttention/q_slot_vc\n",
            "Skipping decoder/block_002/layer_001/EncDecAttention/q_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_002', 'layer_001', 'EncDecAttention', 'v']\n",
            "Initialize PyTorch weight ['decoder', 'block_002', 'layer_001', 'EncDecAttention', 'v']\n",
            "Skipping decoder/block_002/layer_001/EncDecAttention/v_slot_vc\n",
            "Skipping decoder/block_002/layer_001/EncDecAttention/v_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['decoder', 'block_002', 'layer_001', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['decoder', 'block_002', 'layer_001', 'layer_norm', 'scale']\n",
            "Skipping decoder/block_002/layer_001/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 2048) for ['decoder', 'block_002', 'layer_002', 'DenseReluDense', 'wi', 'kernel']\n",
            "Initialize PyTorch weight ['decoder', 'block_002', 'layer_002', 'DenseReluDense', 'wi', 'kernel']\n",
            "Skipping decoder/block_002/layer_002/DenseReluDense/wi/kernel_slot_vc\n",
            "Skipping decoder/block_002/layer_002/DenseReluDense/wi/kernel_slot_vr\n",
            "Transposing numpy weight of shape (2048, 512) for ['decoder', 'block_002', 'layer_002', 'DenseReluDense', 'wo', 'kernel']\n",
            "Initialize PyTorch weight ['decoder', 'block_002', 'layer_002', 'DenseReluDense', 'wo', 'kernel']\n",
            "Skipping decoder/block_002/layer_002/DenseReluDense/wo/kernel_slot_vc\n",
            "Skipping decoder/block_002/layer_002/DenseReluDense/wo/kernel_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['decoder', 'block_002', 'layer_002', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['decoder', 'block_002', 'layer_002', 'layer_norm', 'scale']\n",
            "Skipping decoder/block_002/layer_002/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_003', 'layer_000', 'SelfAttention', 'k']\n",
            "Initialize PyTorch weight ['decoder', 'block_003', 'layer_000', 'SelfAttention', 'k']\n",
            "Skipping decoder/block_003/layer_000/SelfAttention/k_slot_vc\n",
            "Skipping decoder/block_003/layer_000/SelfAttention/k_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_003', 'layer_000', 'SelfAttention', 'o']\n",
            "Initialize PyTorch weight ['decoder', 'block_003', 'layer_000', 'SelfAttention', 'o']\n",
            "Skipping decoder/block_003/layer_000/SelfAttention/o_slot_vc\n",
            "Skipping decoder/block_003/layer_000/SelfAttention/o_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_003', 'layer_000', 'SelfAttention', 'q']\n",
            "Initialize PyTorch weight ['decoder', 'block_003', 'layer_000', 'SelfAttention', 'q']\n",
            "Skipping decoder/block_003/layer_000/SelfAttention/q_slot_vc\n",
            "Skipping decoder/block_003/layer_000/SelfAttention/q_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_003', 'layer_000', 'SelfAttention', 'v']\n",
            "Initialize PyTorch weight ['decoder', 'block_003', 'layer_000', 'SelfAttention', 'v']\n",
            "Skipping decoder/block_003/layer_000/SelfAttention/v_slot_vc\n",
            "Skipping decoder/block_003/layer_000/SelfAttention/v_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['decoder', 'block_003', 'layer_000', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['decoder', 'block_003', 'layer_000', 'layer_norm', 'scale']\n",
            "Skipping decoder/block_003/layer_000/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_003', 'layer_001', 'EncDecAttention', 'k']\n",
            "Initialize PyTorch weight ['decoder', 'block_003', 'layer_001', 'EncDecAttention', 'k']\n",
            "Skipping decoder/block_003/layer_001/EncDecAttention/k_slot_vc\n",
            "Skipping decoder/block_003/layer_001/EncDecAttention/k_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_003', 'layer_001', 'EncDecAttention', 'o']\n",
            "Initialize PyTorch weight ['decoder', 'block_003', 'layer_001', 'EncDecAttention', 'o']\n",
            "Skipping decoder/block_003/layer_001/EncDecAttention/o_slot_vc\n",
            "Skipping decoder/block_003/layer_001/EncDecAttention/o_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_003', 'layer_001', 'EncDecAttention', 'q']\n",
            "Initialize PyTorch weight ['decoder', 'block_003', 'layer_001', 'EncDecAttention', 'q']\n",
            "Skipping decoder/block_003/layer_001/EncDecAttention/q_slot_vc\n",
            "Skipping decoder/block_003/layer_001/EncDecAttention/q_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_003', 'layer_001', 'EncDecAttention', 'v']\n",
            "Initialize PyTorch weight ['decoder', 'block_003', 'layer_001', 'EncDecAttention', 'v']\n",
            "Skipping decoder/block_003/layer_001/EncDecAttention/v_slot_vc\n",
            "Skipping decoder/block_003/layer_001/EncDecAttention/v_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['decoder', 'block_003', 'layer_001', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['decoder', 'block_003', 'layer_001', 'layer_norm', 'scale']\n",
            "Skipping decoder/block_003/layer_001/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 2048) for ['decoder', 'block_003', 'layer_002', 'DenseReluDense', 'wi', 'kernel']\n",
            "Initialize PyTorch weight ['decoder', 'block_003', 'layer_002', 'DenseReluDense', 'wi', 'kernel']\n",
            "Skipping decoder/block_003/layer_002/DenseReluDense/wi/kernel_slot_vc\n",
            "Skipping decoder/block_003/layer_002/DenseReluDense/wi/kernel_slot_vr\n",
            "Transposing numpy weight of shape (2048, 512) for ['decoder', 'block_003', 'layer_002', 'DenseReluDense', 'wo', 'kernel']\n",
            "Initialize PyTorch weight ['decoder', 'block_003', 'layer_002', 'DenseReluDense', 'wo', 'kernel']\n",
            "Skipping decoder/block_003/layer_002/DenseReluDense/wo/kernel_slot_vc\n",
            "Skipping decoder/block_003/layer_002/DenseReluDense/wo/kernel_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['decoder', 'block_003', 'layer_002', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['decoder', 'block_003', 'layer_002', 'layer_norm', 'scale']\n",
            "Skipping decoder/block_003/layer_002/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_004', 'layer_000', 'SelfAttention', 'k']\n",
            "Initialize PyTorch weight ['decoder', 'block_004', 'layer_000', 'SelfAttention', 'k']\n",
            "Skipping decoder/block_004/layer_000/SelfAttention/k_slot_vc\n",
            "Skipping decoder/block_004/layer_000/SelfAttention/k_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_004', 'layer_000', 'SelfAttention', 'o']\n",
            "Initialize PyTorch weight ['decoder', 'block_004', 'layer_000', 'SelfAttention', 'o']\n",
            "Skipping decoder/block_004/layer_000/SelfAttention/o_slot_vc\n",
            "Skipping decoder/block_004/layer_000/SelfAttention/o_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_004', 'layer_000', 'SelfAttention', 'q']\n",
            "Initialize PyTorch weight ['decoder', 'block_004', 'layer_000', 'SelfAttention', 'q']\n",
            "Skipping decoder/block_004/layer_000/SelfAttention/q_slot_vc\n",
            "Skipping decoder/block_004/layer_000/SelfAttention/q_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_004', 'layer_000', 'SelfAttention', 'v']\n",
            "Initialize PyTorch weight ['decoder', 'block_004', 'layer_000', 'SelfAttention', 'v']\n",
            "Skipping decoder/block_004/layer_000/SelfAttention/v_slot_vc\n",
            "Skipping decoder/block_004/layer_000/SelfAttention/v_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['decoder', 'block_004', 'layer_000', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['decoder', 'block_004', 'layer_000', 'layer_norm', 'scale']\n",
            "Skipping decoder/block_004/layer_000/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_004', 'layer_001', 'EncDecAttention', 'k']\n",
            "Initialize PyTorch weight ['decoder', 'block_004', 'layer_001', 'EncDecAttention', 'k']\n",
            "Skipping decoder/block_004/layer_001/EncDecAttention/k_slot_vc\n",
            "Skipping decoder/block_004/layer_001/EncDecAttention/k_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_004', 'layer_001', 'EncDecAttention', 'o']\n",
            "Initialize PyTorch weight ['decoder', 'block_004', 'layer_001', 'EncDecAttention', 'o']\n",
            "Skipping decoder/block_004/layer_001/EncDecAttention/o_slot_vc\n",
            "Skipping decoder/block_004/layer_001/EncDecAttention/o_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_004', 'layer_001', 'EncDecAttention', 'q']\n",
            "Initialize PyTorch weight ['decoder', 'block_004', 'layer_001', 'EncDecAttention', 'q']\n",
            "Skipping decoder/block_004/layer_001/EncDecAttention/q_slot_vc\n",
            "Skipping decoder/block_004/layer_001/EncDecAttention/q_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_004', 'layer_001', 'EncDecAttention', 'v']\n",
            "Initialize PyTorch weight ['decoder', 'block_004', 'layer_001', 'EncDecAttention', 'v']\n",
            "Skipping decoder/block_004/layer_001/EncDecAttention/v_slot_vc\n",
            "Skipping decoder/block_004/layer_001/EncDecAttention/v_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['decoder', 'block_004', 'layer_001', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['decoder', 'block_004', 'layer_001', 'layer_norm', 'scale']\n",
            "Skipping decoder/block_004/layer_001/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 2048) for ['decoder', 'block_004', 'layer_002', 'DenseReluDense', 'wi', 'kernel']\n",
            "Initialize PyTorch weight ['decoder', 'block_004', 'layer_002', 'DenseReluDense', 'wi', 'kernel']\n",
            "Skipping decoder/block_004/layer_002/DenseReluDense/wi/kernel_slot_vc\n",
            "Skipping decoder/block_004/layer_002/DenseReluDense/wi/kernel_slot_vr\n",
            "Transposing numpy weight of shape (2048, 512) for ['decoder', 'block_004', 'layer_002', 'DenseReluDense', 'wo', 'kernel']\n",
            "Initialize PyTorch weight ['decoder', 'block_004', 'layer_002', 'DenseReluDense', 'wo', 'kernel']\n",
            "Skipping decoder/block_004/layer_002/DenseReluDense/wo/kernel_slot_vc\n",
            "Skipping decoder/block_004/layer_002/DenseReluDense/wo/kernel_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['decoder', 'block_004', 'layer_002', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['decoder', 'block_004', 'layer_002', 'layer_norm', 'scale']\n",
            "Skipping decoder/block_004/layer_002/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_005', 'layer_000', 'SelfAttention', 'k']\n",
            "Initialize PyTorch weight ['decoder', 'block_005', 'layer_000', 'SelfAttention', 'k']\n",
            "Skipping decoder/block_005/layer_000/SelfAttention/k_slot_vc\n",
            "Skipping decoder/block_005/layer_000/SelfAttention/k_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_005', 'layer_000', 'SelfAttention', 'o']\n",
            "Initialize PyTorch weight ['decoder', 'block_005', 'layer_000', 'SelfAttention', 'o']\n",
            "Skipping decoder/block_005/layer_000/SelfAttention/o_slot_vc\n",
            "Skipping decoder/block_005/layer_000/SelfAttention/o_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_005', 'layer_000', 'SelfAttention', 'q']\n",
            "Initialize PyTorch weight ['decoder', 'block_005', 'layer_000', 'SelfAttention', 'q']\n",
            "Skipping decoder/block_005/layer_000/SelfAttention/q_slot_vc\n",
            "Skipping decoder/block_005/layer_000/SelfAttention/q_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_005', 'layer_000', 'SelfAttention', 'v']\n",
            "Initialize PyTorch weight ['decoder', 'block_005', 'layer_000', 'SelfAttention', 'v']\n",
            "Skipping decoder/block_005/layer_000/SelfAttention/v_slot_vc\n",
            "Skipping decoder/block_005/layer_000/SelfAttention/v_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['decoder', 'block_005', 'layer_000', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['decoder', 'block_005', 'layer_000', 'layer_norm', 'scale']\n",
            "Skipping decoder/block_005/layer_000/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_005', 'layer_001', 'EncDecAttention', 'k']\n",
            "Initialize PyTorch weight ['decoder', 'block_005', 'layer_001', 'EncDecAttention', 'k']\n",
            "Skipping decoder/block_005/layer_001/EncDecAttention/k_slot_vc\n",
            "Skipping decoder/block_005/layer_001/EncDecAttention/k_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_005', 'layer_001', 'EncDecAttention', 'o']\n",
            "Initialize PyTorch weight ['decoder', 'block_005', 'layer_001', 'EncDecAttention', 'o']\n",
            "Skipping decoder/block_005/layer_001/EncDecAttention/o_slot_vc\n",
            "Skipping decoder/block_005/layer_001/EncDecAttention/o_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_005', 'layer_001', 'EncDecAttention', 'q']\n",
            "Initialize PyTorch weight ['decoder', 'block_005', 'layer_001', 'EncDecAttention', 'q']\n",
            "Skipping decoder/block_005/layer_001/EncDecAttention/q_slot_vc\n",
            "Skipping decoder/block_005/layer_001/EncDecAttention/q_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_005', 'layer_001', 'EncDecAttention', 'v']\n",
            "Initialize PyTorch weight ['decoder', 'block_005', 'layer_001', 'EncDecAttention', 'v']\n",
            "Skipping decoder/block_005/layer_001/EncDecAttention/v_slot_vc\n",
            "Skipping decoder/block_005/layer_001/EncDecAttention/v_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['decoder', 'block_005', 'layer_001', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['decoder', 'block_005', 'layer_001', 'layer_norm', 'scale']\n",
            "Skipping decoder/block_005/layer_001/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 2048) for ['decoder', 'block_005', 'layer_002', 'DenseReluDense', 'wi', 'kernel']\n",
            "Initialize PyTorch weight ['decoder', 'block_005', 'layer_002', 'DenseReluDense', 'wi', 'kernel']\n",
            "Skipping decoder/block_005/layer_002/DenseReluDense/wi/kernel_slot_vc\n",
            "Skipping decoder/block_005/layer_002/DenseReluDense/wi/kernel_slot_vr\n",
            "Transposing numpy weight of shape (2048, 512) for ['decoder', 'block_005', 'layer_002', 'DenseReluDense', 'wo', 'kernel']\n",
            "Initialize PyTorch weight ['decoder', 'block_005', 'layer_002', 'DenseReluDense', 'wo', 'kernel']\n",
            "Skipping decoder/block_005/layer_002/DenseReluDense/wo/kernel_slot_vc\n",
            "Skipping decoder/block_005/layer_002/DenseReluDense/wo/kernel_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['decoder', 'block_005', 'layer_002', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['decoder', 'block_005', 'layer_002', 'layer_norm', 'scale']\n",
            "Skipping decoder/block_005/layer_002/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512,) for ['decoder', 'final_layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['decoder', 'final_layer_norm', 'scale']\n",
            "Skipping decoder/final_layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 512) for ['encoder', 'block_000', 'layer_000', 'SelfAttention', 'k']\n",
            "Initialize PyTorch weight ['encoder', 'block_000', 'layer_000', 'SelfAttention', 'k']\n",
            "Skipping encoder/block_000/layer_000/SelfAttention/k_slot_vc\n",
            "Skipping encoder/block_000/layer_000/SelfAttention/k_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['encoder', 'block_000', 'layer_000', 'SelfAttention', 'o']\n",
            "Initialize PyTorch weight ['encoder', 'block_000', 'layer_000', 'SelfAttention', 'o']\n",
            "Skipping encoder/block_000/layer_000/SelfAttention/o_slot_vc\n",
            "Skipping encoder/block_000/layer_000/SelfAttention/o_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['encoder', 'block_000', 'layer_000', 'SelfAttention', 'q']\n",
            "Initialize PyTorch weight ['encoder', 'block_000', 'layer_000', 'SelfAttention', 'q']\n",
            "Skipping encoder/block_000/layer_000/SelfAttention/q_slot_vc\n",
            "Skipping encoder/block_000/layer_000/SelfAttention/q_slot_vr\n",
            "Transposing numpy weight of shape (8, 32) for ['encoder', 'block_000', 'layer_000', 'SelfAttention', 'relative_attention_bias']\n",
            "Initialize PyTorch weight ['encoder', 'block_000', 'layer_000', 'SelfAttention', 'relative_attention_bias']\n",
            "Skipping encoder/block_000/layer_000/SelfAttention/relative_attention_bias_slot_v\n",
            "Transposing numpy weight of shape (512, 512) for ['encoder', 'block_000', 'layer_000', 'SelfAttention', 'v']\n",
            "Initialize PyTorch weight ['encoder', 'block_000', 'layer_000', 'SelfAttention', 'v']\n",
            "Skipping encoder/block_000/layer_000/SelfAttention/v_slot_vc\n",
            "Skipping encoder/block_000/layer_000/SelfAttention/v_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['encoder', 'block_000', 'layer_000', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['encoder', 'block_000', 'layer_000', 'layer_norm', 'scale']\n",
            "Skipping encoder/block_000/layer_000/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 2048) for ['encoder', 'block_000', 'layer_001', 'DenseReluDense', 'wi', 'kernel']\n",
            "Initialize PyTorch weight ['encoder', 'block_000', 'layer_001', 'DenseReluDense', 'wi', 'kernel']\n",
            "Skipping encoder/block_000/layer_001/DenseReluDense/wi/kernel_slot_vc\n",
            "Skipping encoder/block_000/layer_001/DenseReluDense/wi/kernel_slot_vr\n",
            "Transposing numpy weight of shape (2048, 512) for ['encoder', 'block_000', 'layer_001', 'DenseReluDense', 'wo', 'kernel']\n",
            "Initialize PyTorch weight ['encoder', 'block_000', 'layer_001', 'DenseReluDense', 'wo', 'kernel']\n",
            "Skipping encoder/block_000/layer_001/DenseReluDense/wo/kernel_slot_vc\n",
            "Skipping encoder/block_000/layer_001/DenseReluDense/wo/kernel_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['encoder', 'block_000', 'layer_001', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['encoder', 'block_000', 'layer_001', 'layer_norm', 'scale']\n",
            "Skipping encoder/block_000/layer_001/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 512) for ['encoder', 'block_001', 'layer_000', 'SelfAttention', 'k']\n",
            "Initialize PyTorch weight ['encoder', 'block_001', 'layer_000', 'SelfAttention', 'k']\n",
            "Skipping encoder/block_001/layer_000/SelfAttention/k_slot_vc\n",
            "Skipping encoder/block_001/layer_000/SelfAttention/k_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['encoder', 'block_001', 'layer_000', 'SelfAttention', 'o']\n",
            "Initialize PyTorch weight ['encoder', 'block_001', 'layer_000', 'SelfAttention', 'o']\n",
            "Skipping encoder/block_001/layer_000/SelfAttention/o_slot_vc\n",
            "Skipping encoder/block_001/layer_000/SelfAttention/o_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['encoder', 'block_001', 'layer_000', 'SelfAttention', 'q']\n",
            "Initialize PyTorch weight ['encoder', 'block_001', 'layer_000', 'SelfAttention', 'q']\n",
            "Skipping encoder/block_001/layer_000/SelfAttention/q_slot_vc\n",
            "Skipping encoder/block_001/layer_000/SelfAttention/q_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['encoder', 'block_001', 'layer_000', 'SelfAttention', 'v']\n",
            "Initialize PyTorch weight ['encoder', 'block_001', 'layer_000', 'SelfAttention', 'v']\n",
            "Skipping encoder/block_001/layer_000/SelfAttention/v_slot_vc\n",
            "Skipping encoder/block_001/layer_000/SelfAttention/v_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['encoder', 'block_001', 'layer_000', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['encoder', 'block_001', 'layer_000', 'layer_norm', 'scale']\n",
            "Skipping encoder/block_001/layer_000/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 2048) for ['encoder', 'block_001', 'layer_001', 'DenseReluDense', 'wi', 'kernel']\n",
            "Initialize PyTorch weight ['encoder', 'block_001', 'layer_001', 'DenseReluDense', 'wi', 'kernel']\n",
            "Skipping encoder/block_001/layer_001/DenseReluDense/wi/kernel_slot_vc\n",
            "Skipping encoder/block_001/layer_001/DenseReluDense/wi/kernel_slot_vr\n",
            "Transposing numpy weight of shape (2048, 512) for ['encoder', 'block_001', 'layer_001', 'DenseReluDense', 'wo', 'kernel']\n",
            "Initialize PyTorch weight ['encoder', 'block_001', 'layer_001', 'DenseReluDense', 'wo', 'kernel']\n",
            "Skipping encoder/block_001/layer_001/DenseReluDense/wo/kernel_slot_vc\n",
            "Skipping encoder/block_001/layer_001/DenseReluDense/wo/kernel_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['encoder', 'block_001', 'layer_001', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['encoder', 'block_001', 'layer_001', 'layer_norm', 'scale']\n",
            "Skipping encoder/block_001/layer_001/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 512) for ['encoder', 'block_002', 'layer_000', 'SelfAttention', 'k']\n",
            "Initialize PyTorch weight ['encoder', 'block_002', 'layer_000', 'SelfAttention', 'k']\n",
            "Skipping encoder/block_002/layer_000/SelfAttention/k_slot_vc\n",
            "Skipping encoder/block_002/layer_000/SelfAttention/k_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['encoder', 'block_002', 'layer_000', 'SelfAttention', 'o']\n",
            "Initialize PyTorch weight ['encoder', 'block_002', 'layer_000', 'SelfAttention', 'o']\n",
            "Skipping encoder/block_002/layer_000/SelfAttention/o_slot_vc\n",
            "Skipping encoder/block_002/layer_000/SelfAttention/o_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['encoder', 'block_002', 'layer_000', 'SelfAttention', 'q']\n",
            "Initialize PyTorch weight ['encoder', 'block_002', 'layer_000', 'SelfAttention', 'q']\n",
            "Skipping encoder/block_002/layer_000/SelfAttention/q_slot_vc\n",
            "Skipping encoder/block_002/layer_000/SelfAttention/q_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['encoder', 'block_002', 'layer_000', 'SelfAttention', 'v']\n",
            "Initialize PyTorch weight ['encoder', 'block_002', 'layer_000', 'SelfAttention', 'v']\n",
            "Skipping encoder/block_002/layer_000/SelfAttention/v_slot_vc\n",
            "Skipping encoder/block_002/layer_000/SelfAttention/v_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['encoder', 'block_002', 'layer_000', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['encoder', 'block_002', 'layer_000', 'layer_norm', 'scale']\n",
            "Skipping encoder/block_002/layer_000/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 2048) for ['encoder', 'block_002', 'layer_001', 'DenseReluDense', 'wi', 'kernel']\n",
            "Initialize PyTorch weight ['encoder', 'block_002', 'layer_001', 'DenseReluDense', 'wi', 'kernel']\n",
            "Skipping encoder/block_002/layer_001/DenseReluDense/wi/kernel_slot_vc\n",
            "Skipping encoder/block_002/layer_001/DenseReluDense/wi/kernel_slot_vr\n",
            "Transposing numpy weight of shape (2048, 512) for ['encoder', 'block_002', 'layer_001', 'DenseReluDense', 'wo', 'kernel']\n",
            "Initialize PyTorch weight ['encoder', 'block_002', 'layer_001', 'DenseReluDense', 'wo', 'kernel']\n",
            "Skipping encoder/block_002/layer_001/DenseReluDense/wo/kernel_slot_vc\n",
            "Skipping encoder/block_002/layer_001/DenseReluDense/wo/kernel_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['encoder', 'block_002', 'layer_001', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['encoder', 'block_002', 'layer_001', 'layer_norm', 'scale']\n",
            "Skipping encoder/block_002/layer_001/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 512) for ['encoder', 'block_003', 'layer_000', 'SelfAttention', 'k']\n",
            "Initialize PyTorch weight ['encoder', 'block_003', 'layer_000', 'SelfAttention', 'k']\n",
            "Skipping encoder/block_003/layer_000/SelfAttention/k_slot_vc\n",
            "Skipping encoder/block_003/layer_000/SelfAttention/k_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['encoder', 'block_003', 'layer_000', 'SelfAttention', 'o']\n",
            "Initialize PyTorch weight ['encoder', 'block_003', 'layer_000', 'SelfAttention', 'o']\n",
            "Skipping encoder/block_003/layer_000/SelfAttention/o_slot_vc\n",
            "Skipping encoder/block_003/layer_000/SelfAttention/o_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['encoder', 'block_003', 'layer_000', 'SelfAttention', 'q']\n",
            "Initialize PyTorch weight ['encoder', 'block_003', 'layer_000', 'SelfAttention', 'q']\n",
            "Skipping encoder/block_003/layer_000/SelfAttention/q_slot_vc\n",
            "Skipping encoder/block_003/layer_000/SelfAttention/q_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['encoder', 'block_003', 'layer_000', 'SelfAttention', 'v']\n",
            "Initialize PyTorch weight ['encoder', 'block_003', 'layer_000', 'SelfAttention', 'v']\n",
            "Skipping encoder/block_003/layer_000/SelfAttention/v_slot_vc\n",
            "Skipping encoder/block_003/layer_000/SelfAttention/v_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['encoder', 'block_003', 'layer_000', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['encoder', 'block_003', 'layer_000', 'layer_norm', 'scale']\n",
            "Skipping encoder/block_003/layer_000/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 2048) for ['encoder', 'block_003', 'layer_001', 'DenseReluDense', 'wi', 'kernel']\n",
            "Initialize PyTorch weight ['encoder', 'block_003', 'layer_001', 'DenseReluDense', 'wi', 'kernel']\n",
            "Skipping encoder/block_003/layer_001/DenseReluDense/wi/kernel_slot_vc\n",
            "Skipping encoder/block_003/layer_001/DenseReluDense/wi/kernel_slot_vr\n",
            "Transposing numpy weight of shape (2048, 512) for ['encoder', 'block_003', 'layer_001', 'DenseReluDense', 'wo', 'kernel']\n",
            "Initialize PyTorch weight ['encoder', 'block_003', 'layer_001', 'DenseReluDense', 'wo', 'kernel']\n",
            "Skipping encoder/block_003/layer_001/DenseReluDense/wo/kernel_slot_vc\n",
            "Skipping encoder/block_003/layer_001/DenseReluDense/wo/kernel_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['encoder', 'block_003', 'layer_001', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['encoder', 'block_003', 'layer_001', 'layer_norm', 'scale']\n",
            "Skipping encoder/block_003/layer_001/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 512) for ['encoder', 'block_004', 'layer_000', 'SelfAttention', 'k']\n",
            "Initialize PyTorch weight ['encoder', 'block_004', 'layer_000', 'SelfAttention', 'k']\n",
            "Skipping encoder/block_004/layer_000/SelfAttention/k_slot_vc\n",
            "Skipping encoder/block_004/layer_000/SelfAttention/k_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['encoder', 'block_004', 'layer_000', 'SelfAttention', 'o']\n",
            "Initialize PyTorch weight ['encoder', 'block_004', 'layer_000', 'SelfAttention', 'o']\n",
            "Skipping encoder/block_004/layer_000/SelfAttention/o_slot_vc\n",
            "Skipping encoder/block_004/layer_000/SelfAttention/o_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['encoder', 'block_004', 'layer_000', 'SelfAttention', 'q']\n",
            "Initialize PyTorch weight ['encoder', 'block_004', 'layer_000', 'SelfAttention', 'q']\n",
            "Skipping encoder/block_004/layer_000/SelfAttention/q_slot_vc\n",
            "Skipping encoder/block_004/layer_000/SelfAttention/q_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['encoder', 'block_004', 'layer_000', 'SelfAttention', 'v']\n",
            "Initialize PyTorch weight ['encoder', 'block_004', 'layer_000', 'SelfAttention', 'v']\n",
            "Skipping encoder/block_004/layer_000/SelfAttention/v_slot_vc\n",
            "Skipping encoder/block_004/layer_000/SelfAttention/v_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['encoder', 'block_004', 'layer_000', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['encoder', 'block_004', 'layer_000', 'layer_norm', 'scale']\n",
            "Skipping encoder/block_004/layer_000/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 2048) for ['encoder', 'block_004', 'layer_001', 'DenseReluDense', 'wi', 'kernel']\n",
            "Initialize PyTorch weight ['encoder', 'block_004', 'layer_001', 'DenseReluDense', 'wi', 'kernel']\n",
            "Skipping encoder/block_004/layer_001/DenseReluDense/wi/kernel_slot_vc\n",
            "Skipping encoder/block_004/layer_001/DenseReluDense/wi/kernel_slot_vr\n",
            "Transposing numpy weight of shape (2048, 512) for ['encoder', 'block_004', 'layer_001', 'DenseReluDense', 'wo', 'kernel']\n",
            "Initialize PyTorch weight ['encoder', 'block_004', 'layer_001', 'DenseReluDense', 'wo', 'kernel']\n",
            "Skipping encoder/block_004/layer_001/DenseReluDense/wo/kernel_slot_vc\n",
            "Skipping encoder/block_004/layer_001/DenseReluDense/wo/kernel_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['encoder', 'block_004', 'layer_001', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['encoder', 'block_004', 'layer_001', 'layer_norm', 'scale']\n",
            "Skipping encoder/block_004/layer_001/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 512) for ['encoder', 'block_005', 'layer_000', 'SelfAttention', 'k']\n",
            "Initialize PyTorch weight ['encoder', 'block_005', 'layer_000', 'SelfAttention', 'k']\n",
            "Skipping encoder/block_005/layer_000/SelfAttention/k_slot_vc\n",
            "Skipping encoder/block_005/layer_000/SelfAttention/k_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['encoder', 'block_005', 'layer_000', 'SelfAttention', 'o']\n",
            "Initialize PyTorch weight ['encoder', 'block_005', 'layer_000', 'SelfAttention', 'o']\n",
            "Skipping encoder/block_005/layer_000/SelfAttention/o_slot_vc\n",
            "Skipping encoder/block_005/layer_000/SelfAttention/o_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['encoder', 'block_005', 'layer_000', 'SelfAttention', 'q']\n",
            "Initialize PyTorch weight ['encoder', 'block_005', 'layer_000', 'SelfAttention', 'q']\n",
            "Skipping encoder/block_005/layer_000/SelfAttention/q_slot_vc\n",
            "Skipping encoder/block_005/layer_000/SelfAttention/q_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['encoder', 'block_005', 'layer_000', 'SelfAttention', 'v']\n",
            "Initialize PyTorch weight ['encoder', 'block_005', 'layer_000', 'SelfAttention', 'v']\n",
            "Skipping encoder/block_005/layer_000/SelfAttention/v_slot_vc\n",
            "Skipping encoder/block_005/layer_000/SelfAttention/v_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['encoder', 'block_005', 'layer_000', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['encoder', 'block_005', 'layer_000', 'layer_norm', 'scale']\n",
            "Skipping encoder/block_005/layer_000/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 2048) for ['encoder', 'block_005', 'layer_001', 'DenseReluDense', 'wi', 'kernel']\n",
            "Initialize PyTorch weight ['encoder', 'block_005', 'layer_001', 'DenseReluDense', 'wi', 'kernel']\n",
            "Skipping encoder/block_005/layer_001/DenseReluDense/wi/kernel_slot_vc\n",
            "Skipping encoder/block_005/layer_001/DenseReluDense/wi/kernel_slot_vr\n",
            "Transposing numpy weight of shape (2048, 512) for ['encoder', 'block_005', 'layer_001', 'DenseReluDense', 'wo', 'kernel']\n",
            "Initialize PyTorch weight ['encoder', 'block_005', 'layer_001', 'DenseReluDense', 'wo', 'kernel']\n",
            "Skipping encoder/block_005/layer_001/DenseReluDense/wo/kernel_slot_vc\n",
            "Skipping encoder/block_005/layer_001/DenseReluDense/wo/kernel_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['encoder', 'block_005', 'layer_001', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['encoder', 'block_005', 'layer_001', 'layer_norm', 'scale']\n",
            "Skipping encoder/block_005/layer_001/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512,) for ['encoder', 'final_layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['encoder', 'final_layer_norm', 'scale']\n",
            "Skipping encoder/final_layer_norm/scale_slot_v\n",
            "Skipping global_step\n",
            "Initialize PyTorch weight ['shared', 'embedding']\n",
            "Skipping shared/embedding_slot_vc\n",
            "Skipping shared/embedding_slot_vr\n",
            "Weights not copied to PyTorch model: \n",
            "Save PyTorch model to ./dumps\n",
            "Configuration saved in ./dumps/config.json\n",
            "Model weights saved in ./dumps/pytorch_model.bin\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V3DnUlPWVyDa",
        "outputId": "726ac6ff-ca83-42e1-bad4-4acf08a35268"
      },
      "source": [
        "# Download the validation script\n",
        "!gsutil cp gs://{bucket_name}/pyTorch_coversion/code/validation.py ./validation.py\n",
        "\n",
        "\n",
        "# Download the model and vocab\n",
        "!gsutil cp gs://{bucket_name}/CodeReviewModel/TestModel.model ./current_model/TestModel.model\n",
        "!gsutil cp gs://{bucket_name}/CodeReviewModel/TestModel.vocab ./current_model/TestModel.vocab\n",
        "\n",
        "# download the test set\n",
        "!gsutil cp gs://{bucket_name}/dataset/old/fineTuningDataset_v1/codeANDcomment_code/test.tsv ./data/codeANDcomment_code/test.tsv\n",
        "\n",
        "# prepare source and target files\n",
        "df = pd.read_csv(\"./data/codeANDcomment_code/test.tsv\", sep='\\t', names=[\"source\",\"target\"])\n",
        "\n",
        "# initialize source and target files\n",
        "f = open(\"./data/codeANDcomment_code/test.source\", \"w\")\n",
        "f.close()\n",
        "f = open(\"./data/codeANDcomment_code/test.target\", \"w\")\n",
        "\n",
        "f.close()\n",
        "with open(\"./data/codeANDcomment_code/test.source\", \"a\") as source:\n",
        "  with open(\"./data/codeANDcomment_code/test.target\", \"a\") as target:\n",
        "    for index, row in df.iterrows():\n",
        "      if index < 20:\n",
        "        source.write(\"code&comment2code: \" + row.source + \"\\n\")\n",
        "        target.write(row.target + \"\\n\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying gs://code_review_automation/pyTorch_coversion/code/validation.py...\n",
            "/ [1 files][  6.1 KiB/  6.1 KiB]                                                \n",
            "Operation completed over 1 objects/6.1 KiB.                                      \n",
            "Copying gs://code_review_automation/CodeReviewModel/TestModel.model...\n",
            "/ [1 files][762.0 KiB/762.0 KiB]                                                \n",
            "Operation completed over 1 objects/762.0 KiB.                                    \n",
            "Copying gs://code_review_automation/CodeReviewModel/TestModel.vocab...\n",
            "/ [1 files][557.4 KiB/557.4 KiB]                                                \n",
            "Operation completed over 1 objects/557.4 KiB.                                    \n",
            "Copying gs://code_review_automation/dataset/old/fineTuningDataset_v1/codeANDcomment_code/test.tsv...\n",
            "/ [1 files][  1.1 MiB/  1.1 MiB]                                                \n",
            "Operation completed over 1 objects/1.1 MiB.                                      \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbBqAZvxqAcM"
      },
      "source": [
        "class EvalDataset(torch.utils.data.Dataset):\n",
        "  samples = []\n",
        "\n",
        "  def __init__(self,  data_dir_path):\n",
        "    data_dir = data_dir_path\n",
        "    datasets = ['test.source', 'test.target']\n",
        "    self.samples = []    \n",
        "\n",
        "    input_file = open(os.path.join(data_dir, 'test.source'), 'r')\n",
        "    output_file = open(os.path.join(data_dir, 'test.target'), 'r')\n",
        "\n",
        "    lines_input = input_file.readlines()\n",
        "    output_lines = output_file.readlines()\n",
        "\n",
        "    for (inp, out) in zip(lines_input, output_lines):\n",
        "      self.samples.append((inp.rstrip(), out.rstrip()))\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    return self.samples[index]\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.samples)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T5MKKN4MjnpL",
        "outputId": "269b7dd1-ba45-4d64-9421-2161d454c0a6"
      },
      "source": [
        "beam_size = 3\n",
        "batch_size = 10\n",
        "data_dir = \"/content/data/codeANDcomment_code\"\n",
        "tokenizer_name = \"./current_model/TestModel.model\" \n",
        "model_name_or_path = \"./dumps/pytorch_model.bin\"\n",
        "config_name = \"./current_model/config.json\"\n",
        "\n",
        "dataset = EvalDataset(data_dir)\n",
        "dloader = DataLoader(dataset=dataset, batch_size=batch_size, num_workers=0)\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # My envirnment uses CPU\n",
        "\n",
        "t5_tokenizer = T5Tokenizer.from_pretrained(tokenizer_name)\n",
        "\n",
        "t5_config = T5Config.from_pretrained(config_name)\n",
        "t5_mlm = T5ForConditionalGeneration.from_pretrained(model_name_or_path, config=t5_config).to(DEVICE)\n",
        "        \n",
        "# t5_mlm.eval()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Calling T5Tokenizer.from_pretrained() with the path to a single file or url is deprecated\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sRBQbX1Mm8W6",
        "outputId": "ea0c58d9-cacb-48f2-ea2a-2702ddc40a31"
      },
      "source": [
        "predictions = []\n",
        "code_bleues = []\n",
        "\n",
        "perfect = 0\n",
        "\n",
        "# indexes for batches\n",
        "old = 0\n",
        "new = batch_size * beam_size\n",
        "\n",
        "for batch in tqdm(dloader): \n",
        "  encoded = t5_tokenizer.batch_encode_plus(batch[0], add_special_tokens=False, return_tensors='pt', padding=True)\n",
        "  \n",
        "  input_ids = encoded['input_ids'].to(DEVICE)\n",
        "  attention_mask = encoded['attention_mask'].to(DEVICE)\n",
        "\n",
        "  outputs = t5_mlm.generate(\n",
        "      input_ids=input_ids,\n",
        "      max_length=512, #Change here \n",
        "      num_beams=beam_size,\n",
        "      attention_mask=attention_mask,\n",
        "      early_stopping=True,\n",
        "      num_return_sequences=beam_size).to(DEVICE)\n",
        "\n",
        "  predictions.extend(t5_tokenizer.batch_decode(outputs, skip_special_tokens=True))\n",
        "\n",
        "  to_analyze = predictions[old:new]\n",
        "  target_list = batch[1]\n",
        "  input_list = batch[0]\n",
        "\n",
        "  idx = 0\n",
        "  for (input_item, target_item) in zip(input_list,target_list):\n",
        "\n",
        "    flag_perfect = False\n",
        "    target_item = \" \".join(target_item.split(' '))\n",
        "    best_code_bleu = 0\n",
        "    \n",
        "    for i in range(beam_size):\n",
        "      prediction_item = \" \".join(to_analyze[idx].split(' '))\n",
        "\n",
        "      if not flag_perfect and prediction_item == target_item:\n",
        "        perfect += 1\n",
        "        flag_perfect = True\n",
        "      \n",
        "      idx += 1\n",
        "\n",
        "      with open(\"code_bleu_target.txt\", \"w\") as target_cb:\n",
        "        target_cb.write(target_item + \"\\n\")\n",
        "      with open(\"code_bleu_prediction.txt\", \"w\") as prediction_cb:\n",
        "        prediction_cb.write(prediction_item + \"\\n\")\n",
        "\n",
        "      try:\n",
        "        result = !cd /content/CodeXGLUE/Code-Code/code-to-code-trans/evaluator/CodeBLEU/ && python calc_code_bleu.py --refs ./../../../../../code_bleu_target.txt --hyp ./../../../../../code_bleu_prediction.txt --lang java --params 0.25,0.25,0.25,0.25\n",
        "        code_bleu = float(result[1][result[1].index(\"CodeBLEU score:  \")+17:])\n",
        "        best_code_bleu = code_bleu if code_bleu > best_code_bleu else best_code_bleu\n",
        "      \n",
        "      except: \n",
        "        print(\"problem in computing code bleu\")\n",
        "        print(result)\n",
        "        print(target_item)\n",
        "        print(prediction_item)\n",
        "    code_bleues.append(best_code_bleu)\n",
        "\n",
        "  old = new\n",
        "  new = new + (batch_size * beam_size)\n",
        "\n",
        "   \n",
        "print()\n",
        "print('#perfect prediction: ', perfect)\n",
        "print('Perfect prediction {}'.format((perfect/len(dataset))*100))\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 50%|█████     | 1/2 [00:09<00:09,  9.28s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "problem in computing code bleu\n",
            "['Traceback (most recent call last):', '  File \"calc_code_bleu.py\", line 64, in <module>', '    dataflow_match_score = dataflow_match.corpus_dataflow_match(references, hypothesis, args.lang)', '  File \"/content/CodeXGLUE/Code-Code/code-to-code-trans/evaluator/CodeBLEU/dataflow_match.py\", line 58, in corpus_dataflow_match', '    score = match_count / total_count', 'ZeroDivisionError: division by zero']\n",
            "private HashMap<String, ArrayList<Order>> getBuyOrders() { return buyOrders; }\n",
            "private HashMap<String, ArrayList<Order>> getBuyOrders() { return buyOrders; }\n",
            "problem in computing code bleu\n",
            "['Traceback (most recent call last):', '  File \"calc_code_bleu.py\", line 64, in <module>', '    dataflow_match_score = dataflow_match.corpus_dataflow_match(references, hypothesis, args.lang)', '  File \"/content/CodeXGLUE/Code-Code/code-to-code-trans/evaluator/CodeBLEU/dataflow_match.py\", line 58, in corpus_dataflow_match', '    score = match_count / total_count', 'ZeroDivisionError: division by zero']\n",
            "private HashMap<String, ArrayList<Order>> getBuyOrders() { return buyOrders; }\n",
            "private Map<String, ArrayList<Order>> getBuyOrders() { return buyOrders; }\n",
            "problem in computing code bleu\n",
            "['Traceback (most recent call last):', '  File \"calc_code_bleu.py\", line 64, in <module>', '    dataflow_match_score = dataflow_match.corpus_dataflow_match(references, hypothesis, args.lang)', '  File \"/content/CodeXGLUE/Code-Code/code-to-code-trans/evaluator/CodeBLEU/dataflow_match.py\", line 58, in corpus_dataflow_match', '    score = match_count / total_count', 'ZeroDivisionError: division by zero']\n",
            "private HashMap<String, ArrayList<Order>> getBuyOrders() { return buyOrders; }\n",
            "private ArrayList<Order>> getBuyOrders() { return buyOrders; }\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 2/2 [00:17<00:00,  8.66s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "#perfect prediction:  6\n",
            "Perfect prediction 30.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pYOv-1jUXJny",
        "outputId": "0392d660-a45b-4a16-a6eb-87e25e1e4da8"
      },
      "source": [
        "code = \"public static void main(String[] args) { \\u003CSTART> System.out.println(\\\" is vowel\\\"); else System.out.println(ch + \\\" is consonant\\\"); \\u003CEND>}\"#@param {type:\"string\"}\n",
        "comment = \"use a logger here\" #@param {type:\"string\"}\n",
        "input = \"code&comment2code: <code>\" + code + \"</code><technical_language>\" + comment + \"</technical_language>\"\n",
        "encoded = t5_tokenizer.encode(input, add_special_tokens=False, return_tensors='pt', padding=True).to(DEVICE)\n",
        "\n",
        "\n",
        "input_ids = encoded\n",
        "outputs = t5_mlm.generate(\n",
        "      input_ids=input_ids,\n",
        "      max_length=512, #Change here \n",
        "      num_beams=1,\n",
        "      early_stopping=True,\n",
        "      num_return_sequences=1).to(DEVICE)\n",
        "\n",
        "print(code)\n",
        "print(t5_tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])\n"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "public static void main(String[] args) { <START> System.out.println(\" is vowel\"); else System.out.println(ch + \" is consonant\"); <END>}\n",
            "public static void main(String[] args) { logger.info(\" is vowel\"); else logger.info(ch + \" is consonant\"); }\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UDFMVaOHVl3u",
        "outputId": "3a6d78f6-0638-4ab6-d599-51ac4c02c1c2"
      },
      "source": [
        "# run validation script\n",
        "!python3 validation.py --beam_size 3 --batch_size 10 --data_dir /content/data/codeANDcomment_code --tokenizer_name ./current_model/TestModel.model --model_name_or_path ./dumps/pytorch_model.bin --config_name ./current_model/config.json\n"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-04-09 15:20:56.282517: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "Calling T5Tokenizer.from_pretrained() with the path to a single file or url is deprecated\n",
            "100% 2/2 [00:09<00:00,  4.88s/it]\n",
            "#perfect prediction:  7\n",
            "Perfect prediction 35.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOMwy8f0xpd5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "264d77e0-4e5f-4c9d-acf8-75fec712d418"
      },
      "source": [
        "# from transformers import T5Tokenizer, T5Config, T5ForConditionalGeneration\n",
        "# t5_tokenizer = T5Tokenizer.from_pretrained(\"./current_model/TestModel.model\")\n",
        "# tokens = t5_tokenizer.encode(\"public static main(String args){ Int a = 4}\")\n",
        "# print(len(tokens))\n",
        "# t5_tokenizer.convert_ids_to_tokens(tokens)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "13\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:1621: FutureWarning: Calling T5Tokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['▁public',\n",
              " '▁static',\n",
              " '▁main',\n",
              " '(',\n",
              " 'String',\n",
              " '▁args',\n",
              " '){',\n",
              " '▁Int',\n",
              " '▁a',\n",
              " '▁=',\n",
              " '▁4',\n",
              " '}',\n",
              " '</s>']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    }
  ]
}