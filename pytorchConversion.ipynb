{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pytorchConversion.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN8GvJTMWv8N6M42bS4RU5f",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/masies/CRA/blob/main/pytorchConversion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8i-CsM5qG8P4",
        "outputId": "9cca065f-2611-4150-b5c9-451a4a5ebce4"
      },
      "source": [
        "import tensorflow as tf\n",
        "import torch\n",
        "!pip3 install transformers\n",
        "import transformers\n",
        "!pip install sentencepiece\n",
        "import sentencepiece as spm\n",
        "import pandas as pd\n",
        "\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "project_id = 'helical-loop-303918'\n",
        "bucket_name = 'code_review_automation'\n",
        "\n",
        "!gcloud config set project {project_id}\n",
        "!gsutil ls"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/81/91/61d69d58a1af1bd81d9ca9d62c90a6de3ab80d77f27c5df65d9a2c1f5626/transformers-4.5.0-py3-none-any.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.2MB 5.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 29.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/08/cd/342e584ee544d044fb573ae697404ce22ede086c9e87ce5960772084cad0/sacremoses-0.0.44.tar.gz (862kB)\n",
            "\u001b[K     |████████████████████████████████| 870kB 40.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.44-cp37-none-any.whl size=886084 sha256=e825b85926dd36b02cbbb20a87f8398b92315848ee861448aed94a51a125bd58\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/fb/c0/13ab4d63d537658f448366744654323077c4d90069b6512f3c\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.44 tokenizers-0.10.2 transformers-4.5.0\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 5.7MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.95\n",
            "Updated property [core/project].\n",
            "gs://code_review_automation/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-xKBubBHvGD",
        "outputId": "0813ba77-fd42-425c-e456-3e03aa570c74"
      },
      "source": [
        "model_number = 240000\n",
        "\n",
        "# script for conversion in pythorch\n",
        "!gsutil cp gs://{bucket_name}/pyTorch_coversion/code/tf_2_pytorch_T5.py ./tf_2_pytorch_T5.py\n",
        "\n",
        "# Download the selected best model\n",
        "!mkdir dumps\n",
        "!mkdir current_model\n",
        "!gsutil -m cp \\\n",
        "  \"gs://code_review_automation/fine_tuning/HP_tuning/final_version/comment_v1/codeANDcomment_code/pretrained/model.ckpt-{model_number}.data-00000-of-00002\" \\\n",
        "  \"gs://code_review_automation/fine_tuning/HP_tuning/final_version/comment_v1/codeANDcomment_code/pretrained/model.ckpt-{model_number}.data-00001-of-00002\" \\\n",
        "  \"gs://code_review_automation/fine_tuning/HP_tuning/final_version/comment_v1/codeANDcomment_code/pretrained/model.ckpt-{model_number}.index\" \\\n",
        "  \"gs://code_review_automation/fine_tuning/HP_tuning/final_version/comment_v1/codeANDcomment_code/pretrained/model.ckpt-{model_number}.meta\" \\\n",
        "  ./current_model/\n",
        "\n",
        "# Download the configuration file\n",
        "!gsutil cp gs://{bucket_name}/pyTorch_coversion/config/config.json ./current_model/config.json\n",
        "\n",
        "# Convert the model\n",
        "!python3 ./tf_2_pytorch_T5.py --tf_checkpoint_path ./current_model/model.ckpt-{model_number} --config_file ./current_model/config.json --pytorch_dump_path ./dumps"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying gs://code_review_automation/pyTorch_coversion/code/tf_2_pytorch_T5.py...\n",
            "- [1 files][  2.3 KiB/  2.3 KiB]                                                \n",
            "Operation completed over 1 objects/2.3 KiB.                                      \n",
            "Copying gs://code_review_automation/fine_tuning/HP_tuning/final_version/comment_v1/codeANDcomment_code/pretrained/model.ckpt-240000.data-00000-of-00002...\n",
            "Copying gs://code_review_automation/fine_tuning/HP_tuning/final_version/comment_v1/codeANDcomment_code/pretrained/model.ckpt-240000.data-00001-of-00002...\n",
            "Copying gs://code_review_automation/fine_tuning/HP_tuning/final_version/comment_v1/codeANDcomment_code/pretrained/model.ckpt-240000.index...\n",
            "Copying gs://code_review_automation/fine_tuning/HP_tuning/final_version/comment_v1/codeANDcomment_code/pretrained/model.ckpt-240000.meta...\n",
            "| [4/4 files][126.0 MiB/126.0 MiB] 100% Done                                    \n",
            "Operation completed over 4 objects/126.0 MiB.                                    \n",
            "Copying gs://code_review_automation/pyTorch_coversion/config/config.json...\n",
            "/ [1 files][  463.0 B/  463.0 B]                                                \n",
            "Operation completed over 1 objects/463.0 B.                                      \n",
            "Building PyTorch model from configuration: T5Config {\n",
            "  \"architectures\": [\n",
            "    \"T5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 512,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"relu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"n_positions\": 512,\n",
            "  \"num_decoder_layers\": 6,\n",
            "  \"num_heads\": 8,\n",
            "  \"num_layers\": 6,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"transformers_version\": \"4.5.0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "2021-04-08 08:24:53.146864: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "Converting TensorFlow checkpoint from /content/current_model/model.ckpt-240000\n",
            "Loading TF weight decoder/block_000/layer_000/SelfAttention/k with shape [512, 512]\n",
            "Loading TF weight decoder/block_000/layer_000/SelfAttention/k_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_000/layer_000/SelfAttention/k_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_000/layer_000/SelfAttention/o with shape [512, 512]\n",
            "Loading TF weight decoder/block_000/layer_000/SelfAttention/o_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_000/layer_000/SelfAttention/o_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_000/layer_000/SelfAttention/q with shape [512, 512]\n",
            "Loading TF weight decoder/block_000/layer_000/SelfAttention/q_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_000/layer_000/SelfAttention/q_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_000/layer_000/SelfAttention/relative_attention_bias with shape [8, 32]\n",
            "Loading TF weight decoder/block_000/layer_000/SelfAttention/relative_attention_bias_slot_v with shape [8, 32]\n",
            "Loading TF weight decoder/block_000/layer_000/SelfAttention/v with shape [512, 512]\n",
            "Loading TF weight decoder/block_000/layer_000/SelfAttention/v_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_000/layer_000/SelfAttention/v_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_000/layer_000/layer_norm/scale with shape [512]\n",
            "Loading TF weight decoder/block_000/layer_000/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight decoder/block_000/layer_001/EncDecAttention/k with shape [512, 512]\n",
            "Loading TF weight decoder/block_000/layer_001/EncDecAttention/k_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_000/layer_001/EncDecAttention/k_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_000/layer_001/EncDecAttention/o with shape [512, 512]\n",
            "Loading TF weight decoder/block_000/layer_001/EncDecAttention/o_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_000/layer_001/EncDecAttention/o_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_000/layer_001/EncDecAttention/q with shape [512, 512]\n",
            "Loading TF weight decoder/block_000/layer_001/EncDecAttention/q_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_000/layer_001/EncDecAttention/q_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_000/layer_001/EncDecAttention/v with shape [512, 512]\n",
            "Loading TF weight decoder/block_000/layer_001/EncDecAttention/v_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_000/layer_001/EncDecAttention/v_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_000/layer_001/layer_norm/scale with shape [512]\n",
            "Loading TF weight decoder/block_000/layer_001/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight decoder/block_000/layer_002/DenseReluDense/wi/kernel with shape [512, 2048]\n",
            "Loading TF weight decoder/block_000/layer_002/DenseReluDense/wi/kernel_slot_vc with shape [2048]\n",
            "Loading TF weight decoder/block_000/layer_002/DenseReluDense/wi/kernel_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_000/layer_002/DenseReluDense/wo/kernel with shape [2048, 512]\n",
            "Loading TF weight decoder/block_000/layer_002/DenseReluDense/wo/kernel_slot_vc with shape [2048]\n",
            "Loading TF weight decoder/block_000/layer_002/DenseReluDense/wo/kernel_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_000/layer_002/layer_norm/scale with shape [512]\n",
            "Loading TF weight decoder/block_000/layer_002/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight decoder/block_001/layer_000/SelfAttention/k with shape [512, 512]\n",
            "Loading TF weight decoder/block_001/layer_000/SelfAttention/k_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_001/layer_000/SelfAttention/k_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_001/layer_000/SelfAttention/o with shape [512, 512]\n",
            "Loading TF weight decoder/block_001/layer_000/SelfAttention/o_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_001/layer_000/SelfAttention/o_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_001/layer_000/SelfAttention/q with shape [512, 512]\n",
            "Loading TF weight decoder/block_001/layer_000/SelfAttention/q_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_001/layer_000/SelfAttention/q_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_001/layer_000/SelfAttention/v with shape [512, 512]\n",
            "Loading TF weight decoder/block_001/layer_000/SelfAttention/v_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_001/layer_000/SelfAttention/v_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_001/layer_000/layer_norm/scale with shape [512]\n",
            "Loading TF weight decoder/block_001/layer_000/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight decoder/block_001/layer_001/EncDecAttention/k with shape [512, 512]\n",
            "Loading TF weight decoder/block_001/layer_001/EncDecAttention/k_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_001/layer_001/EncDecAttention/k_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_001/layer_001/EncDecAttention/o with shape [512, 512]\n",
            "Loading TF weight decoder/block_001/layer_001/EncDecAttention/o_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_001/layer_001/EncDecAttention/o_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_001/layer_001/EncDecAttention/q with shape [512, 512]\n",
            "Loading TF weight decoder/block_001/layer_001/EncDecAttention/q_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_001/layer_001/EncDecAttention/q_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_001/layer_001/EncDecAttention/v with shape [512, 512]\n",
            "Loading TF weight decoder/block_001/layer_001/EncDecAttention/v_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_001/layer_001/EncDecAttention/v_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_001/layer_001/layer_norm/scale with shape [512]\n",
            "Loading TF weight decoder/block_001/layer_001/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight decoder/block_001/layer_002/DenseReluDense/wi/kernel with shape [512, 2048]\n",
            "Loading TF weight decoder/block_001/layer_002/DenseReluDense/wi/kernel_slot_vc with shape [2048]\n",
            "Loading TF weight decoder/block_001/layer_002/DenseReluDense/wi/kernel_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_001/layer_002/DenseReluDense/wo/kernel with shape [2048, 512]\n",
            "Loading TF weight decoder/block_001/layer_002/DenseReluDense/wo/kernel_slot_vc with shape [2048]\n",
            "Loading TF weight decoder/block_001/layer_002/DenseReluDense/wo/kernel_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_001/layer_002/layer_norm/scale with shape [512]\n",
            "Loading TF weight decoder/block_001/layer_002/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight decoder/block_002/layer_000/SelfAttention/k with shape [512, 512]\n",
            "Loading TF weight decoder/block_002/layer_000/SelfAttention/k_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_002/layer_000/SelfAttention/k_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_002/layer_000/SelfAttention/o with shape [512, 512]\n",
            "Loading TF weight decoder/block_002/layer_000/SelfAttention/o_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_002/layer_000/SelfAttention/o_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_002/layer_000/SelfAttention/q with shape [512, 512]\n",
            "Loading TF weight decoder/block_002/layer_000/SelfAttention/q_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_002/layer_000/SelfAttention/q_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_002/layer_000/SelfAttention/v with shape [512, 512]\n",
            "Loading TF weight decoder/block_002/layer_000/SelfAttention/v_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_002/layer_000/SelfAttention/v_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_002/layer_000/layer_norm/scale with shape [512]\n",
            "Loading TF weight decoder/block_002/layer_000/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight decoder/block_002/layer_001/EncDecAttention/k with shape [512, 512]\n",
            "Loading TF weight decoder/block_002/layer_001/EncDecAttention/k_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_002/layer_001/EncDecAttention/k_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_002/layer_001/EncDecAttention/o with shape [512, 512]\n",
            "Loading TF weight decoder/block_002/layer_001/EncDecAttention/o_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_002/layer_001/EncDecAttention/o_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_002/layer_001/EncDecAttention/q with shape [512, 512]\n",
            "Loading TF weight decoder/block_002/layer_001/EncDecAttention/q_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_002/layer_001/EncDecAttention/q_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_002/layer_001/EncDecAttention/v with shape [512, 512]\n",
            "Loading TF weight decoder/block_002/layer_001/EncDecAttention/v_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_002/layer_001/EncDecAttention/v_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_002/layer_001/layer_norm/scale with shape [512]\n",
            "Loading TF weight decoder/block_002/layer_001/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight decoder/block_002/layer_002/DenseReluDense/wi/kernel with shape [512, 2048]\n",
            "Loading TF weight decoder/block_002/layer_002/DenseReluDense/wi/kernel_slot_vc with shape [2048]\n",
            "Loading TF weight decoder/block_002/layer_002/DenseReluDense/wi/kernel_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_002/layer_002/DenseReluDense/wo/kernel with shape [2048, 512]\n",
            "Loading TF weight decoder/block_002/layer_002/DenseReluDense/wo/kernel_slot_vc with shape [2048]\n",
            "Loading TF weight decoder/block_002/layer_002/DenseReluDense/wo/kernel_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_002/layer_002/layer_norm/scale with shape [512]\n",
            "Loading TF weight decoder/block_002/layer_002/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight decoder/block_003/layer_000/SelfAttention/k with shape [512, 512]\n",
            "Loading TF weight decoder/block_003/layer_000/SelfAttention/k_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_003/layer_000/SelfAttention/k_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_003/layer_000/SelfAttention/o with shape [512, 512]\n",
            "Loading TF weight decoder/block_003/layer_000/SelfAttention/o_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_003/layer_000/SelfAttention/o_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_003/layer_000/SelfAttention/q with shape [512, 512]\n",
            "Loading TF weight decoder/block_003/layer_000/SelfAttention/q_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_003/layer_000/SelfAttention/q_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_003/layer_000/SelfAttention/v with shape [512, 512]\n",
            "Loading TF weight decoder/block_003/layer_000/SelfAttention/v_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_003/layer_000/SelfAttention/v_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_003/layer_000/layer_norm/scale with shape [512]\n",
            "Loading TF weight decoder/block_003/layer_000/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight decoder/block_003/layer_001/EncDecAttention/k with shape [512, 512]\n",
            "Loading TF weight decoder/block_003/layer_001/EncDecAttention/k_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_003/layer_001/EncDecAttention/k_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_003/layer_001/EncDecAttention/o with shape [512, 512]\n",
            "Loading TF weight decoder/block_003/layer_001/EncDecAttention/o_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_003/layer_001/EncDecAttention/o_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_003/layer_001/EncDecAttention/q with shape [512, 512]\n",
            "Loading TF weight decoder/block_003/layer_001/EncDecAttention/q_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_003/layer_001/EncDecAttention/q_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_003/layer_001/EncDecAttention/v with shape [512, 512]\n",
            "Loading TF weight decoder/block_003/layer_001/EncDecAttention/v_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_003/layer_001/EncDecAttention/v_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_003/layer_001/layer_norm/scale with shape [512]\n",
            "Loading TF weight decoder/block_003/layer_001/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight decoder/block_003/layer_002/DenseReluDense/wi/kernel with shape [512, 2048]\n",
            "Loading TF weight decoder/block_003/layer_002/DenseReluDense/wi/kernel_slot_vc with shape [2048]\n",
            "Loading TF weight decoder/block_003/layer_002/DenseReluDense/wi/kernel_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_003/layer_002/DenseReluDense/wo/kernel with shape [2048, 512]\n",
            "Loading TF weight decoder/block_003/layer_002/DenseReluDense/wo/kernel_slot_vc with shape [2048]\n",
            "Loading TF weight decoder/block_003/layer_002/DenseReluDense/wo/kernel_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_003/layer_002/layer_norm/scale with shape [512]\n",
            "Loading TF weight decoder/block_003/layer_002/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight decoder/block_004/layer_000/SelfAttention/k with shape [512, 512]\n",
            "Loading TF weight decoder/block_004/layer_000/SelfAttention/k_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_004/layer_000/SelfAttention/k_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_004/layer_000/SelfAttention/o with shape [512, 512]\n",
            "Loading TF weight decoder/block_004/layer_000/SelfAttention/o_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_004/layer_000/SelfAttention/o_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_004/layer_000/SelfAttention/q with shape [512, 512]\n",
            "Loading TF weight decoder/block_004/layer_000/SelfAttention/q_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_004/layer_000/SelfAttention/q_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_004/layer_000/SelfAttention/v with shape [512, 512]\n",
            "Loading TF weight decoder/block_004/layer_000/SelfAttention/v_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_004/layer_000/SelfAttention/v_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_004/layer_000/layer_norm/scale with shape [512]\n",
            "Loading TF weight decoder/block_004/layer_000/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight decoder/block_004/layer_001/EncDecAttention/k with shape [512, 512]\n",
            "Loading TF weight decoder/block_004/layer_001/EncDecAttention/k_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_004/layer_001/EncDecAttention/k_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_004/layer_001/EncDecAttention/o with shape [512, 512]\n",
            "Loading TF weight decoder/block_004/layer_001/EncDecAttention/o_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_004/layer_001/EncDecAttention/o_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_004/layer_001/EncDecAttention/q with shape [512, 512]\n",
            "Loading TF weight decoder/block_004/layer_001/EncDecAttention/q_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_004/layer_001/EncDecAttention/q_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_004/layer_001/EncDecAttention/v with shape [512, 512]\n",
            "Loading TF weight decoder/block_004/layer_001/EncDecAttention/v_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_004/layer_001/EncDecAttention/v_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_004/layer_001/layer_norm/scale with shape [512]\n",
            "Loading TF weight decoder/block_004/layer_001/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight decoder/block_004/layer_002/DenseReluDense/wi/kernel with shape [512, 2048]\n",
            "Loading TF weight decoder/block_004/layer_002/DenseReluDense/wi/kernel_slot_vc with shape [2048]\n",
            "Loading TF weight decoder/block_004/layer_002/DenseReluDense/wi/kernel_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_004/layer_002/DenseReluDense/wo/kernel with shape [2048, 512]\n",
            "Loading TF weight decoder/block_004/layer_002/DenseReluDense/wo/kernel_slot_vc with shape [2048]\n",
            "Loading TF weight decoder/block_004/layer_002/DenseReluDense/wo/kernel_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_004/layer_002/layer_norm/scale with shape [512]\n",
            "Loading TF weight decoder/block_004/layer_002/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight decoder/block_005/layer_000/SelfAttention/k with shape [512, 512]\n",
            "Loading TF weight decoder/block_005/layer_000/SelfAttention/k_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_005/layer_000/SelfAttention/k_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_005/layer_000/SelfAttention/o with shape [512, 512]\n",
            "Loading TF weight decoder/block_005/layer_000/SelfAttention/o_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_005/layer_000/SelfAttention/o_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_005/layer_000/SelfAttention/q with shape [512, 512]\n",
            "Loading TF weight decoder/block_005/layer_000/SelfAttention/q_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_005/layer_000/SelfAttention/q_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_005/layer_000/SelfAttention/v with shape [512, 512]\n",
            "Loading TF weight decoder/block_005/layer_000/SelfAttention/v_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_005/layer_000/SelfAttention/v_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_005/layer_000/layer_norm/scale with shape [512]\n",
            "Loading TF weight decoder/block_005/layer_000/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight decoder/block_005/layer_001/EncDecAttention/k with shape [512, 512]\n",
            "Loading TF weight decoder/block_005/layer_001/EncDecAttention/k_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_005/layer_001/EncDecAttention/k_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_005/layer_001/EncDecAttention/o with shape [512, 512]\n",
            "Loading TF weight decoder/block_005/layer_001/EncDecAttention/o_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_005/layer_001/EncDecAttention/o_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_005/layer_001/EncDecAttention/q with shape [512, 512]\n",
            "Loading TF weight decoder/block_005/layer_001/EncDecAttention/q_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_005/layer_001/EncDecAttention/q_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_005/layer_001/EncDecAttention/v with shape [512, 512]\n",
            "Loading TF weight decoder/block_005/layer_001/EncDecAttention/v_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_005/layer_001/EncDecAttention/v_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_005/layer_001/layer_norm/scale with shape [512]\n",
            "Loading TF weight decoder/block_005/layer_001/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight decoder/block_005/layer_002/DenseReluDense/wi/kernel with shape [512, 2048]\n",
            "Loading TF weight decoder/block_005/layer_002/DenseReluDense/wi/kernel_slot_vc with shape [2048]\n",
            "Loading TF weight decoder/block_005/layer_002/DenseReluDense/wi/kernel_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_005/layer_002/DenseReluDense/wo/kernel with shape [2048, 512]\n",
            "Loading TF weight decoder/block_005/layer_002/DenseReluDense/wo/kernel_slot_vc with shape [2048]\n",
            "Loading TF weight decoder/block_005/layer_002/DenseReluDense/wo/kernel_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_005/layer_002/layer_norm/scale with shape [512]\n",
            "Loading TF weight decoder/block_005/layer_002/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight decoder/final_layer_norm/scale with shape [512]\n",
            "Loading TF weight decoder/final_layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight encoder/block_000/layer_000/SelfAttention/k with shape [512, 512]\n",
            "Loading TF weight encoder/block_000/layer_000/SelfAttention/k_slot_vc with shape [512]\n",
            "Loading TF weight encoder/block_000/layer_000/SelfAttention/k_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_000/layer_000/SelfAttention/o with shape [512, 512]\n",
            "Loading TF weight encoder/block_000/layer_000/SelfAttention/o_slot_vc with shape [512]\n",
            "Loading TF weight encoder/block_000/layer_000/SelfAttention/o_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_000/layer_000/SelfAttention/q with shape [512, 512]\n",
            "Loading TF weight encoder/block_000/layer_000/SelfAttention/q_slot_vc with shape [512]\n",
            "Loading TF weight encoder/block_000/layer_000/SelfAttention/q_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_000/layer_000/SelfAttention/relative_attention_bias with shape [8, 32]\n",
            "Loading TF weight encoder/block_000/layer_000/SelfAttention/relative_attention_bias_slot_v with shape [8, 32]\n",
            "Loading TF weight encoder/block_000/layer_000/SelfAttention/v with shape [512, 512]\n",
            "Loading TF weight encoder/block_000/layer_000/SelfAttention/v_slot_vc with shape [512]\n",
            "Loading TF weight encoder/block_000/layer_000/SelfAttention/v_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_000/layer_000/layer_norm/scale with shape [512]\n",
            "Loading TF weight encoder/block_000/layer_000/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight encoder/block_000/layer_001/DenseReluDense/wi/kernel with shape [512, 2048]\n",
            "Loading TF weight encoder/block_000/layer_001/DenseReluDense/wi/kernel_slot_vc with shape [2048]\n",
            "Loading TF weight encoder/block_000/layer_001/DenseReluDense/wi/kernel_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_000/layer_001/DenseReluDense/wo/kernel with shape [2048, 512]\n",
            "Loading TF weight encoder/block_000/layer_001/DenseReluDense/wo/kernel_slot_vc with shape [2048]\n",
            "Loading TF weight encoder/block_000/layer_001/DenseReluDense/wo/kernel_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_000/layer_001/layer_norm/scale with shape [512]\n",
            "Loading TF weight encoder/block_000/layer_001/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight encoder/block_001/layer_000/SelfAttention/k with shape [512, 512]\n",
            "Loading TF weight encoder/block_001/layer_000/SelfAttention/k_slot_vc with shape [512]\n",
            "Loading TF weight encoder/block_001/layer_000/SelfAttention/k_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_001/layer_000/SelfAttention/o with shape [512, 512]\n",
            "Loading TF weight encoder/block_001/layer_000/SelfAttention/o_slot_vc with shape [512]\n",
            "Loading TF weight encoder/block_001/layer_000/SelfAttention/o_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_001/layer_000/SelfAttention/q with shape [512, 512]\n",
            "Loading TF weight encoder/block_001/layer_000/SelfAttention/q_slot_vc with shape [512]\n",
            "Loading TF weight encoder/block_001/layer_000/SelfAttention/q_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_001/layer_000/SelfAttention/v with shape [512, 512]\n",
            "Loading TF weight encoder/block_001/layer_000/SelfAttention/v_slot_vc with shape [512]\n",
            "Loading TF weight encoder/block_001/layer_000/SelfAttention/v_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_001/layer_000/layer_norm/scale with shape [512]\n",
            "Loading TF weight encoder/block_001/layer_000/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight encoder/block_001/layer_001/DenseReluDense/wi/kernel with shape [512, 2048]\n",
            "Loading TF weight encoder/block_001/layer_001/DenseReluDense/wi/kernel_slot_vc with shape [2048]\n",
            "Loading TF weight encoder/block_001/layer_001/DenseReluDense/wi/kernel_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_001/layer_001/DenseReluDense/wo/kernel with shape [2048, 512]\n",
            "Loading TF weight encoder/block_001/layer_001/DenseReluDense/wo/kernel_slot_vc with shape [2048]\n",
            "Loading TF weight encoder/block_001/layer_001/DenseReluDense/wo/kernel_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_001/layer_001/layer_norm/scale with shape [512]\n",
            "Loading TF weight encoder/block_001/layer_001/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight encoder/block_002/layer_000/SelfAttention/k with shape [512, 512]\n",
            "Loading TF weight encoder/block_002/layer_000/SelfAttention/k_slot_vc with shape [512]\n",
            "Loading TF weight encoder/block_002/layer_000/SelfAttention/k_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_002/layer_000/SelfAttention/o with shape [512, 512]\n",
            "Loading TF weight encoder/block_002/layer_000/SelfAttention/o_slot_vc with shape [512]\n",
            "Loading TF weight encoder/block_002/layer_000/SelfAttention/o_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_002/layer_000/SelfAttention/q with shape [512, 512]\n",
            "Loading TF weight encoder/block_002/layer_000/SelfAttention/q_slot_vc with shape [512]\n",
            "Loading TF weight encoder/block_002/layer_000/SelfAttention/q_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_002/layer_000/SelfAttention/v with shape [512, 512]\n",
            "Loading TF weight encoder/block_002/layer_000/SelfAttention/v_slot_vc with shape [512]\n",
            "Loading TF weight encoder/block_002/layer_000/SelfAttention/v_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_002/layer_000/layer_norm/scale with shape [512]\n",
            "Loading TF weight encoder/block_002/layer_000/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight encoder/block_002/layer_001/DenseReluDense/wi/kernel with shape [512, 2048]\n",
            "Loading TF weight encoder/block_002/layer_001/DenseReluDense/wi/kernel_slot_vc with shape [2048]\n",
            "Loading TF weight encoder/block_002/layer_001/DenseReluDense/wi/kernel_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_002/layer_001/DenseReluDense/wo/kernel with shape [2048, 512]\n",
            "Loading TF weight encoder/block_002/layer_001/DenseReluDense/wo/kernel_slot_vc with shape [2048]\n",
            "Loading TF weight encoder/block_002/layer_001/DenseReluDense/wo/kernel_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_002/layer_001/layer_norm/scale with shape [512]\n",
            "Loading TF weight encoder/block_002/layer_001/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight encoder/block_003/layer_000/SelfAttention/k with shape [512, 512]\n",
            "Loading TF weight encoder/block_003/layer_000/SelfAttention/k_slot_vc with shape [512]\n",
            "Loading TF weight encoder/block_003/layer_000/SelfAttention/k_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_003/layer_000/SelfAttention/o with shape [512, 512]\n",
            "Loading TF weight encoder/block_003/layer_000/SelfAttention/o_slot_vc with shape [512]\n",
            "Loading TF weight encoder/block_003/layer_000/SelfAttention/o_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_003/layer_000/SelfAttention/q with shape [512, 512]\n",
            "Loading TF weight encoder/block_003/layer_000/SelfAttention/q_slot_vc with shape [512]\n",
            "Loading TF weight encoder/block_003/layer_000/SelfAttention/q_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_003/layer_000/SelfAttention/v with shape [512, 512]\n",
            "Loading TF weight encoder/block_003/layer_000/SelfAttention/v_slot_vc with shape [512]\n",
            "Loading TF weight encoder/block_003/layer_000/SelfAttention/v_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_003/layer_000/layer_norm/scale with shape [512]\n",
            "Loading TF weight encoder/block_003/layer_000/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight encoder/block_003/layer_001/DenseReluDense/wi/kernel with shape [512, 2048]\n",
            "Loading TF weight encoder/block_003/layer_001/DenseReluDense/wi/kernel_slot_vc with shape [2048]\n",
            "Loading TF weight encoder/block_003/layer_001/DenseReluDense/wi/kernel_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_003/layer_001/DenseReluDense/wo/kernel with shape [2048, 512]\n",
            "Loading TF weight encoder/block_003/layer_001/DenseReluDense/wo/kernel_slot_vc with shape [2048]\n",
            "Loading TF weight encoder/block_003/layer_001/DenseReluDense/wo/kernel_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_003/layer_001/layer_norm/scale with shape [512]\n",
            "Loading TF weight encoder/block_003/layer_001/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight encoder/block_004/layer_000/SelfAttention/k with shape [512, 512]\n",
            "Loading TF weight encoder/block_004/layer_000/SelfAttention/k_slot_vc with shape [512]\n",
            "Loading TF weight encoder/block_004/layer_000/SelfAttention/k_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_004/layer_000/SelfAttention/o with shape [512, 512]\n",
            "Loading TF weight encoder/block_004/layer_000/SelfAttention/o_slot_vc with shape [512]\n",
            "Loading TF weight encoder/block_004/layer_000/SelfAttention/o_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_004/layer_000/SelfAttention/q with shape [512, 512]\n",
            "Loading TF weight encoder/block_004/layer_000/SelfAttention/q_slot_vc with shape [512]\n",
            "Loading TF weight encoder/block_004/layer_000/SelfAttention/q_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_004/layer_000/SelfAttention/v with shape [512, 512]\n",
            "Loading TF weight encoder/block_004/layer_000/SelfAttention/v_slot_vc with shape [512]\n",
            "Loading TF weight encoder/block_004/layer_000/SelfAttention/v_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_004/layer_000/layer_norm/scale with shape [512]\n",
            "Loading TF weight encoder/block_004/layer_000/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight encoder/block_004/layer_001/DenseReluDense/wi/kernel with shape [512, 2048]\n",
            "Loading TF weight encoder/block_004/layer_001/DenseReluDense/wi/kernel_slot_vc with shape [2048]\n",
            "Loading TF weight encoder/block_004/layer_001/DenseReluDense/wi/kernel_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_004/layer_001/DenseReluDense/wo/kernel with shape [2048, 512]\n",
            "Loading TF weight encoder/block_004/layer_001/DenseReluDense/wo/kernel_slot_vc with shape [2048]\n",
            "Loading TF weight encoder/block_004/layer_001/DenseReluDense/wo/kernel_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_004/layer_001/layer_norm/scale with shape [512]\n",
            "Loading TF weight encoder/block_004/layer_001/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight encoder/block_005/layer_000/SelfAttention/k with shape [512, 512]\n",
            "Loading TF weight encoder/block_005/layer_000/SelfAttention/k_slot_vc with shape [512]\n",
            "Loading TF weight encoder/block_005/layer_000/SelfAttention/k_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_005/layer_000/SelfAttention/o with shape [512, 512]\n",
            "Loading TF weight encoder/block_005/layer_000/SelfAttention/o_slot_vc with shape [512]\n",
            "Loading TF weight encoder/block_005/layer_000/SelfAttention/o_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_005/layer_000/SelfAttention/q with shape [512, 512]\n",
            "Loading TF weight encoder/block_005/layer_000/SelfAttention/q_slot_vc with shape [512]\n",
            "Loading TF weight encoder/block_005/layer_000/SelfAttention/q_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_005/layer_000/SelfAttention/v with shape [512, 512]\n",
            "Loading TF weight encoder/block_005/layer_000/SelfAttention/v_slot_vc with shape [512]\n",
            "Loading TF weight encoder/block_005/layer_000/SelfAttention/v_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_005/layer_000/layer_norm/scale with shape [512]\n",
            "Loading TF weight encoder/block_005/layer_000/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight encoder/block_005/layer_001/DenseReluDense/wi/kernel with shape [512, 2048]\n",
            "Loading TF weight encoder/block_005/layer_001/DenseReluDense/wi/kernel_slot_vc with shape [2048]\n",
            "Loading TF weight encoder/block_005/layer_001/DenseReluDense/wi/kernel_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_005/layer_001/DenseReluDense/wo/kernel with shape [2048, 512]\n",
            "Loading TF weight encoder/block_005/layer_001/DenseReluDense/wo/kernel_slot_vc with shape [2048]\n",
            "Loading TF weight encoder/block_005/layer_001/DenseReluDense/wo/kernel_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_005/layer_001/layer_norm/scale with shape [512]\n",
            "Loading TF weight encoder/block_005/layer_001/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight encoder/final_layer_norm/scale with shape [512]\n",
            "Loading TF weight encoder/final_layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight global_step with shape []\n",
            "Loading TF weight shared/embedding with shape [32128, 512]\n",
            "Loading TF weight shared/embedding_slot_vc with shape [32128]\n",
            "Loading TF weight shared/embedding_slot_vr with shape [512]\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_000', 'layer_000', 'SelfAttention', 'k']\n",
            "Initialize PyTorch weight ['decoder', 'block_000', 'layer_000', 'SelfAttention', 'k']\n",
            "Skipping decoder/block_000/layer_000/SelfAttention/k_slot_vc\n",
            "Skipping decoder/block_000/layer_000/SelfAttention/k_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_000', 'layer_000', 'SelfAttention', 'o']\n",
            "Initialize PyTorch weight ['decoder', 'block_000', 'layer_000', 'SelfAttention', 'o']\n",
            "Skipping decoder/block_000/layer_000/SelfAttention/o_slot_vc\n",
            "Skipping decoder/block_000/layer_000/SelfAttention/o_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_000', 'layer_000', 'SelfAttention', 'q']\n",
            "Initialize PyTorch weight ['decoder', 'block_000', 'layer_000', 'SelfAttention', 'q']\n",
            "Skipping decoder/block_000/layer_000/SelfAttention/q_slot_vc\n",
            "Skipping decoder/block_000/layer_000/SelfAttention/q_slot_vr\n",
            "Transposing numpy weight of shape (8, 32) for ['decoder', 'block_000', 'layer_000', 'SelfAttention', 'relative_attention_bias']\n",
            "Initialize PyTorch weight ['decoder', 'block_000', 'layer_000', 'SelfAttention', 'relative_attention_bias']\n",
            "Skipping decoder/block_000/layer_000/SelfAttention/relative_attention_bias_slot_v\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_000', 'layer_000', 'SelfAttention', 'v']\n",
            "Initialize PyTorch weight ['decoder', 'block_000', 'layer_000', 'SelfAttention', 'v']\n",
            "Skipping decoder/block_000/layer_000/SelfAttention/v_slot_vc\n",
            "Skipping decoder/block_000/layer_000/SelfAttention/v_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['decoder', 'block_000', 'layer_000', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['decoder', 'block_000', 'layer_000', 'layer_norm', 'scale']\n",
            "Skipping decoder/block_000/layer_000/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_000', 'layer_001', 'EncDecAttention', 'k']\n",
            "Initialize PyTorch weight ['decoder', 'block_000', 'layer_001', 'EncDecAttention', 'k']\n",
            "Skipping decoder/block_000/layer_001/EncDecAttention/k_slot_vc\n",
            "Skipping decoder/block_000/layer_001/EncDecAttention/k_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_000', 'layer_001', 'EncDecAttention', 'o']\n",
            "Initialize PyTorch weight ['decoder', 'block_000', 'layer_001', 'EncDecAttention', 'o']\n",
            "Skipping decoder/block_000/layer_001/EncDecAttention/o_slot_vc\n",
            "Skipping decoder/block_000/layer_001/EncDecAttention/o_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_000', 'layer_001', 'EncDecAttention', 'q']\n",
            "Initialize PyTorch weight ['decoder', 'block_000', 'layer_001', 'EncDecAttention', 'q']\n",
            "Skipping decoder/block_000/layer_001/EncDecAttention/q_slot_vc\n",
            "Skipping decoder/block_000/layer_001/EncDecAttention/q_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_000', 'layer_001', 'EncDecAttention', 'v']\n",
            "Initialize PyTorch weight ['decoder', 'block_000', 'layer_001', 'EncDecAttention', 'v']\n",
            "Skipping decoder/block_000/layer_001/EncDecAttention/v_slot_vc\n",
            "Skipping decoder/block_000/layer_001/EncDecAttention/v_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['decoder', 'block_000', 'layer_001', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['decoder', 'block_000', 'layer_001', 'layer_norm', 'scale']\n",
            "Skipping decoder/block_000/layer_001/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 2048) for ['decoder', 'block_000', 'layer_002', 'DenseReluDense', 'wi', 'kernel']\n",
            "Initialize PyTorch weight ['decoder', 'block_000', 'layer_002', 'DenseReluDense', 'wi', 'kernel']\n",
            "Skipping decoder/block_000/layer_002/DenseReluDense/wi/kernel_slot_vc\n",
            "Skipping decoder/block_000/layer_002/DenseReluDense/wi/kernel_slot_vr\n",
            "Transposing numpy weight of shape (2048, 512) for ['decoder', 'block_000', 'layer_002', 'DenseReluDense', 'wo', 'kernel']\n",
            "Initialize PyTorch weight ['decoder', 'block_000', 'layer_002', 'DenseReluDense', 'wo', 'kernel']\n",
            "Skipping decoder/block_000/layer_002/DenseReluDense/wo/kernel_slot_vc\n",
            "Skipping decoder/block_000/layer_002/DenseReluDense/wo/kernel_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['decoder', 'block_000', 'layer_002', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['decoder', 'block_000', 'layer_002', 'layer_norm', 'scale']\n",
            "Skipping decoder/block_000/layer_002/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_001', 'layer_000', 'SelfAttention', 'k']\n",
            "Initialize PyTorch weight ['decoder', 'block_001', 'layer_000', 'SelfAttention', 'k']\n",
            "Skipping decoder/block_001/layer_000/SelfAttention/k_slot_vc\n",
            "Skipping decoder/block_001/layer_000/SelfAttention/k_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_001', 'layer_000', 'SelfAttention', 'o']\n",
            "Initialize PyTorch weight ['decoder', 'block_001', 'layer_000', 'SelfAttention', 'o']\n",
            "Skipping decoder/block_001/layer_000/SelfAttention/o_slot_vc\n",
            "Skipping decoder/block_001/layer_000/SelfAttention/o_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_001', 'layer_000', 'SelfAttention', 'q']\n",
            "Initialize PyTorch weight ['decoder', 'block_001', 'layer_000', 'SelfAttention', 'q']\n",
            "Skipping decoder/block_001/layer_000/SelfAttention/q_slot_vc\n",
            "Skipping decoder/block_001/layer_000/SelfAttention/q_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_001', 'layer_000', 'SelfAttention', 'v']\n",
            "Initialize PyTorch weight ['decoder', 'block_001', 'layer_000', 'SelfAttention', 'v']\n",
            "Skipping decoder/block_001/layer_000/SelfAttention/v_slot_vc\n",
            "Skipping decoder/block_001/layer_000/SelfAttention/v_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['decoder', 'block_001', 'layer_000', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['decoder', 'block_001', 'layer_000', 'layer_norm', 'scale']\n",
            "Skipping decoder/block_001/layer_000/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_001', 'layer_001', 'EncDecAttention', 'k']\n",
            "Initialize PyTorch weight ['decoder', 'block_001', 'layer_001', 'EncDecAttention', 'k']\n",
            "Skipping decoder/block_001/layer_001/EncDecAttention/k_slot_vc\n",
            "Skipping decoder/block_001/layer_001/EncDecAttention/k_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_001', 'layer_001', 'EncDecAttention', 'o']\n",
            "Initialize PyTorch weight ['decoder', 'block_001', 'layer_001', 'EncDecAttention', 'o']\n",
            "Skipping decoder/block_001/layer_001/EncDecAttention/o_slot_vc\n",
            "Skipping decoder/block_001/layer_001/EncDecAttention/o_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_001', 'layer_001', 'EncDecAttention', 'q']\n",
            "Initialize PyTorch weight ['decoder', 'block_001', 'layer_001', 'EncDecAttention', 'q']\n",
            "Skipping decoder/block_001/layer_001/EncDecAttention/q_slot_vc\n",
            "Skipping decoder/block_001/layer_001/EncDecAttention/q_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_001', 'layer_001', 'EncDecAttention', 'v']\n",
            "Initialize PyTorch weight ['decoder', 'block_001', 'layer_001', 'EncDecAttention', 'v']\n",
            "Skipping decoder/block_001/layer_001/EncDecAttention/v_slot_vc\n",
            "Skipping decoder/block_001/layer_001/EncDecAttention/v_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['decoder', 'block_001', 'layer_001', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['decoder', 'block_001', 'layer_001', 'layer_norm', 'scale']\n",
            "Skipping decoder/block_001/layer_001/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 2048) for ['decoder', 'block_001', 'layer_002', 'DenseReluDense', 'wi', 'kernel']\n",
            "Initialize PyTorch weight ['decoder', 'block_001', 'layer_002', 'DenseReluDense', 'wi', 'kernel']\n",
            "Skipping decoder/block_001/layer_002/DenseReluDense/wi/kernel_slot_vc\n",
            "Skipping decoder/block_001/layer_002/DenseReluDense/wi/kernel_slot_vr\n",
            "Transposing numpy weight of shape (2048, 512) for ['decoder', 'block_001', 'layer_002', 'DenseReluDense', 'wo', 'kernel']\n",
            "Initialize PyTorch weight ['decoder', 'block_001', 'layer_002', 'DenseReluDense', 'wo', 'kernel']\n",
            "Skipping decoder/block_001/layer_002/DenseReluDense/wo/kernel_slot_vc\n",
            "Skipping decoder/block_001/layer_002/DenseReluDense/wo/kernel_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['decoder', 'block_001', 'layer_002', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['decoder', 'block_001', 'layer_002', 'layer_norm', 'scale']\n",
            "Skipping decoder/block_001/layer_002/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_002', 'layer_000', 'SelfAttention', 'k']\n",
            "Initialize PyTorch weight ['decoder', 'block_002', 'layer_000', 'SelfAttention', 'k']\n",
            "Skipping decoder/block_002/layer_000/SelfAttention/k_slot_vc\n",
            "Skipping decoder/block_002/layer_000/SelfAttention/k_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_002', 'layer_000', 'SelfAttention', 'o']\n",
            "Initialize PyTorch weight ['decoder', 'block_002', 'layer_000', 'SelfAttention', 'o']\n",
            "Skipping decoder/block_002/layer_000/SelfAttention/o_slot_vc\n",
            "Skipping decoder/block_002/layer_000/SelfAttention/o_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_002', 'layer_000', 'SelfAttention', 'q']\n",
            "Initialize PyTorch weight ['decoder', 'block_002', 'layer_000', 'SelfAttention', 'q']\n",
            "Skipping decoder/block_002/layer_000/SelfAttention/q_slot_vc\n",
            "Skipping decoder/block_002/layer_000/SelfAttention/q_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_002', 'layer_000', 'SelfAttention', 'v']\n",
            "Initialize PyTorch weight ['decoder', 'block_002', 'layer_000', 'SelfAttention', 'v']\n",
            "Skipping decoder/block_002/layer_000/SelfAttention/v_slot_vc\n",
            "Skipping decoder/block_002/layer_000/SelfAttention/v_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['decoder', 'block_002', 'layer_000', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['decoder', 'block_002', 'layer_000', 'layer_norm', 'scale']\n",
            "Skipping decoder/block_002/layer_000/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_002', 'layer_001', 'EncDecAttention', 'k']\n",
            "Initialize PyTorch weight ['decoder', 'block_002', 'layer_001', 'EncDecAttention', 'k']\n",
            "Skipping decoder/block_002/layer_001/EncDecAttention/k_slot_vc\n",
            "Skipping decoder/block_002/layer_001/EncDecAttention/k_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_002', 'layer_001', 'EncDecAttention', 'o']\n",
            "Initialize PyTorch weight ['decoder', 'block_002', 'layer_001', 'EncDecAttention', 'o']\n",
            "Skipping decoder/block_002/layer_001/EncDecAttention/o_slot_vc\n",
            "Skipping decoder/block_002/layer_001/EncDecAttention/o_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_002', 'layer_001', 'EncDecAttention', 'q']\n",
            "Initialize PyTorch weight ['decoder', 'block_002', 'layer_001', 'EncDecAttention', 'q']\n",
            "Skipping decoder/block_002/layer_001/EncDecAttention/q_slot_vc\n",
            "Skipping decoder/block_002/layer_001/EncDecAttention/q_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_002', 'layer_001', 'EncDecAttention', 'v']\n",
            "Initialize PyTorch weight ['decoder', 'block_002', 'layer_001', 'EncDecAttention', 'v']\n",
            "Skipping decoder/block_002/layer_001/EncDecAttention/v_slot_vc\n",
            "Skipping decoder/block_002/layer_001/EncDecAttention/v_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['decoder', 'block_002', 'layer_001', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['decoder', 'block_002', 'layer_001', 'layer_norm', 'scale']\n",
            "Skipping decoder/block_002/layer_001/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 2048) for ['decoder', 'block_002', 'layer_002', 'DenseReluDense', 'wi', 'kernel']\n",
            "Initialize PyTorch weight ['decoder', 'block_002', 'layer_002', 'DenseReluDense', 'wi', 'kernel']\n",
            "Skipping decoder/block_002/layer_002/DenseReluDense/wi/kernel_slot_vc\n",
            "Skipping decoder/block_002/layer_002/DenseReluDense/wi/kernel_slot_vr\n",
            "Transposing numpy weight of shape (2048, 512) for ['decoder', 'block_002', 'layer_002', 'DenseReluDense', 'wo', 'kernel']\n",
            "Initialize PyTorch weight ['decoder', 'block_002', 'layer_002', 'DenseReluDense', 'wo', 'kernel']\n",
            "Skipping decoder/block_002/layer_002/DenseReluDense/wo/kernel_slot_vc\n",
            "Skipping decoder/block_002/layer_002/DenseReluDense/wo/kernel_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['decoder', 'block_002', 'layer_002', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['decoder', 'block_002', 'layer_002', 'layer_norm', 'scale']\n",
            "Skipping decoder/block_002/layer_002/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_003', 'layer_000', 'SelfAttention', 'k']\n",
            "Initialize PyTorch weight ['decoder', 'block_003', 'layer_000', 'SelfAttention', 'k']\n",
            "Skipping decoder/block_003/layer_000/SelfAttention/k_slot_vc\n",
            "Skipping decoder/block_003/layer_000/SelfAttention/k_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_003', 'layer_000', 'SelfAttention', 'o']\n",
            "Initialize PyTorch weight ['decoder', 'block_003', 'layer_000', 'SelfAttention', 'o']\n",
            "Skipping decoder/block_003/layer_000/SelfAttention/o_slot_vc\n",
            "Skipping decoder/block_003/layer_000/SelfAttention/o_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_003', 'layer_000', 'SelfAttention', 'q']\n",
            "Initialize PyTorch weight ['decoder', 'block_003', 'layer_000', 'SelfAttention', 'q']\n",
            "Skipping decoder/block_003/layer_000/SelfAttention/q_slot_vc\n",
            "Skipping decoder/block_003/layer_000/SelfAttention/q_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_003', 'layer_000', 'SelfAttention', 'v']\n",
            "Initialize PyTorch weight ['decoder', 'block_003', 'layer_000', 'SelfAttention', 'v']\n",
            "Skipping decoder/block_003/layer_000/SelfAttention/v_slot_vc\n",
            "Skipping decoder/block_003/layer_000/SelfAttention/v_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['decoder', 'block_003', 'layer_000', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['decoder', 'block_003', 'layer_000', 'layer_norm', 'scale']\n",
            "Skipping decoder/block_003/layer_000/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_003', 'layer_001', 'EncDecAttention', 'k']\n",
            "Initialize PyTorch weight ['decoder', 'block_003', 'layer_001', 'EncDecAttention', 'k']\n",
            "Skipping decoder/block_003/layer_001/EncDecAttention/k_slot_vc\n",
            "Skipping decoder/block_003/layer_001/EncDecAttention/k_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_003', 'layer_001', 'EncDecAttention', 'o']\n",
            "Initialize PyTorch weight ['decoder', 'block_003', 'layer_001', 'EncDecAttention', 'o']\n",
            "Skipping decoder/block_003/layer_001/EncDecAttention/o_slot_vc\n",
            "Skipping decoder/block_003/layer_001/EncDecAttention/o_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_003', 'layer_001', 'EncDecAttention', 'q']\n",
            "Initialize PyTorch weight ['decoder', 'block_003', 'layer_001', 'EncDecAttention', 'q']\n",
            "Skipping decoder/block_003/layer_001/EncDecAttention/q_slot_vc\n",
            "Skipping decoder/block_003/layer_001/EncDecAttention/q_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_003', 'layer_001', 'EncDecAttention', 'v']\n",
            "Initialize PyTorch weight ['decoder', 'block_003', 'layer_001', 'EncDecAttention', 'v']\n",
            "Skipping decoder/block_003/layer_001/EncDecAttention/v_slot_vc\n",
            "Skipping decoder/block_003/layer_001/EncDecAttention/v_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['decoder', 'block_003', 'layer_001', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['decoder', 'block_003', 'layer_001', 'layer_norm', 'scale']\n",
            "Skipping decoder/block_003/layer_001/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 2048) for ['decoder', 'block_003', 'layer_002', 'DenseReluDense', 'wi', 'kernel']\n",
            "Initialize PyTorch weight ['decoder', 'block_003', 'layer_002', 'DenseReluDense', 'wi', 'kernel']\n",
            "Skipping decoder/block_003/layer_002/DenseReluDense/wi/kernel_slot_vc\n",
            "Skipping decoder/block_003/layer_002/DenseReluDense/wi/kernel_slot_vr\n",
            "Transposing numpy weight of shape (2048, 512) for ['decoder', 'block_003', 'layer_002', 'DenseReluDense', 'wo', 'kernel']\n",
            "Initialize PyTorch weight ['decoder', 'block_003', 'layer_002', 'DenseReluDense', 'wo', 'kernel']\n",
            "Skipping decoder/block_003/layer_002/DenseReluDense/wo/kernel_slot_vc\n",
            "Skipping decoder/block_003/layer_002/DenseReluDense/wo/kernel_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['decoder', 'block_003', 'layer_002', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['decoder', 'block_003', 'layer_002', 'layer_norm', 'scale']\n",
            "Skipping decoder/block_003/layer_002/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_004', 'layer_000', 'SelfAttention', 'k']\n",
            "Initialize PyTorch weight ['decoder', 'block_004', 'layer_000', 'SelfAttention', 'k']\n",
            "Skipping decoder/block_004/layer_000/SelfAttention/k_slot_vc\n",
            "Skipping decoder/block_004/layer_000/SelfAttention/k_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_004', 'layer_000', 'SelfAttention', 'o']\n",
            "Initialize PyTorch weight ['decoder', 'block_004', 'layer_000', 'SelfAttention', 'o']\n",
            "Skipping decoder/block_004/layer_000/SelfAttention/o_slot_vc\n",
            "Skipping decoder/block_004/layer_000/SelfAttention/o_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_004', 'layer_000', 'SelfAttention', 'q']\n",
            "Initialize PyTorch weight ['decoder', 'block_004', 'layer_000', 'SelfAttention', 'q']\n",
            "Skipping decoder/block_004/layer_000/SelfAttention/q_slot_vc\n",
            "Skipping decoder/block_004/layer_000/SelfAttention/q_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_004', 'layer_000', 'SelfAttention', 'v']\n",
            "Initialize PyTorch weight ['decoder', 'block_004', 'layer_000', 'SelfAttention', 'v']\n",
            "Skipping decoder/block_004/layer_000/SelfAttention/v_slot_vc\n",
            "Skipping decoder/block_004/layer_000/SelfAttention/v_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['decoder', 'block_004', 'layer_000', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['decoder', 'block_004', 'layer_000', 'layer_norm', 'scale']\n",
            "Skipping decoder/block_004/layer_000/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_004', 'layer_001', 'EncDecAttention', 'k']\n",
            "Initialize PyTorch weight ['decoder', 'block_004', 'layer_001', 'EncDecAttention', 'k']\n",
            "Skipping decoder/block_004/layer_001/EncDecAttention/k_slot_vc\n",
            "Skipping decoder/block_004/layer_001/EncDecAttention/k_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_004', 'layer_001', 'EncDecAttention', 'o']\n",
            "Initialize PyTorch weight ['decoder', 'block_004', 'layer_001', 'EncDecAttention', 'o']\n",
            "Skipping decoder/block_004/layer_001/EncDecAttention/o_slot_vc\n",
            "Skipping decoder/block_004/layer_001/EncDecAttention/o_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_004', 'layer_001', 'EncDecAttention', 'q']\n",
            "Initialize PyTorch weight ['decoder', 'block_004', 'layer_001', 'EncDecAttention', 'q']\n",
            "Skipping decoder/block_004/layer_001/EncDecAttention/q_slot_vc\n",
            "Skipping decoder/block_004/layer_001/EncDecAttention/q_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_004', 'layer_001', 'EncDecAttention', 'v']\n",
            "Initialize PyTorch weight ['decoder', 'block_004', 'layer_001', 'EncDecAttention', 'v']\n",
            "Skipping decoder/block_004/layer_001/EncDecAttention/v_slot_vc\n",
            "Skipping decoder/block_004/layer_001/EncDecAttention/v_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['decoder', 'block_004', 'layer_001', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['decoder', 'block_004', 'layer_001', 'layer_norm', 'scale']\n",
            "Skipping decoder/block_004/layer_001/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 2048) for ['decoder', 'block_004', 'layer_002', 'DenseReluDense', 'wi', 'kernel']\n",
            "Initialize PyTorch weight ['decoder', 'block_004', 'layer_002', 'DenseReluDense', 'wi', 'kernel']\n",
            "Skipping decoder/block_004/layer_002/DenseReluDense/wi/kernel_slot_vc\n",
            "Skipping decoder/block_004/layer_002/DenseReluDense/wi/kernel_slot_vr\n",
            "Transposing numpy weight of shape (2048, 512) for ['decoder', 'block_004', 'layer_002', 'DenseReluDense', 'wo', 'kernel']\n",
            "Initialize PyTorch weight ['decoder', 'block_004', 'layer_002', 'DenseReluDense', 'wo', 'kernel']\n",
            "Skipping decoder/block_004/layer_002/DenseReluDense/wo/kernel_slot_vc\n",
            "Skipping decoder/block_004/layer_002/DenseReluDense/wo/kernel_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['decoder', 'block_004', 'layer_002', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['decoder', 'block_004', 'layer_002', 'layer_norm', 'scale']\n",
            "Skipping decoder/block_004/layer_002/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_005', 'layer_000', 'SelfAttention', 'k']\n",
            "Initialize PyTorch weight ['decoder', 'block_005', 'layer_000', 'SelfAttention', 'k']\n",
            "Skipping decoder/block_005/layer_000/SelfAttention/k_slot_vc\n",
            "Skipping decoder/block_005/layer_000/SelfAttention/k_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_005', 'layer_000', 'SelfAttention', 'o']\n",
            "Initialize PyTorch weight ['decoder', 'block_005', 'layer_000', 'SelfAttention', 'o']\n",
            "Skipping decoder/block_005/layer_000/SelfAttention/o_slot_vc\n",
            "Skipping decoder/block_005/layer_000/SelfAttention/o_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_005', 'layer_000', 'SelfAttention', 'q']\n",
            "Initialize PyTorch weight ['decoder', 'block_005', 'layer_000', 'SelfAttention', 'q']\n",
            "Skipping decoder/block_005/layer_000/SelfAttention/q_slot_vc\n",
            "Skipping decoder/block_005/layer_000/SelfAttention/q_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_005', 'layer_000', 'SelfAttention', 'v']\n",
            "Initialize PyTorch weight ['decoder', 'block_005', 'layer_000', 'SelfAttention', 'v']\n",
            "Skipping decoder/block_005/layer_000/SelfAttention/v_slot_vc\n",
            "Skipping decoder/block_005/layer_000/SelfAttention/v_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['decoder', 'block_005', 'layer_000', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['decoder', 'block_005', 'layer_000', 'layer_norm', 'scale']\n",
            "Skipping decoder/block_005/layer_000/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_005', 'layer_001', 'EncDecAttention', 'k']\n",
            "Initialize PyTorch weight ['decoder', 'block_005', 'layer_001', 'EncDecAttention', 'k']\n",
            "Skipping decoder/block_005/layer_001/EncDecAttention/k_slot_vc\n",
            "Skipping decoder/block_005/layer_001/EncDecAttention/k_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_005', 'layer_001', 'EncDecAttention', 'o']\n",
            "Initialize PyTorch weight ['decoder', 'block_005', 'layer_001', 'EncDecAttention', 'o']\n",
            "Skipping decoder/block_005/layer_001/EncDecAttention/o_slot_vc\n",
            "Skipping decoder/block_005/layer_001/EncDecAttention/o_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_005', 'layer_001', 'EncDecAttention', 'q']\n",
            "Initialize PyTorch weight ['decoder', 'block_005', 'layer_001', 'EncDecAttention', 'q']\n",
            "Skipping decoder/block_005/layer_001/EncDecAttention/q_slot_vc\n",
            "Skipping decoder/block_005/layer_001/EncDecAttention/q_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_005', 'layer_001', 'EncDecAttention', 'v']\n",
            "Initialize PyTorch weight ['decoder', 'block_005', 'layer_001', 'EncDecAttention', 'v']\n",
            "Skipping decoder/block_005/layer_001/EncDecAttention/v_slot_vc\n",
            "Skipping decoder/block_005/layer_001/EncDecAttention/v_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['decoder', 'block_005', 'layer_001', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['decoder', 'block_005', 'layer_001', 'layer_norm', 'scale']\n",
            "Skipping decoder/block_005/layer_001/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 2048) for ['decoder', 'block_005', 'layer_002', 'DenseReluDense', 'wi', 'kernel']\n",
            "Initialize PyTorch weight ['decoder', 'block_005', 'layer_002', 'DenseReluDense', 'wi', 'kernel']\n",
            "Skipping decoder/block_005/layer_002/DenseReluDense/wi/kernel_slot_vc\n",
            "Skipping decoder/block_005/layer_002/DenseReluDense/wi/kernel_slot_vr\n",
            "Transposing numpy weight of shape (2048, 512) for ['decoder', 'block_005', 'layer_002', 'DenseReluDense', 'wo', 'kernel']\n",
            "Initialize PyTorch weight ['decoder', 'block_005', 'layer_002', 'DenseReluDense', 'wo', 'kernel']\n",
            "Skipping decoder/block_005/layer_002/DenseReluDense/wo/kernel_slot_vc\n",
            "Skipping decoder/block_005/layer_002/DenseReluDense/wo/kernel_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['decoder', 'block_005', 'layer_002', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['decoder', 'block_005', 'layer_002', 'layer_norm', 'scale']\n",
            "Skipping decoder/block_005/layer_002/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512,) for ['decoder', 'final_layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['decoder', 'final_layer_norm', 'scale']\n",
            "Skipping decoder/final_layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 512) for ['encoder', 'block_000', 'layer_000', 'SelfAttention', 'k']\n",
            "Initialize PyTorch weight ['encoder', 'block_000', 'layer_000', 'SelfAttention', 'k']\n",
            "Skipping encoder/block_000/layer_000/SelfAttention/k_slot_vc\n",
            "Skipping encoder/block_000/layer_000/SelfAttention/k_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['encoder', 'block_000', 'layer_000', 'SelfAttention', 'o']\n",
            "Initialize PyTorch weight ['encoder', 'block_000', 'layer_000', 'SelfAttention', 'o']\n",
            "Skipping encoder/block_000/layer_000/SelfAttention/o_slot_vc\n",
            "Skipping encoder/block_000/layer_000/SelfAttention/o_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['encoder', 'block_000', 'layer_000', 'SelfAttention', 'q']\n",
            "Initialize PyTorch weight ['encoder', 'block_000', 'layer_000', 'SelfAttention', 'q']\n",
            "Skipping encoder/block_000/layer_000/SelfAttention/q_slot_vc\n",
            "Skipping encoder/block_000/layer_000/SelfAttention/q_slot_vr\n",
            "Transposing numpy weight of shape (8, 32) for ['encoder', 'block_000', 'layer_000', 'SelfAttention', 'relative_attention_bias']\n",
            "Initialize PyTorch weight ['encoder', 'block_000', 'layer_000', 'SelfAttention', 'relative_attention_bias']\n",
            "Skipping encoder/block_000/layer_000/SelfAttention/relative_attention_bias_slot_v\n",
            "Transposing numpy weight of shape (512, 512) for ['encoder', 'block_000', 'layer_000', 'SelfAttention', 'v']\n",
            "Initialize PyTorch weight ['encoder', 'block_000', 'layer_000', 'SelfAttention', 'v']\n",
            "Skipping encoder/block_000/layer_000/SelfAttention/v_slot_vc\n",
            "Skipping encoder/block_000/layer_000/SelfAttention/v_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['encoder', 'block_000', 'layer_000', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['encoder', 'block_000', 'layer_000', 'layer_norm', 'scale']\n",
            "Skipping encoder/block_000/layer_000/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 2048) for ['encoder', 'block_000', 'layer_001', 'DenseReluDense', 'wi', 'kernel']\n",
            "Initialize PyTorch weight ['encoder', 'block_000', 'layer_001', 'DenseReluDense', 'wi', 'kernel']\n",
            "Skipping encoder/block_000/layer_001/DenseReluDense/wi/kernel_slot_vc\n",
            "Skipping encoder/block_000/layer_001/DenseReluDense/wi/kernel_slot_vr\n",
            "Transposing numpy weight of shape (2048, 512) for ['encoder', 'block_000', 'layer_001', 'DenseReluDense', 'wo', 'kernel']\n",
            "Initialize PyTorch weight ['encoder', 'block_000', 'layer_001', 'DenseReluDense', 'wo', 'kernel']\n",
            "Skipping encoder/block_000/layer_001/DenseReluDense/wo/kernel_slot_vc\n",
            "Skipping encoder/block_000/layer_001/DenseReluDense/wo/kernel_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['encoder', 'block_000', 'layer_001', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['encoder', 'block_000', 'layer_001', 'layer_norm', 'scale']\n",
            "Skipping encoder/block_000/layer_001/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 512) for ['encoder', 'block_001', 'layer_000', 'SelfAttention', 'k']\n",
            "Initialize PyTorch weight ['encoder', 'block_001', 'layer_000', 'SelfAttention', 'k']\n",
            "Skipping encoder/block_001/layer_000/SelfAttention/k_slot_vc\n",
            "Skipping encoder/block_001/layer_000/SelfAttention/k_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['encoder', 'block_001', 'layer_000', 'SelfAttention', 'o']\n",
            "Initialize PyTorch weight ['encoder', 'block_001', 'layer_000', 'SelfAttention', 'o']\n",
            "Skipping encoder/block_001/layer_000/SelfAttention/o_slot_vc\n",
            "Skipping encoder/block_001/layer_000/SelfAttention/o_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['encoder', 'block_001', 'layer_000', 'SelfAttention', 'q']\n",
            "Initialize PyTorch weight ['encoder', 'block_001', 'layer_000', 'SelfAttention', 'q']\n",
            "Skipping encoder/block_001/layer_000/SelfAttention/q_slot_vc\n",
            "Skipping encoder/block_001/layer_000/SelfAttention/q_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['encoder', 'block_001', 'layer_000', 'SelfAttention', 'v']\n",
            "Initialize PyTorch weight ['encoder', 'block_001', 'layer_000', 'SelfAttention', 'v']\n",
            "Skipping encoder/block_001/layer_000/SelfAttention/v_slot_vc\n",
            "Skipping encoder/block_001/layer_000/SelfAttention/v_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['encoder', 'block_001', 'layer_000', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['encoder', 'block_001', 'layer_000', 'layer_norm', 'scale']\n",
            "Skipping encoder/block_001/layer_000/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 2048) for ['encoder', 'block_001', 'layer_001', 'DenseReluDense', 'wi', 'kernel']\n",
            "Initialize PyTorch weight ['encoder', 'block_001', 'layer_001', 'DenseReluDense', 'wi', 'kernel']\n",
            "Skipping encoder/block_001/layer_001/DenseReluDense/wi/kernel_slot_vc\n",
            "Skipping encoder/block_001/layer_001/DenseReluDense/wi/kernel_slot_vr\n",
            "Transposing numpy weight of shape (2048, 512) for ['encoder', 'block_001', 'layer_001', 'DenseReluDense', 'wo', 'kernel']\n",
            "Initialize PyTorch weight ['encoder', 'block_001', 'layer_001', 'DenseReluDense', 'wo', 'kernel']\n",
            "Skipping encoder/block_001/layer_001/DenseReluDense/wo/kernel_slot_vc\n",
            "Skipping encoder/block_001/layer_001/DenseReluDense/wo/kernel_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['encoder', 'block_001', 'layer_001', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['encoder', 'block_001', 'layer_001', 'layer_norm', 'scale']\n",
            "Skipping encoder/block_001/layer_001/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 512) for ['encoder', 'block_002', 'layer_000', 'SelfAttention', 'k']\n",
            "Initialize PyTorch weight ['encoder', 'block_002', 'layer_000', 'SelfAttention', 'k']\n",
            "Skipping encoder/block_002/layer_000/SelfAttention/k_slot_vc\n",
            "Skipping encoder/block_002/layer_000/SelfAttention/k_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['encoder', 'block_002', 'layer_000', 'SelfAttention', 'o']\n",
            "Initialize PyTorch weight ['encoder', 'block_002', 'layer_000', 'SelfAttention', 'o']\n",
            "Skipping encoder/block_002/layer_000/SelfAttention/o_slot_vc\n",
            "Skipping encoder/block_002/layer_000/SelfAttention/o_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['encoder', 'block_002', 'layer_000', 'SelfAttention', 'q']\n",
            "Initialize PyTorch weight ['encoder', 'block_002', 'layer_000', 'SelfAttention', 'q']\n",
            "Skipping encoder/block_002/layer_000/SelfAttention/q_slot_vc\n",
            "Skipping encoder/block_002/layer_000/SelfAttention/q_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['encoder', 'block_002', 'layer_000', 'SelfAttention', 'v']\n",
            "Initialize PyTorch weight ['encoder', 'block_002', 'layer_000', 'SelfAttention', 'v']\n",
            "Skipping encoder/block_002/layer_000/SelfAttention/v_slot_vc\n",
            "Skipping encoder/block_002/layer_000/SelfAttention/v_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['encoder', 'block_002', 'layer_000', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['encoder', 'block_002', 'layer_000', 'layer_norm', 'scale']\n",
            "Skipping encoder/block_002/layer_000/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 2048) for ['encoder', 'block_002', 'layer_001', 'DenseReluDense', 'wi', 'kernel']\n",
            "Initialize PyTorch weight ['encoder', 'block_002', 'layer_001', 'DenseReluDense', 'wi', 'kernel']\n",
            "Skipping encoder/block_002/layer_001/DenseReluDense/wi/kernel_slot_vc\n",
            "Skipping encoder/block_002/layer_001/DenseReluDense/wi/kernel_slot_vr\n",
            "Transposing numpy weight of shape (2048, 512) for ['encoder', 'block_002', 'layer_001', 'DenseReluDense', 'wo', 'kernel']\n",
            "Initialize PyTorch weight ['encoder', 'block_002', 'layer_001', 'DenseReluDense', 'wo', 'kernel']\n",
            "Skipping encoder/block_002/layer_001/DenseReluDense/wo/kernel_slot_vc\n",
            "Skipping encoder/block_002/layer_001/DenseReluDense/wo/kernel_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['encoder', 'block_002', 'layer_001', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['encoder', 'block_002', 'layer_001', 'layer_norm', 'scale']\n",
            "Skipping encoder/block_002/layer_001/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 512) for ['encoder', 'block_003', 'layer_000', 'SelfAttention', 'k']\n",
            "Initialize PyTorch weight ['encoder', 'block_003', 'layer_000', 'SelfAttention', 'k']\n",
            "Skipping encoder/block_003/layer_000/SelfAttention/k_slot_vc\n",
            "Skipping encoder/block_003/layer_000/SelfAttention/k_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['encoder', 'block_003', 'layer_000', 'SelfAttention', 'o']\n",
            "Initialize PyTorch weight ['encoder', 'block_003', 'layer_000', 'SelfAttention', 'o']\n",
            "Skipping encoder/block_003/layer_000/SelfAttention/o_slot_vc\n",
            "Skipping encoder/block_003/layer_000/SelfAttention/o_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['encoder', 'block_003', 'layer_000', 'SelfAttention', 'q']\n",
            "Initialize PyTorch weight ['encoder', 'block_003', 'layer_000', 'SelfAttention', 'q']\n",
            "Skipping encoder/block_003/layer_000/SelfAttention/q_slot_vc\n",
            "Skipping encoder/block_003/layer_000/SelfAttention/q_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['encoder', 'block_003', 'layer_000', 'SelfAttention', 'v']\n",
            "Initialize PyTorch weight ['encoder', 'block_003', 'layer_000', 'SelfAttention', 'v']\n",
            "Skipping encoder/block_003/layer_000/SelfAttention/v_slot_vc\n",
            "Skipping encoder/block_003/layer_000/SelfAttention/v_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['encoder', 'block_003', 'layer_000', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['encoder', 'block_003', 'layer_000', 'layer_norm', 'scale']\n",
            "Skipping encoder/block_003/layer_000/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 2048) for ['encoder', 'block_003', 'layer_001', 'DenseReluDense', 'wi', 'kernel']\n",
            "Initialize PyTorch weight ['encoder', 'block_003', 'layer_001', 'DenseReluDense', 'wi', 'kernel']\n",
            "Skipping encoder/block_003/layer_001/DenseReluDense/wi/kernel_slot_vc\n",
            "Skipping encoder/block_003/layer_001/DenseReluDense/wi/kernel_slot_vr\n",
            "Transposing numpy weight of shape (2048, 512) for ['encoder', 'block_003', 'layer_001', 'DenseReluDense', 'wo', 'kernel']\n",
            "Initialize PyTorch weight ['encoder', 'block_003', 'layer_001', 'DenseReluDense', 'wo', 'kernel']\n",
            "Skipping encoder/block_003/layer_001/DenseReluDense/wo/kernel_slot_vc\n",
            "Skipping encoder/block_003/layer_001/DenseReluDense/wo/kernel_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['encoder', 'block_003', 'layer_001', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['encoder', 'block_003', 'layer_001', 'layer_norm', 'scale']\n",
            "Skipping encoder/block_003/layer_001/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 512) for ['encoder', 'block_004', 'layer_000', 'SelfAttention', 'k']\n",
            "Initialize PyTorch weight ['encoder', 'block_004', 'layer_000', 'SelfAttention', 'k']\n",
            "Skipping encoder/block_004/layer_000/SelfAttention/k_slot_vc\n",
            "Skipping encoder/block_004/layer_000/SelfAttention/k_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['encoder', 'block_004', 'layer_000', 'SelfAttention', 'o']\n",
            "Initialize PyTorch weight ['encoder', 'block_004', 'layer_000', 'SelfAttention', 'o']\n",
            "Skipping encoder/block_004/layer_000/SelfAttention/o_slot_vc\n",
            "Skipping encoder/block_004/layer_000/SelfAttention/o_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['encoder', 'block_004', 'layer_000', 'SelfAttention', 'q']\n",
            "Initialize PyTorch weight ['encoder', 'block_004', 'layer_000', 'SelfAttention', 'q']\n",
            "Skipping encoder/block_004/layer_000/SelfAttention/q_slot_vc\n",
            "Skipping encoder/block_004/layer_000/SelfAttention/q_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['encoder', 'block_004', 'layer_000', 'SelfAttention', 'v']\n",
            "Initialize PyTorch weight ['encoder', 'block_004', 'layer_000', 'SelfAttention', 'v']\n",
            "Skipping encoder/block_004/layer_000/SelfAttention/v_slot_vc\n",
            "Skipping encoder/block_004/layer_000/SelfAttention/v_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['encoder', 'block_004', 'layer_000', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['encoder', 'block_004', 'layer_000', 'layer_norm', 'scale']\n",
            "Skipping encoder/block_004/layer_000/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 2048) for ['encoder', 'block_004', 'layer_001', 'DenseReluDense', 'wi', 'kernel']\n",
            "Initialize PyTorch weight ['encoder', 'block_004', 'layer_001', 'DenseReluDense', 'wi', 'kernel']\n",
            "Skipping encoder/block_004/layer_001/DenseReluDense/wi/kernel_slot_vc\n",
            "Skipping encoder/block_004/layer_001/DenseReluDense/wi/kernel_slot_vr\n",
            "Transposing numpy weight of shape (2048, 512) for ['encoder', 'block_004', 'layer_001', 'DenseReluDense', 'wo', 'kernel']\n",
            "Initialize PyTorch weight ['encoder', 'block_004', 'layer_001', 'DenseReluDense', 'wo', 'kernel']\n",
            "Skipping encoder/block_004/layer_001/DenseReluDense/wo/kernel_slot_vc\n",
            "Skipping encoder/block_004/layer_001/DenseReluDense/wo/kernel_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['encoder', 'block_004', 'layer_001', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['encoder', 'block_004', 'layer_001', 'layer_norm', 'scale']\n",
            "Skipping encoder/block_004/layer_001/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 512) for ['encoder', 'block_005', 'layer_000', 'SelfAttention', 'k']\n",
            "Initialize PyTorch weight ['encoder', 'block_005', 'layer_000', 'SelfAttention', 'k']\n",
            "Skipping encoder/block_005/layer_000/SelfAttention/k_slot_vc\n",
            "Skipping encoder/block_005/layer_000/SelfAttention/k_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['encoder', 'block_005', 'layer_000', 'SelfAttention', 'o']\n",
            "Initialize PyTorch weight ['encoder', 'block_005', 'layer_000', 'SelfAttention', 'o']\n",
            "Skipping encoder/block_005/layer_000/SelfAttention/o_slot_vc\n",
            "Skipping encoder/block_005/layer_000/SelfAttention/o_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['encoder', 'block_005', 'layer_000', 'SelfAttention', 'q']\n",
            "Initialize PyTorch weight ['encoder', 'block_005', 'layer_000', 'SelfAttention', 'q']\n",
            "Skipping encoder/block_005/layer_000/SelfAttention/q_slot_vc\n",
            "Skipping encoder/block_005/layer_000/SelfAttention/q_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['encoder', 'block_005', 'layer_000', 'SelfAttention', 'v']\n",
            "Initialize PyTorch weight ['encoder', 'block_005', 'layer_000', 'SelfAttention', 'v']\n",
            "Skipping encoder/block_005/layer_000/SelfAttention/v_slot_vc\n",
            "Skipping encoder/block_005/layer_000/SelfAttention/v_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['encoder', 'block_005', 'layer_000', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['encoder', 'block_005', 'layer_000', 'layer_norm', 'scale']\n",
            "Skipping encoder/block_005/layer_000/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 2048) for ['encoder', 'block_005', 'layer_001', 'DenseReluDense', 'wi', 'kernel']\n",
            "Initialize PyTorch weight ['encoder', 'block_005', 'layer_001', 'DenseReluDense', 'wi', 'kernel']\n",
            "Skipping encoder/block_005/layer_001/DenseReluDense/wi/kernel_slot_vc\n",
            "Skipping encoder/block_005/layer_001/DenseReluDense/wi/kernel_slot_vr\n",
            "Transposing numpy weight of shape (2048, 512) for ['encoder', 'block_005', 'layer_001', 'DenseReluDense', 'wo', 'kernel']\n",
            "Initialize PyTorch weight ['encoder', 'block_005', 'layer_001', 'DenseReluDense', 'wo', 'kernel']\n",
            "Skipping encoder/block_005/layer_001/DenseReluDense/wo/kernel_slot_vc\n",
            "Skipping encoder/block_005/layer_001/DenseReluDense/wo/kernel_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['encoder', 'block_005', 'layer_001', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['encoder', 'block_005', 'layer_001', 'layer_norm', 'scale']\n",
            "Skipping encoder/block_005/layer_001/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512,) for ['encoder', 'final_layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['encoder', 'final_layer_norm', 'scale']\n",
            "Skipping encoder/final_layer_norm/scale_slot_v\n",
            "Skipping global_step\n",
            "Initialize PyTorch weight ['shared', 'embedding']\n",
            "Skipping shared/embedding_slot_vc\n",
            "Skipping shared/embedding_slot_vr\n",
            "Weights not copied to PyTorch model: .\n",
            "Save PyTorch model to ./dumps\n",
            "Configuration saved in ./dumps/config.json\n",
            "Model weights saved in ./dumps/pytorch_model.bin\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mVX5MFN-jwnK",
        "outputId": "738239d3-0d6b-4052-8426-bc95f7c5c53b"
      },
      "source": [
        "# Download the validation script\n",
        "!gsutil cp gs://{bucket_name}/pyTorch_coversion/code/validation.py ./validation.py\n",
        "!mkdir core\n",
        "\n",
        "!gsutil -m cp -r \\\n",
        "  \"gs://code_review_automation/pyTorch_coversion/core/Logger.py\" \\\n",
        "  ./core/\n",
        "\n",
        "# Download the model and vocab\n",
        "!gsutil cp gs://{bucket_name}/CodeReviewModel/TestModel.model ./current_model/TestModel.model\n",
        "!gsutil cp gs://{bucket_name}/CodeReviewModel/TestModel.vocab ./current_model/TestModel.vocab"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying gs://code_review_automation/pyTorch_coversion/code/validation.py...\n",
            "/ [1 files][  6.1 KiB/  6.1 KiB]                                                \n",
            "Operation completed over 1 objects/6.1 KiB.                                      \n",
            "Copying gs://code_review_automation/pyTorch_coversion/core/Logger.py...\n",
            "/ [1/1 files][   1016 B/   1016 B] 100% Done                                    \n",
            "Operation completed over 1 objects/1016.0 B.                                     \n",
            "Copying gs://code_review_automation/CodeReviewModel/TestModel.model...\n",
            "- [1 files][762.0 KiB/762.0 KiB]                                                \n",
            "Operation completed over 1 objects/762.0 KiB.                                    \n",
            "Copying gs://code_review_automation/CodeReviewModel/TestModel.vocab...\n",
            "/ [1 files][557.4 KiB/557.4 KiB]                                                \n",
            "Operation completed over 1 objects/557.4 KiB.                                    \n",
            "Copying gs://code_review_automation/dataset/old/fineTuningDataset_v1/codeANDcomment_code/test.tsv...\n",
            "/ [1 files][  1.1 MiB/  1.1 MiB]                                                \n",
            "Operation completed over 1 objects/1.1 MiB.                                      \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7U71mjoijsnw",
        "outputId": "0b1c7be1-7fe6-4427-db5f-a92becc4470c"
      },
      "source": [
        "# Download the test set to get model accuracy on it\n",
        "!gsutil cp gs://{bucket_name}/dataset/old/fineTuningDataset_v1/codeANDcomment_code/test.tsv ./data/codeANDcomment_code/test.tsv\n",
        "\n",
        "# prepare source and target files\n",
        "df = pd.read_csv(\"./data/codeANDcomment_code/test.tsv\", sep='\\t', names=[\"source\",\"target\"])\n",
        "\n",
        "# initialize source and target files\n",
        "f = open(\"./data/codeANDcomment_code/test.source\", \"w\")\n",
        "f.close()\n",
        "f = open(\"./data/codeANDcomment_code/test.target\", \"w\")\n",
        "\n",
        "\n",
        "f.close()\n",
        "with open(\"./data/codeANDcomment_code/test.source\", \"a\") as source:\n",
        "  with open(\"./data/codeANDcomment_code/test.target\", \"a\") as target:\n",
        "    for index, row in df.iterrows():\n",
        "      source.write(row.source + \"\\n\")\n",
        "      target.write(row.target + \"\\n\")\n",
        "\n",
        "# run validation script\n",
        "!python3 validation.py --beam_size 5 --batch_size 100 --data_dir /content/data/codeANDcomment_code --tokenizer_name ./current_model/TestModel.model --model_name_or_path ./dumps/pytorch_model.bin --config_name ./current_model/config.json"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying gs://code_review_automation/dataset/old/fineTuningDataset_v1/codeANDcomment_code/test.tsv...\n",
            "/ [1 files][  1.1 MiB/  1.1 MiB]                                                \n",
            "Operation completed over 1 objects/1.1 MiB.                                      \n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:1621: FutureWarning: Calling T5Tokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
            "  FutureWarning,\n",
            "  0% 0/18 [00:00<?, ?it/s]"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}