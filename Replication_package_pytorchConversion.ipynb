{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Replication_package_pytorchConversion.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNhjgqFtZQSnSNFOeLJcA2I",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/masies/CRA/blob/main/Replication_package_pytorchConversion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8i-CsM5qG8P4",
        "outputId": "6721a0e9-6e11-4a3e-abff-c72f03b55088"
      },
      "source": [
        "!pip install sentencepiece==0.1.94\n",
        "!pip install -q torch==1.4.0 -f https://download.pytorch.org/whl/cu101/torch_stable.html\n",
        "!pip install -q transformers==3.5.0 fast-trees\n",
        "!git clone -q https://github.com/microsoft/CodeXGLUE.git\n",
        "\n",
        "import pandas as pd\n",
        "from transformers import T5Tokenizer, T5Config, T5ForConditionalGeneration\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import logging\n",
        "import shutil\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import json\n",
        "import sys\n",
        "import statistics\n",
        "\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "project_id = 'helical-loop-303918'\n",
        "bucket_name = 'code_review_automation'\n",
        "\n",
        "!gcloud config set project {project_id}\n",
        "\n",
        "# script for conversion in pythorch\n",
        "!gsutil cp gs://{bucket_name}/pyTorch_coversion/code/tf_2_pytorch_T5.py ./tf_2_pytorch_T5.py\n",
        "\n",
        "# Download the configuration file\n",
        "!gsutil cp gs://{bucket_name}/pyTorch_coversion/config/config.json ./current_model/config.json\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentencepiece==0.1.94\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6e/f0/7614029138ec9422f1a3ed3cd82c3bfc0821157e8032ca1828cee6b198bb/sentencepiece-0.1.94-cp37-cp37m-manylinux2014_x86_64.whl (1.1MB)\n",
            "\r\u001b[K     |▎                               | 10kB 18.7MB/s eta 0:00:01\r\u001b[K     |▋                               | 20kB 26.3MB/s eta 0:00:01\r\u001b[K     |▉                               | 30kB 25.8MB/s eta 0:00:01\r\u001b[K     |█▏                              | 40kB 19.2MB/s eta 0:00:01\r\u001b[K     |█▌                              | 51kB 15.8MB/s eta 0:00:01\r\u001b[K     |█▊                              | 61kB 17.9MB/s eta 0:00:01\r\u001b[K     |██                              | 71kB 14.1MB/s eta 0:00:01\r\u001b[K     |██▍                             | 81kB 13.5MB/s eta 0:00:01\r\u001b[K     |██▋                             | 92kB 14.5MB/s eta 0:00:01\r\u001b[K     |███                             | 102kB 14.4MB/s eta 0:00:01\r\u001b[K     |███▎                            | 112kB 14.4MB/s eta 0:00:01\r\u001b[K     |███▌                            | 122kB 14.4MB/s eta 0:00:01\r\u001b[K     |███▉                            | 133kB 14.4MB/s eta 0:00:01\r\u001b[K     |████▏                           | 143kB 14.4MB/s eta 0:00:01\r\u001b[K     |████▍                           | 153kB 14.4MB/s eta 0:00:01\r\u001b[K     |████▊                           | 163kB 14.4MB/s eta 0:00:01\r\u001b[K     |█████                           | 174kB 14.4MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 184kB 14.4MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 194kB 14.4MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 204kB 14.4MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 215kB 14.4MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 225kB 14.4MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 235kB 14.4MB/s eta 0:00:01\r\u001b[K     |███████                         | 245kB 14.4MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 256kB 14.4MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 266kB 14.4MB/s eta 0:00:01\r\u001b[K     |████████                        | 276kB 14.4MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 286kB 14.4MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 296kB 14.4MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 307kB 14.4MB/s eta 0:00:01\r\u001b[K     |█████████                       | 317kB 14.4MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 327kB 14.4MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 337kB 14.4MB/s eta 0:00:01\r\u001b[K     |██████████                      | 348kB 14.4MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 358kB 14.4MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 368kB 14.4MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 378kB 14.4MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 389kB 14.4MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 399kB 14.4MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 409kB 14.4MB/s eta 0:00:01\r\u001b[K     |████████████                    | 419kB 14.4MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 430kB 14.4MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 440kB 14.4MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 450kB 14.4MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 460kB 14.4MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 471kB 14.4MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 481kB 14.4MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 491kB 14.4MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 501kB 14.4MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 512kB 14.4MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 522kB 14.4MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 532kB 14.4MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 542kB 14.4MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 552kB 14.4MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 563kB 14.4MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 573kB 14.4MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 583kB 14.4MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 593kB 14.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 604kB 14.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 614kB 14.4MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 624kB 14.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 634kB 14.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 645kB 14.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 655kB 14.4MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 665kB 14.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 675kB 14.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 686kB 14.4MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 696kB 14.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 706kB 14.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 716kB 14.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 727kB 14.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 737kB 14.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 747kB 14.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 757kB 14.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 768kB 14.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 778kB 14.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 788kB 14.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 798kB 14.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 808kB 14.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 819kB 14.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 829kB 14.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 839kB 14.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 849kB 14.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 860kB 14.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 870kB 14.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 880kB 14.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 890kB 14.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 901kB 14.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 911kB 14.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 921kB 14.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 931kB 14.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 942kB 14.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 952kB 14.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 962kB 14.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 972kB 14.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 983kB 14.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 993kB 14.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.0MB 14.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.0MB 14.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.0MB 14.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.0MB 14.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.0MB 14.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.1MB 14.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 1.1MB 14.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 1.1MB 14.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.1MB 14.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 1.1MB 14.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 1.1MB 14.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.1MB 14.4MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.94\n",
            "\u001b[K     |████████████████████████████████| 753.4MB 20kB/s \n",
            "\u001b[31mERROR: torchvision 0.9.1+cu101 has requirement torch==1.8.1, but you'll have torch 1.4.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: torchtext 0.9.1 has requirement torch==1.8.1, but you'll have torch 1.4.0 which is incompatible.\u001b[0m\n",
            "\u001b[K     |████████████████████████████████| 1.3MB 14.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 901kB 51.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1MB 37.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.9MB 53.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 163kB 46.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 112kB 56.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 9.3MB/s \n",
            "\u001b[?25h  Building wheel for tree-sitter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Updated property [core/project].\n",
            "\n",
            "\n",
            "To take a quick anonymous survey, run:\n",
            "  $ gcloud survey\n",
            "\n",
            "Copying gs://code_review_automation/pyTorch_coversion/code/tf_2_pytorch_T5.py...\n",
            "/ [1 files][  2.3 KiB/  2.3 KiB]                                                \n",
            "Operation completed over 1 objects/2.3 KiB.                                      \n",
            "Copying gs://code_review_automation/pyTorch_coversion/config/config.json...\n",
            "/ [1 files][  463.0 B/  463.0 B]                                                \n",
            "Operation completed over 1 objects/463.0 B.                                      \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-xKBubBHvGD",
        "outputId": "a68d41a1-151f-4838-cb1f-780521b0b939"
      },
      "source": [
        "# Download the selected best model\n",
        "model_number = 800000\n",
        "\n",
        "!mkdir dumps\n",
        "!mkdir current_model\n",
        "!gsutil -m cp \\\n",
        "  \"gs://code_review_automation/fine_tuning/HP_tuning/big_model/mixture/slanted/model.ckpt-{model_number}.data-00000-of-00002\" \\\n",
        "  \"gs://code_review_automation/fine_tuning/HP_tuning/big_model/mixture/slanted/model.ckpt-{model_number}.data-00001-of-00002\" \\\n",
        "  \"gs://code_review_automation/fine_tuning/HP_tuning/big_model/mixture/slanted/model.ckpt-{model_number}.index\" \\\n",
        "  \"gs://code_review_automation/fine_tuning/HP_tuning/big_model/mixture/slanted/model.ckpt-{model_number}.meta\" \\\n",
        "  ./current_model/\n",
        "\n",
        "# Download the model and vocab\n",
        "!gsutil cp gs://{bucket_name}/replication_package/code_review_model/TestModel.model ./current_model/TestModel.model\n",
        "!gsutil cp gs://{bucket_name}/replication_package/code_review_model/TestModel.vocab ./current_model/TestModel.vocab\n",
        "\n",
        "\n",
        "# download the test set\n",
        "!gsutil cp gs://{bucket_name}/replication_package/dataset/fine-tuning/large/code_comment/test.tsv ./data/test.tsv\n",
        "\n",
        "# prepare source and target files\n",
        "df = pd.read_csv(\"./data/test.tsv\", sep='\\t', names=[\"source\",\"target\"])\n",
        "\n",
        "# initialize source and target files\n",
        "f = open(\"./data/test.source\", \"w\")\n",
        "f.close()\n",
        "f = open(\"./data/test.target\", \"w\")\n",
        "f.close()\n",
        "\n",
        "with open(\"./data/test.source\", \"a\") as source:\n",
        "  with open(\"./data/test.target\", \"a\") as target:\n",
        "    for index, row in df.iterrows():\n",
        "      source.write(\"code&comment2code: \" + row.source + \"\\n\")\n",
        "      target.write(row.target + \"\\n\")\n",
        "\n",
        "# Convert the model\n",
        "!python3 ./tf_2_pytorch_T5.py --tf_checkpoint_path ./current_model/model.ckpt-{model_number} --config_file ./current_model/config.json --pytorch_dump_path ./dumps\n",
        "\n",
        "!head -n 1 ./data/test.source\n",
        "!head -n 1 ./data/test.target"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘dumps’: File exists\n",
            "mkdir: cannot create directory ‘current_model’: File exists\n",
            "Copying gs://code_review_automation/fine_tuning/HP_tuning/big_model/mixture/slanted/model.ckpt-800000.data-00000-of-00002...\n",
            "Copying gs://code_review_automation/fine_tuning/HP_tuning/big_model/mixture/slanted/model.ckpt-800000.data-00001-of-00002...\n",
            "Copying gs://code_review_automation/fine_tuning/HP_tuning/big_model/mixture/slanted/model.ckpt-800000.index...\n",
            "Copying gs://code_review_automation/fine_tuning/HP_tuning/big_model/mixture/slanted/model.ckpt-800000.meta...\n",
            "/ [4/4 files][129.0 MiB/129.0 MiB] 100% Done                                    \n",
            "Operation completed over 4 objects/129.0 MiB.                                    \n",
            "Copying gs://code_review_automation/replication_package/code_review_model/TestModel.model...\n",
            "/ [1 files][761.9 KiB/761.9 KiB]                                                \n",
            "Operation completed over 1 objects/761.9 KiB.                                    \n",
            "Copying gs://code_review_automation/replication_package/code_review_model/TestModel.vocab...\n",
            "/ [1 files][557.4 KiB/557.4 KiB]                                                \n",
            "Operation completed over 1 objects/557.4 KiB.                                    \n",
            "Copying gs://code_review_automation/replication_package/dataset/fine-tuning/large/code_comment/test.tsv...\n",
            "- [1 files][ 12.4 MiB/ 12.4 MiB]                                                \n",
            "Operation completed over 1 objects/12.4 MiB.                                     \n",
            "2021-05-27 20:29:13.821349: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "Building PyTorch model from configuration: T5Config {\n",
            "  \"architectures\": [\n",
            "    \"T5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 512,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"n_positions\": 512,\n",
            "  \"num_decoder_layers\": 6,\n",
            "  \"num_heads\": 8,\n",
            "  \"num_layers\": 6,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "Converting TensorFlow checkpoint from /content/current_model/model.ckpt-800000\n",
            "Loading TF weight decoder/block_000/layer_000/SelfAttention/k with shape [512, 512]\n",
            "Loading TF weight decoder/block_000/layer_000/SelfAttention/k_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_000/layer_000/SelfAttention/k_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_000/layer_000/SelfAttention/o with shape [512, 512]\n",
            "Loading TF weight decoder/block_000/layer_000/SelfAttention/o_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_000/layer_000/SelfAttention/o_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_000/layer_000/SelfAttention/q with shape [512, 512]\n",
            "Loading TF weight decoder/block_000/layer_000/SelfAttention/q_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_000/layer_000/SelfAttention/q_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_000/layer_000/SelfAttention/relative_attention_bias with shape [8, 32]\n",
            "Loading TF weight decoder/block_000/layer_000/SelfAttention/relative_attention_bias_slot_v with shape [8, 32]\n",
            "Loading TF weight decoder/block_000/layer_000/SelfAttention/v with shape [512, 512]\n",
            "Loading TF weight decoder/block_000/layer_000/SelfAttention/v_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_000/layer_000/SelfAttention/v_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_000/layer_000/layer_norm/scale with shape [512]\n",
            "Loading TF weight decoder/block_000/layer_000/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight decoder/block_000/layer_001/EncDecAttention/k with shape [512, 512]\n",
            "Loading TF weight decoder/block_000/layer_001/EncDecAttention/k_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_000/layer_001/EncDecAttention/k_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_000/layer_001/EncDecAttention/o with shape [512, 512]\n",
            "Loading TF weight decoder/block_000/layer_001/EncDecAttention/o_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_000/layer_001/EncDecAttention/o_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_000/layer_001/EncDecAttention/q with shape [512, 512]\n",
            "Loading TF weight decoder/block_000/layer_001/EncDecAttention/q_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_000/layer_001/EncDecAttention/q_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_000/layer_001/EncDecAttention/v with shape [512, 512]\n",
            "Loading TF weight decoder/block_000/layer_001/EncDecAttention/v_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_000/layer_001/EncDecAttention/v_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_000/layer_001/layer_norm/scale with shape [512]\n",
            "Loading TF weight decoder/block_000/layer_001/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight decoder/block_000/layer_002/DenseReluDense/wi/kernel with shape [512, 2048]\n",
            "Loading TF weight decoder/block_000/layer_002/DenseReluDense/wi/kernel_slot_vc with shape [2048]\n",
            "Loading TF weight decoder/block_000/layer_002/DenseReluDense/wi/kernel_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_000/layer_002/DenseReluDense/wo/kernel with shape [2048, 512]\n",
            "Loading TF weight decoder/block_000/layer_002/DenseReluDense/wo/kernel_slot_vc with shape [2048]\n",
            "Loading TF weight decoder/block_000/layer_002/DenseReluDense/wo/kernel_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_000/layer_002/layer_norm/scale with shape [512]\n",
            "Loading TF weight decoder/block_000/layer_002/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight decoder/block_001/layer_000/SelfAttention/k with shape [512, 512]\n",
            "Loading TF weight decoder/block_001/layer_000/SelfAttention/k_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_001/layer_000/SelfAttention/k_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_001/layer_000/SelfAttention/o with shape [512, 512]\n",
            "Loading TF weight decoder/block_001/layer_000/SelfAttention/o_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_001/layer_000/SelfAttention/o_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_001/layer_000/SelfAttention/q with shape [512, 512]\n",
            "Loading TF weight decoder/block_001/layer_000/SelfAttention/q_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_001/layer_000/SelfAttention/q_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_001/layer_000/SelfAttention/v with shape [512, 512]\n",
            "Loading TF weight decoder/block_001/layer_000/SelfAttention/v_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_001/layer_000/SelfAttention/v_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_001/layer_000/layer_norm/scale with shape [512]\n",
            "Loading TF weight decoder/block_001/layer_000/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight decoder/block_001/layer_001/EncDecAttention/k with shape [512, 512]\n",
            "Loading TF weight decoder/block_001/layer_001/EncDecAttention/k_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_001/layer_001/EncDecAttention/k_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_001/layer_001/EncDecAttention/o with shape [512, 512]\n",
            "Loading TF weight decoder/block_001/layer_001/EncDecAttention/o_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_001/layer_001/EncDecAttention/o_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_001/layer_001/EncDecAttention/q with shape [512, 512]\n",
            "Loading TF weight decoder/block_001/layer_001/EncDecAttention/q_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_001/layer_001/EncDecAttention/q_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_001/layer_001/EncDecAttention/v with shape [512, 512]\n",
            "Loading TF weight decoder/block_001/layer_001/EncDecAttention/v_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_001/layer_001/EncDecAttention/v_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_001/layer_001/layer_norm/scale with shape [512]\n",
            "Loading TF weight decoder/block_001/layer_001/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight decoder/block_001/layer_002/DenseReluDense/wi/kernel with shape [512, 2048]\n",
            "Loading TF weight decoder/block_001/layer_002/DenseReluDense/wi/kernel_slot_vc with shape [2048]\n",
            "Loading TF weight decoder/block_001/layer_002/DenseReluDense/wi/kernel_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_001/layer_002/DenseReluDense/wo/kernel with shape [2048, 512]\n",
            "Loading TF weight decoder/block_001/layer_002/DenseReluDense/wo/kernel_slot_vc with shape [2048]\n",
            "Loading TF weight decoder/block_001/layer_002/DenseReluDense/wo/kernel_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_001/layer_002/layer_norm/scale with shape [512]\n",
            "Loading TF weight decoder/block_001/layer_002/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight decoder/block_002/layer_000/SelfAttention/k with shape [512, 512]\n",
            "Loading TF weight decoder/block_002/layer_000/SelfAttention/k_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_002/layer_000/SelfAttention/k_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_002/layer_000/SelfAttention/o with shape [512, 512]\n",
            "Loading TF weight decoder/block_002/layer_000/SelfAttention/o_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_002/layer_000/SelfAttention/o_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_002/layer_000/SelfAttention/q with shape [512, 512]\n",
            "Loading TF weight decoder/block_002/layer_000/SelfAttention/q_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_002/layer_000/SelfAttention/q_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_002/layer_000/SelfAttention/v with shape [512, 512]\n",
            "Loading TF weight decoder/block_002/layer_000/SelfAttention/v_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_002/layer_000/SelfAttention/v_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_002/layer_000/layer_norm/scale with shape [512]\n",
            "Loading TF weight decoder/block_002/layer_000/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight decoder/block_002/layer_001/EncDecAttention/k with shape [512, 512]\n",
            "Loading TF weight decoder/block_002/layer_001/EncDecAttention/k_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_002/layer_001/EncDecAttention/k_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_002/layer_001/EncDecAttention/o with shape [512, 512]\n",
            "Loading TF weight decoder/block_002/layer_001/EncDecAttention/o_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_002/layer_001/EncDecAttention/o_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_002/layer_001/EncDecAttention/q with shape [512, 512]\n",
            "Loading TF weight decoder/block_002/layer_001/EncDecAttention/q_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_002/layer_001/EncDecAttention/q_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_002/layer_001/EncDecAttention/v with shape [512, 512]\n",
            "Loading TF weight decoder/block_002/layer_001/EncDecAttention/v_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_002/layer_001/EncDecAttention/v_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_002/layer_001/layer_norm/scale with shape [512]\n",
            "Loading TF weight decoder/block_002/layer_001/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight decoder/block_002/layer_002/DenseReluDense/wi/kernel with shape [512, 2048]\n",
            "Loading TF weight decoder/block_002/layer_002/DenseReluDense/wi/kernel_slot_vc with shape [2048]\n",
            "Loading TF weight decoder/block_002/layer_002/DenseReluDense/wi/kernel_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_002/layer_002/DenseReluDense/wo/kernel with shape [2048, 512]\n",
            "Loading TF weight decoder/block_002/layer_002/DenseReluDense/wo/kernel_slot_vc with shape [2048]\n",
            "Loading TF weight decoder/block_002/layer_002/DenseReluDense/wo/kernel_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_002/layer_002/layer_norm/scale with shape [512]\n",
            "Loading TF weight decoder/block_002/layer_002/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight decoder/block_003/layer_000/SelfAttention/k with shape [512, 512]\n",
            "Loading TF weight decoder/block_003/layer_000/SelfAttention/k_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_003/layer_000/SelfAttention/k_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_003/layer_000/SelfAttention/o with shape [512, 512]\n",
            "Loading TF weight decoder/block_003/layer_000/SelfAttention/o_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_003/layer_000/SelfAttention/o_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_003/layer_000/SelfAttention/q with shape [512, 512]\n",
            "Loading TF weight decoder/block_003/layer_000/SelfAttention/q_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_003/layer_000/SelfAttention/q_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_003/layer_000/SelfAttention/v with shape [512, 512]\n",
            "Loading TF weight decoder/block_003/layer_000/SelfAttention/v_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_003/layer_000/SelfAttention/v_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_003/layer_000/layer_norm/scale with shape [512]\n",
            "Loading TF weight decoder/block_003/layer_000/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight decoder/block_003/layer_001/EncDecAttention/k with shape [512, 512]\n",
            "Loading TF weight decoder/block_003/layer_001/EncDecAttention/k_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_003/layer_001/EncDecAttention/k_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_003/layer_001/EncDecAttention/o with shape [512, 512]\n",
            "Loading TF weight decoder/block_003/layer_001/EncDecAttention/o_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_003/layer_001/EncDecAttention/o_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_003/layer_001/EncDecAttention/q with shape [512, 512]\n",
            "Loading TF weight decoder/block_003/layer_001/EncDecAttention/q_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_003/layer_001/EncDecAttention/q_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_003/layer_001/EncDecAttention/v with shape [512, 512]\n",
            "Loading TF weight decoder/block_003/layer_001/EncDecAttention/v_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_003/layer_001/EncDecAttention/v_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_003/layer_001/layer_norm/scale with shape [512]\n",
            "Loading TF weight decoder/block_003/layer_001/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight decoder/block_003/layer_002/DenseReluDense/wi/kernel with shape [512, 2048]\n",
            "Loading TF weight decoder/block_003/layer_002/DenseReluDense/wi/kernel_slot_vc with shape [2048]\n",
            "Loading TF weight decoder/block_003/layer_002/DenseReluDense/wi/kernel_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_003/layer_002/DenseReluDense/wo/kernel with shape [2048, 512]\n",
            "Loading TF weight decoder/block_003/layer_002/DenseReluDense/wo/kernel_slot_vc with shape [2048]\n",
            "Loading TF weight decoder/block_003/layer_002/DenseReluDense/wo/kernel_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_003/layer_002/layer_norm/scale with shape [512]\n",
            "Loading TF weight decoder/block_003/layer_002/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight decoder/block_004/layer_000/SelfAttention/k with shape [512, 512]\n",
            "Loading TF weight decoder/block_004/layer_000/SelfAttention/k_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_004/layer_000/SelfAttention/k_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_004/layer_000/SelfAttention/o with shape [512, 512]\n",
            "Loading TF weight decoder/block_004/layer_000/SelfAttention/o_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_004/layer_000/SelfAttention/o_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_004/layer_000/SelfAttention/q with shape [512, 512]\n",
            "Loading TF weight decoder/block_004/layer_000/SelfAttention/q_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_004/layer_000/SelfAttention/q_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_004/layer_000/SelfAttention/v with shape [512, 512]\n",
            "Loading TF weight decoder/block_004/layer_000/SelfAttention/v_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_004/layer_000/SelfAttention/v_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_004/layer_000/layer_norm/scale with shape [512]\n",
            "Loading TF weight decoder/block_004/layer_000/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight decoder/block_004/layer_001/EncDecAttention/k with shape [512, 512]\n",
            "Loading TF weight decoder/block_004/layer_001/EncDecAttention/k_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_004/layer_001/EncDecAttention/k_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_004/layer_001/EncDecAttention/o with shape [512, 512]\n",
            "Loading TF weight decoder/block_004/layer_001/EncDecAttention/o_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_004/layer_001/EncDecAttention/o_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_004/layer_001/EncDecAttention/q with shape [512, 512]\n",
            "Loading TF weight decoder/block_004/layer_001/EncDecAttention/q_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_004/layer_001/EncDecAttention/q_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_004/layer_001/EncDecAttention/v with shape [512, 512]\n",
            "Loading TF weight decoder/block_004/layer_001/EncDecAttention/v_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_004/layer_001/EncDecAttention/v_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_004/layer_001/layer_norm/scale with shape [512]\n",
            "Loading TF weight decoder/block_004/layer_001/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight decoder/block_004/layer_002/DenseReluDense/wi/kernel with shape [512, 2048]\n",
            "Loading TF weight decoder/block_004/layer_002/DenseReluDense/wi/kernel_slot_vc with shape [2048]\n",
            "Loading TF weight decoder/block_004/layer_002/DenseReluDense/wi/kernel_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_004/layer_002/DenseReluDense/wo/kernel with shape [2048, 512]\n",
            "Loading TF weight decoder/block_004/layer_002/DenseReluDense/wo/kernel_slot_vc with shape [2048]\n",
            "Loading TF weight decoder/block_004/layer_002/DenseReluDense/wo/kernel_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_004/layer_002/layer_norm/scale with shape [512]\n",
            "Loading TF weight decoder/block_004/layer_002/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight decoder/block_005/layer_000/SelfAttention/k with shape [512, 512]\n",
            "Loading TF weight decoder/block_005/layer_000/SelfAttention/k_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_005/layer_000/SelfAttention/k_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_005/layer_000/SelfAttention/o with shape [512, 512]\n",
            "Loading TF weight decoder/block_005/layer_000/SelfAttention/o_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_005/layer_000/SelfAttention/o_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_005/layer_000/SelfAttention/q with shape [512, 512]\n",
            "Loading TF weight decoder/block_005/layer_000/SelfAttention/q_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_005/layer_000/SelfAttention/q_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_005/layer_000/SelfAttention/v with shape [512, 512]\n",
            "Loading TF weight decoder/block_005/layer_000/SelfAttention/v_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_005/layer_000/SelfAttention/v_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_005/layer_000/layer_norm/scale with shape [512]\n",
            "Loading TF weight decoder/block_005/layer_000/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight decoder/block_005/layer_001/EncDecAttention/k with shape [512, 512]\n",
            "Loading TF weight decoder/block_005/layer_001/EncDecAttention/k_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_005/layer_001/EncDecAttention/k_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_005/layer_001/EncDecAttention/o with shape [512, 512]\n",
            "Loading TF weight decoder/block_005/layer_001/EncDecAttention/o_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_005/layer_001/EncDecAttention/o_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_005/layer_001/EncDecAttention/q with shape [512, 512]\n",
            "Loading TF weight decoder/block_005/layer_001/EncDecAttention/q_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_005/layer_001/EncDecAttention/q_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_005/layer_001/EncDecAttention/v with shape [512, 512]\n",
            "Loading TF weight decoder/block_005/layer_001/EncDecAttention/v_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_005/layer_001/EncDecAttention/v_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_005/layer_001/layer_norm/scale with shape [512]\n",
            "Loading TF weight decoder/block_005/layer_001/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight decoder/block_005/layer_002/DenseReluDense/wi/kernel with shape [512, 2048]\n",
            "Loading TF weight decoder/block_005/layer_002/DenseReluDense/wi/kernel_slot_vc with shape [2048]\n",
            "Loading TF weight decoder/block_005/layer_002/DenseReluDense/wi/kernel_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_005/layer_002/DenseReluDense/wo/kernel with shape [2048, 512]\n",
            "Loading TF weight decoder/block_005/layer_002/DenseReluDense/wo/kernel_slot_vc with shape [2048]\n",
            "Loading TF weight decoder/block_005/layer_002/DenseReluDense/wo/kernel_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_005/layer_002/layer_norm/scale with shape [512]\n",
            "Loading TF weight decoder/block_005/layer_002/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight decoder/final_layer_norm/scale with shape [512]\n",
            "Loading TF weight decoder/final_layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight encoder/block_000/layer_000/SelfAttention/k with shape [512, 512]\n",
            "Loading TF weight encoder/block_000/layer_000/SelfAttention/k_slot_vc with shape [512]\n",
            "Loading TF weight encoder/block_000/layer_000/SelfAttention/k_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_000/layer_000/SelfAttention/o with shape [512, 512]\n",
            "Loading TF weight encoder/block_000/layer_000/SelfAttention/o_slot_vc with shape [512]\n",
            "Loading TF weight encoder/block_000/layer_000/SelfAttention/o_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_000/layer_000/SelfAttention/q with shape [512, 512]\n",
            "Loading TF weight encoder/block_000/layer_000/SelfAttention/q_slot_vc with shape [512]\n",
            "Loading TF weight encoder/block_000/layer_000/SelfAttention/q_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_000/layer_000/SelfAttention/relative_attention_bias with shape [8, 32]\n",
            "Loading TF weight encoder/block_000/layer_000/SelfAttention/relative_attention_bias_slot_v with shape [8, 32]\n",
            "Loading TF weight encoder/block_000/layer_000/SelfAttention/v with shape [512, 512]\n",
            "Loading TF weight encoder/block_000/layer_000/SelfAttention/v_slot_vc with shape [512]\n",
            "Loading TF weight encoder/block_000/layer_000/SelfAttention/v_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_000/layer_000/layer_norm/scale with shape [512]\n",
            "Loading TF weight encoder/block_000/layer_000/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight encoder/block_000/layer_001/DenseReluDense/wi/kernel with shape [512, 2048]\n",
            "Loading TF weight encoder/block_000/layer_001/DenseReluDense/wi/kernel_slot_vc with shape [2048]\n",
            "Loading TF weight encoder/block_000/layer_001/DenseReluDense/wi/kernel_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_000/layer_001/DenseReluDense/wo/kernel with shape [2048, 512]\n",
            "Loading TF weight encoder/block_000/layer_001/DenseReluDense/wo/kernel_slot_vc with shape [2048]\n",
            "Loading TF weight encoder/block_000/layer_001/DenseReluDense/wo/kernel_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_000/layer_001/layer_norm/scale with shape [512]\n",
            "Loading TF weight encoder/block_000/layer_001/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight encoder/block_001/layer_000/SelfAttention/k with shape [512, 512]\n",
            "Loading TF weight encoder/block_001/layer_000/SelfAttention/k_slot_vc with shape [512]\n",
            "Loading TF weight encoder/block_001/layer_000/SelfAttention/k_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_001/layer_000/SelfAttention/o with shape [512, 512]\n",
            "Loading TF weight encoder/block_001/layer_000/SelfAttention/o_slot_vc with shape [512]\n",
            "Loading TF weight encoder/block_001/layer_000/SelfAttention/o_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_001/layer_000/SelfAttention/q with shape [512, 512]\n",
            "Loading TF weight encoder/block_001/layer_000/SelfAttention/q_slot_vc with shape [512]\n",
            "Loading TF weight encoder/block_001/layer_000/SelfAttention/q_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_001/layer_000/SelfAttention/v with shape [512, 512]\n",
            "Loading TF weight encoder/block_001/layer_000/SelfAttention/v_slot_vc with shape [512]\n",
            "Loading TF weight encoder/block_001/layer_000/SelfAttention/v_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_001/layer_000/layer_norm/scale with shape [512]\n",
            "Loading TF weight encoder/block_001/layer_000/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight encoder/block_001/layer_001/DenseReluDense/wi/kernel with shape [512, 2048]\n",
            "Loading TF weight encoder/block_001/layer_001/DenseReluDense/wi/kernel_slot_vc with shape [2048]\n",
            "Loading TF weight encoder/block_001/layer_001/DenseReluDense/wi/kernel_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_001/layer_001/DenseReluDense/wo/kernel with shape [2048, 512]\n",
            "Loading TF weight encoder/block_001/layer_001/DenseReluDense/wo/kernel_slot_vc with shape [2048]\n",
            "Loading TF weight encoder/block_001/layer_001/DenseReluDense/wo/kernel_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_001/layer_001/layer_norm/scale with shape [512]\n",
            "Loading TF weight encoder/block_001/layer_001/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight encoder/block_002/layer_000/SelfAttention/k with shape [512, 512]\n",
            "Loading TF weight encoder/block_002/layer_000/SelfAttention/k_slot_vc with shape [512]\n",
            "Loading TF weight encoder/block_002/layer_000/SelfAttention/k_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_002/layer_000/SelfAttention/o with shape [512, 512]\n",
            "Loading TF weight encoder/block_002/layer_000/SelfAttention/o_slot_vc with shape [512]\n",
            "Loading TF weight encoder/block_002/layer_000/SelfAttention/o_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_002/layer_000/SelfAttention/q with shape [512, 512]\n",
            "Loading TF weight encoder/block_002/layer_000/SelfAttention/q_slot_vc with shape [512]\n",
            "Loading TF weight encoder/block_002/layer_000/SelfAttention/q_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_002/layer_000/SelfAttention/v with shape [512, 512]\n",
            "Loading TF weight encoder/block_002/layer_000/SelfAttention/v_slot_vc with shape [512]\n",
            "Loading TF weight encoder/block_002/layer_000/SelfAttention/v_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_002/layer_000/layer_norm/scale with shape [512]\n",
            "Loading TF weight encoder/block_002/layer_000/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight encoder/block_002/layer_001/DenseReluDense/wi/kernel with shape [512, 2048]\n",
            "Loading TF weight encoder/block_002/layer_001/DenseReluDense/wi/kernel_slot_vc with shape [2048]\n",
            "Loading TF weight encoder/block_002/layer_001/DenseReluDense/wi/kernel_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_002/layer_001/DenseReluDense/wo/kernel with shape [2048, 512]\n",
            "Loading TF weight encoder/block_002/layer_001/DenseReluDense/wo/kernel_slot_vc with shape [2048]\n",
            "Loading TF weight encoder/block_002/layer_001/DenseReluDense/wo/kernel_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_002/layer_001/layer_norm/scale with shape [512]\n",
            "Loading TF weight encoder/block_002/layer_001/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight encoder/block_003/layer_000/SelfAttention/k with shape [512, 512]\n",
            "Loading TF weight encoder/block_003/layer_000/SelfAttention/k_slot_vc with shape [512]\n",
            "Loading TF weight encoder/block_003/layer_000/SelfAttention/k_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_003/layer_000/SelfAttention/o with shape [512, 512]\n",
            "Loading TF weight encoder/block_003/layer_000/SelfAttention/o_slot_vc with shape [512]\n",
            "Loading TF weight encoder/block_003/layer_000/SelfAttention/o_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_003/layer_000/SelfAttention/q with shape [512, 512]\n",
            "Loading TF weight encoder/block_003/layer_000/SelfAttention/q_slot_vc with shape [512]\n",
            "Loading TF weight encoder/block_003/layer_000/SelfAttention/q_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_003/layer_000/SelfAttention/v with shape [512, 512]\n",
            "Loading TF weight encoder/block_003/layer_000/SelfAttention/v_slot_vc with shape [512]\n",
            "Loading TF weight encoder/block_003/layer_000/SelfAttention/v_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_003/layer_000/layer_norm/scale with shape [512]\n",
            "Loading TF weight encoder/block_003/layer_000/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight encoder/block_003/layer_001/DenseReluDense/wi/kernel with shape [512, 2048]\n",
            "Loading TF weight encoder/block_003/layer_001/DenseReluDense/wi/kernel_slot_vc with shape [2048]\n",
            "Loading TF weight encoder/block_003/layer_001/DenseReluDense/wi/kernel_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_003/layer_001/DenseReluDense/wo/kernel with shape [2048, 512]\n",
            "Loading TF weight encoder/block_003/layer_001/DenseReluDense/wo/kernel_slot_vc with shape [2048]\n",
            "Loading TF weight encoder/block_003/layer_001/DenseReluDense/wo/kernel_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_003/layer_001/layer_norm/scale with shape [512]\n",
            "Loading TF weight encoder/block_003/layer_001/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight encoder/block_004/layer_000/SelfAttention/k with shape [512, 512]\n",
            "Loading TF weight encoder/block_004/layer_000/SelfAttention/k_slot_vc with shape [512]\n",
            "Loading TF weight encoder/block_004/layer_000/SelfAttention/k_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_004/layer_000/SelfAttention/o with shape [512, 512]\n",
            "Loading TF weight encoder/block_004/layer_000/SelfAttention/o_slot_vc with shape [512]\n",
            "Loading TF weight encoder/block_004/layer_000/SelfAttention/o_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_004/layer_000/SelfAttention/q with shape [512, 512]\n",
            "Loading TF weight encoder/block_004/layer_000/SelfAttention/q_slot_vc with shape [512]\n",
            "Loading TF weight encoder/block_004/layer_000/SelfAttention/q_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_004/layer_000/SelfAttention/v with shape [512, 512]\n",
            "Loading TF weight encoder/block_004/layer_000/SelfAttention/v_slot_vc with shape [512]\n",
            "Loading TF weight encoder/block_004/layer_000/SelfAttention/v_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_004/layer_000/layer_norm/scale with shape [512]\n",
            "Loading TF weight encoder/block_004/layer_000/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight encoder/block_004/layer_001/DenseReluDense/wi/kernel with shape [512, 2048]\n",
            "Loading TF weight encoder/block_004/layer_001/DenseReluDense/wi/kernel_slot_vc with shape [2048]\n",
            "Loading TF weight encoder/block_004/layer_001/DenseReluDense/wi/kernel_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_004/layer_001/DenseReluDense/wo/kernel with shape [2048, 512]\n",
            "Loading TF weight encoder/block_004/layer_001/DenseReluDense/wo/kernel_slot_vc with shape [2048]\n",
            "Loading TF weight encoder/block_004/layer_001/DenseReluDense/wo/kernel_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_004/layer_001/layer_norm/scale with shape [512]\n",
            "Loading TF weight encoder/block_004/layer_001/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight encoder/block_005/layer_000/SelfAttention/k with shape [512, 512]\n",
            "Loading TF weight encoder/block_005/layer_000/SelfAttention/k_slot_vc with shape [512]\n",
            "Loading TF weight encoder/block_005/layer_000/SelfAttention/k_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_005/layer_000/SelfAttention/o with shape [512, 512]\n",
            "Loading TF weight encoder/block_005/layer_000/SelfAttention/o_slot_vc with shape [512]\n",
            "Loading TF weight encoder/block_005/layer_000/SelfAttention/o_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_005/layer_000/SelfAttention/q with shape [512, 512]\n",
            "Loading TF weight encoder/block_005/layer_000/SelfAttention/q_slot_vc with shape [512]\n",
            "Loading TF weight encoder/block_005/layer_000/SelfAttention/q_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_005/layer_000/SelfAttention/v with shape [512, 512]\n",
            "Loading TF weight encoder/block_005/layer_000/SelfAttention/v_slot_vc with shape [512]\n",
            "Loading TF weight encoder/block_005/layer_000/SelfAttention/v_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_005/layer_000/layer_norm/scale with shape [512]\n",
            "Loading TF weight encoder/block_005/layer_000/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight encoder/block_005/layer_001/DenseReluDense/wi/kernel with shape [512, 2048]\n",
            "Loading TF weight encoder/block_005/layer_001/DenseReluDense/wi/kernel_slot_vc with shape [2048]\n",
            "Loading TF weight encoder/block_005/layer_001/DenseReluDense/wi/kernel_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_005/layer_001/DenseReluDense/wo/kernel with shape [2048, 512]\n",
            "Loading TF weight encoder/block_005/layer_001/DenseReluDense/wo/kernel_slot_vc with shape [2048]\n",
            "Loading TF weight encoder/block_005/layer_001/DenseReluDense/wo/kernel_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_005/layer_001/layer_norm/scale with shape [512]\n",
            "Loading TF weight encoder/block_005/layer_001/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight encoder/final_layer_norm/scale with shape [512]\n",
            "Loading TF weight encoder/final_layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight global_step with shape []\n",
            "Loading TF weight shared/embedding with shape [32128, 512]\n",
            "Loading TF weight shared/embedding_slot_vc with shape [32128]\n",
            "Loading TF weight shared/embedding_slot_vr with shape [512]\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_000', 'layer_000', 'SelfAttention', 'k']\n",
            "Initialize PyTorch weight ['decoder', 'block_000', 'layer_000', 'SelfAttention', 'k']\n",
            "Skipping decoder/block_000/layer_000/SelfAttention/k_slot_vc\n",
            "Skipping decoder/block_000/layer_000/SelfAttention/k_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_000', 'layer_000', 'SelfAttention', 'o']\n",
            "Initialize PyTorch weight ['decoder', 'block_000', 'layer_000', 'SelfAttention', 'o']\n",
            "Skipping decoder/block_000/layer_000/SelfAttention/o_slot_vc\n",
            "Skipping decoder/block_000/layer_000/SelfAttention/o_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_000', 'layer_000', 'SelfAttention', 'q']\n",
            "Initialize PyTorch weight ['decoder', 'block_000', 'layer_000', 'SelfAttention', 'q']\n",
            "Skipping decoder/block_000/layer_000/SelfAttention/q_slot_vc\n",
            "Skipping decoder/block_000/layer_000/SelfAttention/q_slot_vr\n",
            "Transposing numpy weight of shape (8, 32) for ['decoder', 'block_000', 'layer_000', 'SelfAttention', 'relative_attention_bias']\n",
            "Initialize PyTorch weight ['decoder', 'block_000', 'layer_000', 'SelfAttention', 'relative_attention_bias']\n",
            "Skipping decoder/block_000/layer_000/SelfAttention/relative_attention_bias_slot_v\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_000', 'layer_000', 'SelfAttention', 'v']\n",
            "Initialize PyTorch weight ['decoder', 'block_000', 'layer_000', 'SelfAttention', 'v']\n",
            "Skipping decoder/block_000/layer_000/SelfAttention/v_slot_vc\n",
            "Skipping decoder/block_000/layer_000/SelfAttention/v_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['decoder', 'block_000', 'layer_000', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['decoder', 'block_000', 'layer_000', 'layer_norm', 'scale']\n",
            "Skipping decoder/block_000/layer_000/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_000', 'layer_001', 'EncDecAttention', 'k']\n",
            "Initialize PyTorch weight ['decoder', 'block_000', 'layer_001', 'EncDecAttention', 'k']\n",
            "Skipping decoder/block_000/layer_001/EncDecAttention/k_slot_vc\n",
            "Skipping decoder/block_000/layer_001/EncDecAttention/k_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_000', 'layer_001', 'EncDecAttention', 'o']\n",
            "Initialize PyTorch weight ['decoder', 'block_000', 'layer_001', 'EncDecAttention', 'o']\n",
            "Skipping decoder/block_000/layer_001/EncDecAttention/o_slot_vc\n",
            "Skipping decoder/block_000/layer_001/EncDecAttention/o_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_000', 'layer_001', 'EncDecAttention', 'q']\n",
            "Initialize PyTorch weight ['decoder', 'block_000', 'layer_001', 'EncDecAttention', 'q']\n",
            "Skipping decoder/block_000/layer_001/EncDecAttention/q_slot_vc\n",
            "Skipping decoder/block_000/layer_001/EncDecAttention/q_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_000', 'layer_001', 'EncDecAttention', 'v']\n",
            "Initialize PyTorch weight ['decoder', 'block_000', 'layer_001', 'EncDecAttention', 'v']\n",
            "Skipping decoder/block_000/layer_001/EncDecAttention/v_slot_vc\n",
            "Skipping decoder/block_000/layer_001/EncDecAttention/v_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['decoder', 'block_000', 'layer_001', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['decoder', 'block_000', 'layer_001', 'layer_norm', 'scale']\n",
            "Skipping decoder/block_000/layer_001/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 2048) for ['decoder', 'block_000', 'layer_002', 'DenseReluDense', 'wi', 'kernel']\n",
            "Initialize PyTorch weight ['decoder', 'block_000', 'layer_002', 'DenseReluDense', 'wi', 'kernel']\n",
            "Skipping decoder/block_000/layer_002/DenseReluDense/wi/kernel_slot_vc\n",
            "Skipping decoder/block_000/layer_002/DenseReluDense/wi/kernel_slot_vr\n",
            "Transposing numpy weight of shape (2048, 512) for ['decoder', 'block_000', 'layer_002', 'DenseReluDense', 'wo', 'kernel']\n",
            "Initialize PyTorch weight ['decoder', 'block_000', 'layer_002', 'DenseReluDense', 'wo', 'kernel']\n",
            "Skipping decoder/block_000/layer_002/DenseReluDense/wo/kernel_slot_vc\n",
            "Skipping decoder/block_000/layer_002/DenseReluDense/wo/kernel_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['decoder', 'block_000', 'layer_002', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['decoder', 'block_000', 'layer_002', 'layer_norm', 'scale']\n",
            "Skipping decoder/block_000/layer_002/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_001', 'layer_000', 'SelfAttention', 'k']\n",
            "Initialize PyTorch weight ['decoder', 'block_001', 'layer_000', 'SelfAttention', 'k']\n",
            "Skipping decoder/block_001/layer_000/SelfAttention/k_slot_vc\n",
            "Skipping decoder/block_001/layer_000/SelfAttention/k_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_001', 'layer_000', 'SelfAttention', 'o']\n",
            "Initialize PyTorch weight ['decoder', 'block_001', 'layer_000', 'SelfAttention', 'o']\n",
            "Skipping decoder/block_001/layer_000/SelfAttention/o_slot_vc\n",
            "Skipping decoder/block_001/layer_000/SelfAttention/o_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_001', 'layer_000', 'SelfAttention', 'q']\n",
            "Initialize PyTorch weight ['decoder', 'block_001', 'layer_000', 'SelfAttention', 'q']\n",
            "Skipping decoder/block_001/layer_000/SelfAttention/q_slot_vc\n",
            "Skipping decoder/block_001/layer_000/SelfAttention/q_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_001', 'layer_000', 'SelfAttention', 'v']\n",
            "Initialize PyTorch weight ['decoder', 'block_001', 'layer_000', 'SelfAttention', 'v']\n",
            "Skipping decoder/block_001/layer_000/SelfAttention/v_slot_vc\n",
            "Skipping decoder/block_001/layer_000/SelfAttention/v_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['decoder', 'block_001', 'layer_000', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['decoder', 'block_001', 'layer_000', 'layer_norm', 'scale']\n",
            "Skipping decoder/block_001/layer_000/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_001', 'layer_001', 'EncDecAttention', 'k']\n",
            "Initialize PyTorch weight ['decoder', 'block_001', 'layer_001', 'EncDecAttention', 'k']\n",
            "Skipping decoder/block_001/layer_001/EncDecAttention/k_slot_vc\n",
            "Skipping decoder/block_001/layer_001/EncDecAttention/k_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_001', 'layer_001', 'EncDecAttention', 'o']\n",
            "Initialize PyTorch weight ['decoder', 'block_001', 'layer_001', 'EncDecAttention', 'o']\n",
            "Skipping decoder/block_001/layer_001/EncDecAttention/o_slot_vc\n",
            "Skipping decoder/block_001/layer_001/EncDecAttention/o_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_001', 'layer_001', 'EncDecAttention', 'q']\n",
            "Initialize PyTorch weight ['decoder', 'block_001', 'layer_001', 'EncDecAttention', 'q']\n",
            "Skipping decoder/block_001/layer_001/EncDecAttention/q_slot_vc\n",
            "Skipping decoder/block_001/layer_001/EncDecAttention/q_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_001', 'layer_001', 'EncDecAttention', 'v']\n",
            "Initialize PyTorch weight ['decoder', 'block_001', 'layer_001', 'EncDecAttention', 'v']\n",
            "Skipping decoder/block_001/layer_001/EncDecAttention/v_slot_vc\n",
            "Skipping decoder/block_001/layer_001/EncDecAttention/v_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['decoder', 'block_001', 'layer_001', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['decoder', 'block_001', 'layer_001', 'layer_norm', 'scale']\n",
            "Skipping decoder/block_001/layer_001/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 2048) for ['decoder', 'block_001', 'layer_002', 'DenseReluDense', 'wi', 'kernel']\n",
            "Initialize PyTorch weight ['decoder', 'block_001', 'layer_002', 'DenseReluDense', 'wi', 'kernel']\n",
            "Skipping decoder/block_001/layer_002/DenseReluDense/wi/kernel_slot_vc\n",
            "Skipping decoder/block_001/layer_002/DenseReluDense/wi/kernel_slot_vr\n",
            "Transposing numpy weight of shape (2048, 512) for ['decoder', 'block_001', 'layer_002', 'DenseReluDense', 'wo', 'kernel']\n",
            "Initialize PyTorch weight ['decoder', 'block_001', 'layer_002', 'DenseReluDense', 'wo', 'kernel']\n",
            "Skipping decoder/block_001/layer_002/DenseReluDense/wo/kernel_slot_vc\n",
            "Skipping decoder/block_001/layer_002/DenseReluDense/wo/kernel_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['decoder', 'block_001', 'layer_002', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['decoder', 'block_001', 'layer_002', 'layer_norm', 'scale']\n",
            "Skipping decoder/block_001/layer_002/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_002', 'layer_000', 'SelfAttention', 'k']\n",
            "Initialize PyTorch weight ['decoder', 'block_002', 'layer_000', 'SelfAttention', 'k']\n",
            "Skipping decoder/block_002/layer_000/SelfAttention/k_slot_vc\n",
            "Skipping decoder/block_002/layer_000/SelfAttention/k_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_002', 'layer_000', 'SelfAttention', 'o']\n",
            "Initialize PyTorch weight ['decoder', 'block_002', 'layer_000', 'SelfAttention', 'o']\n",
            "Skipping decoder/block_002/layer_000/SelfAttention/o_slot_vc\n",
            "Skipping decoder/block_002/layer_000/SelfAttention/o_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_002', 'layer_000', 'SelfAttention', 'q']\n",
            "Initialize PyTorch weight ['decoder', 'block_002', 'layer_000', 'SelfAttention', 'q']\n",
            "Skipping decoder/block_002/layer_000/SelfAttention/q_slot_vc\n",
            "Skipping decoder/block_002/layer_000/SelfAttention/q_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_002', 'layer_000', 'SelfAttention', 'v']\n",
            "Initialize PyTorch weight ['decoder', 'block_002', 'layer_000', 'SelfAttention', 'v']\n",
            "Skipping decoder/block_002/layer_000/SelfAttention/v_slot_vc\n",
            "Skipping decoder/block_002/layer_000/SelfAttention/v_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['decoder', 'block_002', 'layer_000', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['decoder', 'block_002', 'layer_000', 'layer_norm', 'scale']\n",
            "Skipping decoder/block_002/layer_000/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_002', 'layer_001', 'EncDecAttention', 'k']\n",
            "Initialize PyTorch weight ['decoder', 'block_002', 'layer_001', 'EncDecAttention', 'k']\n",
            "Skipping decoder/block_002/layer_001/EncDecAttention/k_slot_vc\n",
            "Skipping decoder/block_002/layer_001/EncDecAttention/k_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_002', 'layer_001', 'EncDecAttention', 'o']\n",
            "Initialize PyTorch weight ['decoder', 'block_002', 'layer_001', 'EncDecAttention', 'o']\n",
            "Skipping decoder/block_002/layer_001/EncDecAttention/o_slot_vc\n",
            "Skipping decoder/block_002/layer_001/EncDecAttention/o_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_002', 'layer_001', 'EncDecAttention', 'q']\n",
            "Initialize PyTorch weight ['decoder', 'block_002', 'layer_001', 'EncDecAttention', 'q']\n",
            "Skipping decoder/block_002/layer_001/EncDecAttention/q_slot_vc\n",
            "Skipping decoder/block_002/layer_001/EncDecAttention/q_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_002', 'layer_001', 'EncDecAttention', 'v']\n",
            "Initialize PyTorch weight ['decoder', 'block_002', 'layer_001', 'EncDecAttention', 'v']\n",
            "Skipping decoder/block_002/layer_001/EncDecAttention/v_slot_vc\n",
            "Skipping decoder/block_002/layer_001/EncDecAttention/v_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['decoder', 'block_002', 'layer_001', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['decoder', 'block_002', 'layer_001', 'layer_norm', 'scale']\n",
            "Skipping decoder/block_002/layer_001/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 2048) for ['decoder', 'block_002', 'layer_002', 'DenseReluDense', 'wi', 'kernel']\n",
            "Initialize PyTorch weight ['decoder', 'block_002', 'layer_002', 'DenseReluDense', 'wi', 'kernel']\n",
            "Skipping decoder/block_002/layer_002/DenseReluDense/wi/kernel_slot_vc\n",
            "Skipping decoder/block_002/layer_002/DenseReluDense/wi/kernel_slot_vr\n",
            "Transposing numpy weight of shape (2048, 512) for ['decoder', 'block_002', 'layer_002', 'DenseReluDense', 'wo', 'kernel']\n",
            "Initialize PyTorch weight ['decoder', 'block_002', 'layer_002', 'DenseReluDense', 'wo', 'kernel']\n",
            "Skipping decoder/block_002/layer_002/DenseReluDense/wo/kernel_slot_vc\n",
            "Skipping decoder/block_002/layer_002/DenseReluDense/wo/kernel_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['decoder', 'block_002', 'layer_002', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['decoder', 'block_002', 'layer_002', 'layer_norm', 'scale']\n",
            "Skipping decoder/block_002/layer_002/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_003', 'layer_000', 'SelfAttention', 'k']\n",
            "Initialize PyTorch weight ['decoder', 'block_003', 'layer_000', 'SelfAttention', 'k']\n",
            "Skipping decoder/block_003/layer_000/SelfAttention/k_slot_vc\n",
            "Skipping decoder/block_003/layer_000/SelfAttention/k_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_003', 'layer_000', 'SelfAttention', 'o']\n",
            "Initialize PyTorch weight ['decoder', 'block_003', 'layer_000', 'SelfAttention', 'o']\n",
            "Skipping decoder/block_003/layer_000/SelfAttention/o_slot_vc\n",
            "Skipping decoder/block_003/layer_000/SelfAttention/o_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_003', 'layer_000', 'SelfAttention', 'q']\n",
            "Initialize PyTorch weight ['decoder', 'block_003', 'layer_000', 'SelfAttention', 'q']\n",
            "Skipping decoder/block_003/layer_000/SelfAttention/q_slot_vc\n",
            "Skipping decoder/block_003/layer_000/SelfAttention/q_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_003', 'layer_000', 'SelfAttention', 'v']\n",
            "Initialize PyTorch weight ['decoder', 'block_003', 'layer_000', 'SelfAttention', 'v']\n",
            "Skipping decoder/block_003/layer_000/SelfAttention/v_slot_vc\n",
            "Skipping decoder/block_003/layer_000/SelfAttention/v_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['decoder', 'block_003', 'layer_000', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['decoder', 'block_003', 'layer_000', 'layer_norm', 'scale']\n",
            "Skipping decoder/block_003/layer_000/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_003', 'layer_001', 'EncDecAttention', 'k']\n",
            "Initialize PyTorch weight ['decoder', 'block_003', 'layer_001', 'EncDecAttention', 'k']\n",
            "Skipping decoder/block_003/layer_001/EncDecAttention/k_slot_vc\n",
            "Skipping decoder/block_003/layer_001/EncDecAttention/k_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_003', 'layer_001', 'EncDecAttention', 'o']\n",
            "Initialize PyTorch weight ['decoder', 'block_003', 'layer_001', 'EncDecAttention', 'o']\n",
            "Skipping decoder/block_003/layer_001/EncDecAttention/o_slot_vc\n",
            "Skipping decoder/block_003/layer_001/EncDecAttention/o_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_003', 'layer_001', 'EncDecAttention', 'q']\n",
            "Initialize PyTorch weight ['decoder', 'block_003', 'layer_001', 'EncDecAttention', 'q']\n",
            "Skipping decoder/block_003/layer_001/EncDecAttention/q_slot_vc\n",
            "Skipping decoder/block_003/layer_001/EncDecAttention/q_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_003', 'layer_001', 'EncDecAttention', 'v']\n",
            "Initialize PyTorch weight ['decoder', 'block_003', 'layer_001', 'EncDecAttention', 'v']\n",
            "Skipping decoder/block_003/layer_001/EncDecAttention/v_slot_vc\n",
            "Skipping decoder/block_003/layer_001/EncDecAttention/v_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['decoder', 'block_003', 'layer_001', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['decoder', 'block_003', 'layer_001', 'layer_norm', 'scale']\n",
            "Skipping decoder/block_003/layer_001/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 2048) for ['decoder', 'block_003', 'layer_002', 'DenseReluDense', 'wi', 'kernel']\n",
            "Initialize PyTorch weight ['decoder', 'block_003', 'layer_002', 'DenseReluDense', 'wi', 'kernel']\n",
            "Skipping decoder/block_003/layer_002/DenseReluDense/wi/kernel_slot_vc\n",
            "Skipping decoder/block_003/layer_002/DenseReluDense/wi/kernel_slot_vr\n",
            "Transposing numpy weight of shape (2048, 512) for ['decoder', 'block_003', 'layer_002', 'DenseReluDense', 'wo', 'kernel']\n",
            "Initialize PyTorch weight ['decoder', 'block_003', 'layer_002', 'DenseReluDense', 'wo', 'kernel']\n",
            "Skipping decoder/block_003/layer_002/DenseReluDense/wo/kernel_slot_vc\n",
            "Skipping decoder/block_003/layer_002/DenseReluDense/wo/kernel_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['decoder', 'block_003', 'layer_002', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['decoder', 'block_003', 'layer_002', 'layer_norm', 'scale']\n",
            "Skipping decoder/block_003/layer_002/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_004', 'layer_000', 'SelfAttention', 'k']\n",
            "Initialize PyTorch weight ['decoder', 'block_004', 'layer_000', 'SelfAttention', 'k']\n",
            "Skipping decoder/block_004/layer_000/SelfAttention/k_slot_vc\n",
            "Skipping decoder/block_004/layer_000/SelfAttention/k_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_004', 'layer_000', 'SelfAttention', 'o']\n",
            "Initialize PyTorch weight ['decoder', 'block_004', 'layer_000', 'SelfAttention', 'o']\n",
            "Skipping decoder/block_004/layer_000/SelfAttention/o_slot_vc\n",
            "Skipping decoder/block_004/layer_000/SelfAttention/o_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_004', 'layer_000', 'SelfAttention', 'q']\n",
            "Initialize PyTorch weight ['decoder', 'block_004', 'layer_000', 'SelfAttention', 'q']\n",
            "Skipping decoder/block_004/layer_000/SelfAttention/q_slot_vc\n",
            "Skipping decoder/block_004/layer_000/SelfAttention/q_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_004', 'layer_000', 'SelfAttention', 'v']\n",
            "Initialize PyTorch weight ['decoder', 'block_004', 'layer_000', 'SelfAttention', 'v']\n",
            "Skipping decoder/block_004/layer_000/SelfAttention/v_slot_vc\n",
            "Skipping decoder/block_004/layer_000/SelfAttention/v_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['decoder', 'block_004', 'layer_000', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['decoder', 'block_004', 'layer_000', 'layer_norm', 'scale']\n",
            "Skipping decoder/block_004/layer_000/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_004', 'layer_001', 'EncDecAttention', 'k']\n",
            "Initialize PyTorch weight ['decoder', 'block_004', 'layer_001', 'EncDecAttention', 'k']\n",
            "Skipping decoder/block_004/layer_001/EncDecAttention/k_slot_vc\n",
            "Skipping decoder/block_004/layer_001/EncDecAttention/k_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_004', 'layer_001', 'EncDecAttention', 'o']\n",
            "Initialize PyTorch weight ['decoder', 'block_004', 'layer_001', 'EncDecAttention', 'o']\n",
            "Skipping decoder/block_004/layer_001/EncDecAttention/o_slot_vc\n",
            "Skipping decoder/block_004/layer_001/EncDecAttention/o_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_004', 'layer_001', 'EncDecAttention', 'q']\n",
            "Initialize PyTorch weight ['decoder', 'block_004', 'layer_001', 'EncDecAttention', 'q']\n",
            "Skipping decoder/block_004/layer_001/EncDecAttention/q_slot_vc\n",
            "Skipping decoder/block_004/layer_001/EncDecAttention/q_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_004', 'layer_001', 'EncDecAttention', 'v']\n",
            "Initialize PyTorch weight ['decoder', 'block_004', 'layer_001', 'EncDecAttention', 'v']\n",
            "Skipping decoder/block_004/layer_001/EncDecAttention/v_slot_vc\n",
            "Skipping decoder/block_004/layer_001/EncDecAttention/v_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['decoder', 'block_004', 'layer_001', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['decoder', 'block_004', 'layer_001', 'layer_norm', 'scale']\n",
            "Skipping decoder/block_004/layer_001/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 2048) for ['decoder', 'block_004', 'layer_002', 'DenseReluDense', 'wi', 'kernel']\n",
            "Initialize PyTorch weight ['decoder', 'block_004', 'layer_002', 'DenseReluDense', 'wi', 'kernel']\n",
            "Skipping decoder/block_004/layer_002/DenseReluDense/wi/kernel_slot_vc\n",
            "Skipping decoder/block_004/layer_002/DenseReluDense/wi/kernel_slot_vr\n",
            "Transposing numpy weight of shape (2048, 512) for ['decoder', 'block_004', 'layer_002', 'DenseReluDense', 'wo', 'kernel']\n",
            "Initialize PyTorch weight ['decoder', 'block_004', 'layer_002', 'DenseReluDense', 'wo', 'kernel']\n",
            "Skipping decoder/block_004/layer_002/DenseReluDense/wo/kernel_slot_vc\n",
            "Skipping decoder/block_004/layer_002/DenseReluDense/wo/kernel_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['decoder', 'block_004', 'layer_002', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['decoder', 'block_004', 'layer_002', 'layer_norm', 'scale']\n",
            "Skipping decoder/block_004/layer_002/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_005', 'layer_000', 'SelfAttention', 'k']\n",
            "Initialize PyTorch weight ['decoder', 'block_005', 'layer_000', 'SelfAttention', 'k']\n",
            "Skipping decoder/block_005/layer_000/SelfAttention/k_slot_vc\n",
            "Skipping decoder/block_005/layer_000/SelfAttention/k_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_005', 'layer_000', 'SelfAttention', 'o']\n",
            "Initialize PyTorch weight ['decoder', 'block_005', 'layer_000', 'SelfAttention', 'o']\n",
            "Skipping decoder/block_005/layer_000/SelfAttention/o_slot_vc\n",
            "Skipping decoder/block_005/layer_000/SelfAttention/o_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_005', 'layer_000', 'SelfAttention', 'q']\n",
            "Initialize PyTorch weight ['decoder', 'block_005', 'layer_000', 'SelfAttention', 'q']\n",
            "Skipping decoder/block_005/layer_000/SelfAttention/q_slot_vc\n",
            "Skipping decoder/block_005/layer_000/SelfAttention/q_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_005', 'layer_000', 'SelfAttention', 'v']\n",
            "Initialize PyTorch weight ['decoder', 'block_005', 'layer_000', 'SelfAttention', 'v']\n",
            "Skipping decoder/block_005/layer_000/SelfAttention/v_slot_vc\n",
            "Skipping decoder/block_005/layer_000/SelfAttention/v_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['decoder', 'block_005', 'layer_000', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['decoder', 'block_005', 'layer_000', 'layer_norm', 'scale']\n",
            "Skipping decoder/block_005/layer_000/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_005', 'layer_001', 'EncDecAttention', 'k']\n",
            "Initialize PyTorch weight ['decoder', 'block_005', 'layer_001', 'EncDecAttention', 'k']\n",
            "Skipping decoder/block_005/layer_001/EncDecAttention/k_slot_vc\n",
            "Skipping decoder/block_005/layer_001/EncDecAttention/k_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_005', 'layer_001', 'EncDecAttention', 'o']\n",
            "Initialize PyTorch weight ['decoder', 'block_005', 'layer_001', 'EncDecAttention', 'o']\n",
            "Skipping decoder/block_005/layer_001/EncDecAttention/o_slot_vc\n",
            "Skipping decoder/block_005/layer_001/EncDecAttention/o_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_005', 'layer_001', 'EncDecAttention', 'q']\n",
            "Initialize PyTorch weight ['decoder', 'block_005', 'layer_001', 'EncDecAttention', 'q']\n",
            "Skipping decoder/block_005/layer_001/EncDecAttention/q_slot_vc\n",
            "Skipping decoder/block_005/layer_001/EncDecAttention/q_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_005', 'layer_001', 'EncDecAttention', 'v']\n",
            "Initialize PyTorch weight ['decoder', 'block_005', 'layer_001', 'EncDecAttention', 'v']\n",
            "Skipping decoder/block_005/layer_001/EncDecAttention/v_slot_vc\n",
            "Skipping decoder/block_005/layer_001/EncDecAttention/v_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['decoder', 'block_005', 'layer_001', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['decoder', 'block_005', 'layer_001', 'layer_norm', 'scale']\n",
            "Skipping decoder/block_005/layer_001/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 2048) for ['decoder', 'block_005', 'layer_002', 'DenseReluDense', 'wi', 'kernel']\n",
            "Initialize PyTorch weight ['decoder', 'block_005', 'layer_002', 'DenseReluDense', 'wi', 'kernel']\n",
            "Skipping decoder/block_005/layer_002/DenseReluDense/wi/kernel_slot_vc\n",
            "Skipping decoder/block_005/layer_002/DenseReluDense/wi/kernel_slot_vr\n",
            "Transposing numpy weight of shape (2048, 512) for ['decoder', 'block_005', 'layer_002', 'DenseReluDense', 'wo', 'kernel']\n",
            "Initialize PyTorch weight ['decoder', 'block_005', 'layer_002', 'DenseReluDense', 'wo', 'kernel']\n",
            "Skipping decoder/block_005/layer_002/DenseReluDense/wo/kernel_slot_vc\n",
            "Skipping decoder/block_005/layer_002/DenseReluDense/wo/kernel_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['decoder', 'block_005', 'layer_002', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['decoder', 'block_005', 'layer_002', 'layer_norm', 'scale']\n",
            "Skipping decoder/block_005/layer_002/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512,) for ['decoder', 'final_layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['decoder', 'final_layer_norm', 'scale']\n",
            "Skipping decoder/final_layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 512) for ['encoder', 'block_000', 'layer_000', 'SelfAttention', 'k']\n",
            "Initialize PyTorch weight ['encoder', 'block_000', 'layer_000', 'SelfAttention', 'k']\n",
            "Skipping encoder/block_000/layer_000/SelfAttention/k_slot_vc\n",
            "Skipping encoder/block_000/layer_000/SelfAttention/k_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['encoder', 'block_000', 'layer_000', 'SelfAttention', 'o']\n",
            "Initialize PyTorch weight ['encoder', 'block_000', 'layer_000', 'SelfAttention', 'o']\n",
            "Skipping encoder/block_000/layer_000/SelfAttention/o_slot_vc\n",
            "Skipping encoder/block_000/layer_000/SelfAttention/o_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['encoder', 'block_000', 'layer_000', 'SelfAttention', 'q']\n",
            "Initialize PyTorch weight ['encoder', 'block_000', 'layer_000', 'SelfAttention', 'q']\n",
            "Skipping encoder/block_000/layer_000/SelfAttention/q_slot_vc\n",
            "Skipping encoder/block_000/layer_000/SelfAttention/q_slot_vr\n",
            "Transposing numpy weight of shape (8, 32) for ['encoder', 'block_000', 'layer_000', 'SelfAttention', 'relative_attention_bias']\n",
            "Initialize PyTorch weight ['encoder', 'block_000', 'layer_000', 'SelfAttention', 'relative_attention_bias']\n",
            "Skipping encoder/block_000/layer_000/SelfAttention/relative_attention_bias_slot_v\n",
            "Transposing numpy weight of shape (512, 512) for ['encoder', 'block_000', 'layer_000', 'SelfAttention', 'v']\n",
            "Initialize PyTorch weight ['encoder', 'block_000', 'layer_000', 'SelfAttention', 'v']\n",
            "Skipping encoder/block_000/layer_000/SelfAttention/v_slot_vc\n",
            "Skipping encoder/block_000/layer_000/SelfAttention/v_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['encoder', 'block_000', 'layer_000', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['encoder', 'block_000', 'layer_000', 'layer_norm', 'scale']\n",
            "Skipping encoder/block_000/layer_000/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 2048) for ['encoder', 'block_000', 'layer_001', 'DenseReluDense', 'wi', 'kernel']\n",
            "Initialize PyTorch weight ['encoder', 'block_000', 'layer_001', 'DenseReluDense', 'wi', 'kernel']\n",
            "Skipping encoder/block_000/layer_001/DenseReluDense/wi/kernel_slot_vc\n",
            "Skipping encoder/block_000/layer_001/DenseReluDense/wi/kernel_slot_vr\n",
            "Transposing numpy weight of shape (2048, 512) for ['encoder', 'block_000', 'layer_001', 'DenseReluDense', 'wo', 'kernel']\n",
            "Initialize PyTorch weight ['encoder', 'block_000', 'layer_001', 'DenseReluDense', 'wo', 'kernel']\n",
            "Skipping encoder/block_000/layer_001/DenseReluDense/wo/kernel_slot_vc\n",
            "Skipping encoder/block_000/layer_001/DenseReluDense/wo/kernel_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['encoder', 'block_000', 'layer_001', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['encoder', 'block_000', 'layer_001', 'layer_norm', 'scale']\n",
            "Skipping encoder/block_000/layer_001/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 512) for ['encoder', 'block_001', 'layer_000', 'SelfAttention', 'k']\n",
            "Initialize PyTorch weight ['encoder', 'block_001', 'layer_000', 'SelfAttention', 'k']\n",
            "Skipping encoder/block_001/layer_000/SelfAttention/k_slot_vc\n",
            "Skipping encoder/block_001/layer_000/SelfAttention/k_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['encoder', 'block_001', 'layer_000', 'SelfAttention', 'o']\n",
            "Initialize PyTorch weight ['encoder', 'block_001', 'layer_000', 'SelfAttention', 'o']\n",
            "Skipping encoder/block_001/layer_000/SelfAttention/o_slot_vc\n",
            "Skipping encoder/block_001/layer_000/SelfAttention/o_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['encoder', 'block_001', 'layer_000', 'SelfAttention', 'q']\n",
            "Initialize PyTorch weight ['encoder', 'block_001', 'layer_000', 'SelfAttention', 'q']\n",
            "Skipping encoder/block_001/layer_000/SelfAttention/q_slot_vc\n",
            "Skipping encoder/block_001/layer_000/SelfAttention/q_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['encoder', 'block_001', 'layer_000', 'SelfAttention', 'v']\n",
            "Initialize PyTorch weight ['encoder', 'block_001', 'layer_000', 'SelfAttention', 'v']\n",
            "Skipping encoder/block_001/layer_000/SelfAttention/v_slot_vc\n",
            "Skipping encoder/block_001/layer_000/SelfAttention/v_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['encoder', 'block_001', 'layer_000', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['encoder', 'block_001', 'layer_000', 'layer_norm', 'scale']\n",
            "Skipping encoder/block_001/layer_000/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 2048) for ['encoder', 'block_001', 'layer_001', 'DenseReluDense', 'wi', 'kernel']\n",
            "Initialize PyTorch weight ['encoder', 'block_001', 'layer_001', 'DenseReluDense', 'wi', 'kernel']\n",
            "Skipping encoder/block_001/layer_001/DenseReluDense/wi/kernel_slot_vc\n",
            "Skipping encoder/block_001/layer_001/DenseReluDense/wi/kernel_slot_vr\n",
            "Transposing numpy weight of shape (2048, 512) for ['encoder', 'block_001', 'layer_001', 'DenseReluDense', 'wo', 'kernel']\n",
            "Initialize PyTorch weight ['encoder', 'block_001', 'layer_001', 'DenseReluDense', 'wo', 'kernel']\n",
            "Skipping encoder/block_001/layer_001/DenseReluDense/wo/kernel_slot_vc\n",
            "Skipping encoder/block_001/layer_001/DenseReluDense/wo/kernel_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['encoder', 'block_001', 'layer_001', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['encoder', 'block_001', 'layer_001', 'layer_norm', 'scale']\n",
            "Skipping encoder/block_001/layer_001/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 512) for ['encoder', 'block_002', 'layer_000', 'SelfAttention', 'k']\n",
            "Initialize PyTorch weight ['encoder', 'block_002', 'layer_000', 'SelfAttention', 'k']\n",
            "Skipping encoder/block_002/layer_000/SelfAttention/k_slot_vc\n",
            "Skipping encoder/block_002/layer_000/SelfAttention/k_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['encoder', 'block_002', 'layer_000', 'SelfAttention', 'o']\n",
            "Initialize PyTorch weight ['encoder', 'block_002', 'layer_000', 'SelfAttention', 'o']\n",
            "Skipping encoder/block_002/layer_000/SelfAttention/o_slot_vc\n",
            "Skipping encoder/block_002/layer_000/SelfAttention/o_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['encoder', 'block_002', 'layer_000', 'SelfAttention', 'q']\n",
            "Initialize PyTorch weight ['encoder', 'block_002', 'layer_000', 'SelfAttention', 'q']\n",
            "Skipping encoder/block_002/layer_000/SelfAttention/q_slot_vc\n",
            "Skipping encoder/block_002/layer_000/SelfAttention/q_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['encoder', 'block_002', 'layer_000', 'SelfAttention', 'v']\n",
            "Initialize PyTorch weight ['encoder', 'block_002', 'layer_000', 'SelfAttention', 'v']\n",
            "Skipping encoder/block_002/layer_000/SelfAttention/v_slot_vc\n",
            "Skipping encoder/block_002/layer_000/SelfAttention/v_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['encoder', 'block_002', 'layer_000', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['encoder', 'block_002', 'layer_000', 'layer_norm', 'scale']\n",
            "Skipping encoder/block_002/layer_000/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 2048) for ['encoder', 'block_002', 'layer_001', 'DenseReluDense', 'wi', 'kernel']\n",
            "Initialize PyTorch weight ['encoder', 'block_002', 'layer_001', 'DenseReluDense', 'wi', 'kernel']\n",
            "Skipping encoder/block_002/layer_001/DenseReluDense/wi/kernel_slot_vc\n",
            "Skipping encoder/block_002/layer_001/DenseReluDense/wi/kernel_slot_vr\n",
            "Transposing numpy weight of shape (2048, 512) for ['encoder', 'block_002', 'layer_001', 'DenseReluDense', 'wo', 'kernel']\n",
            "Initialize PyTorch weight ['encoder', 'block_002', 'layer_001', 'DenseReluDense', 'wo', 'kernel']\n",
            "Skipping encoder/block_002/layer_001/DenseReluDense/wo/kernel_slot_vc\n",
            "Skipping encoder/block_002/layer_001/DenseReluDense/wo/kernel_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['encoder', 'block_002', 'layer_001', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['encoder', 'block_002', 'layer_001', 'layer_norm', 'scale']\n",
            "Skipping encoder/block_002/layer_001/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 512) for ['encoder', 'block_003', 'layer_000', 'SelfAttention', 'k']\n",
            "Initialize PyTorch weight ['encoder', 'block_003', 'layer_000', 'SelfAttention', 'k']\n",
            "Skipping encoder/block_003/layer_000/SelfAttention/k_slot_vc\n",
            "Skipping encoder/block_003/layer_000/SelfAttention/k_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['encoder', 'block_003', 'layer_000', 'SelfAttention', 'o']\n",
            "Initialize PyTorch weight ['encoder', 'block_003', 'layer_000', 'SelfAttention', 'o']\n",
            "Skipping encoder/block_003/layer_000/SelfAttention/o_slot_vc\n",
            "Skipping encoder/block_003/layer_000/SelfAttention/o_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['encoder', 'block_003', 'layer_000', 'SelfAttention', 'q']\n",
            "Initialize PyTorch weight ['encoder', 'block_003', 'layer_000', 'SelfAttention', 'q']\n",
            "Skipping encoder/block_003/layer_000/SelfAttention/q_slot_vc\n",
            "Skipping encoder/block_003/layer_000/SelfAttention/q_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['encoder', 'block_003', 'layer_000', 'SelfAttention', 'v']\n",
            "Initialize PyTorch weight ['encoder', 'block_003', 'layer_000', 'SelfAttention', 'v']\n",
            "Skipping encoder/block_003/layer_000/SelfAttention/v_slot_vc\n",
            "Skipping encoder/block_003/layer_000/SelfAttention/v_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['encoder', 'block_003', 'layer_000', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['encoder', 'block_003', 'layer_000', 'layer_norm', 'scale']\n",
            "Skipping encoder/block_003/layer_000/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 2048) for ['encoder', 'block_003', 'layer_001', 'DenseReluDense', 'wi', 'kernel']\n",
            "Initialize PyTorch weight ['encoder', 'block_003', 'layer_001', 'DenseReluDense', 'wi', 'kernel']\n",
            "Skipping encoder/block_003/layer_001/DenseReluDense/wi/kernel_slot_vc\n",
            "Skipping encoder/block_003/layer_001/DenseReluDense/wi/kernel_slot_vr\n",
            "Transposing numpy weight of shape (2048, 512) for ['encoder', 'block_003', 'layer_001', 'DenseReluDense', 'wo', 'kernel']\n",
            "Initialize PyTorch weight ['encoder', 'block_003', 'layer_001', 'DenseReluDense', 'wo', 'kernel']\n",
            "Skipping encoder/block_003/layer_001/DenseReluDense/wo/kernel_slot_vc\n",
            "Skipping encoder/block_003/layer_001/DenseReluDense/wo/kernel_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['encoder', 'block_003', 'layer_001', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['encoder', 'block_003', 'layer_001', 'layer_norm', 'scale']\n",
            "Skipping encoder/block_003/layer_001/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 512) for ['encoder', 'block_004', 'layer_000', 'SelfAttention', 'k']\n",
            "Initialize PyTorch weight ['encoder', 'block_004', 'layer_000', 'SelfAttention', 'k']\n",
            "Skipping encoder/block_004/layer_000/SelfAttention/k_slot_vc\n",
            "Skipping encoder/block_004/layer_000/SelfAttention/k_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['encoder', 'block_004', 'layer_000', 'SelfAttention', 'o']\n",
            "Initialize PyTorch weight ['encoder', 'block_004', 'layer_000', 'SelfAttention', 'o']\n",
            "Skipping encoder/block_004/layer_000/SelfAttention/o_slot_vc\n",
            "Skipping encoder/block_004/layer_000/SelfAttention/o_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['encoder', 'block_004', 'layer_000', 'SelfAttention', 'q']\n",
            "Initialize PyTorch weight ['encoder', 'block_004', 'layer_000', 'SelfAttention', 'q']\n",
            "Skipping encoder/block_004/layer_000/SelfAttention/q_slot_vc\n",
            "Skipping encoder/block_004/layer_000/SelfAttention/q_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['encoder', 'block_004', 'layer_000', 'SelfAttention', 'v']\n",
            "Initialize PyTorch weight ['encoder', 'block_004', 'layer_000', 'SelfAttention', 'v']\n",
            "Skipping encoder/block_004/layer_000/SelfAttention/v_slot_vc\n",
            "Skipping encoder/block_004/layer_000/SelfAttention/v_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['encoder', 'block_004', 'layer_000', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['encoder', 'block_004', 'layer_000', 'layer_norm', 'scale']\n",
            "Skipping encoder/block_004/layer_000/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 2048) for ['encoder', 'block_004', 'layer_001', 'DenseReluDense', 'wi', 'kernel']\n",
            "Initialize PyTorch weight ['encoder', 'block_004', 'layer_001', 'DenseReluDense', 'wi', 'kernel']\n",
            "Skipping encoder/block_004/layer_001/DenseReluDense/wi/kernel_slot_vc\n",
            "Skipping encoder/block_004/layer_001/DenseReluDense/wi/kernel_slot_vr\n",
            "Transposing numpy weight of shape (2048, 512) for ['encoder', 'block_004', 'layer_001', 'DenseReluDense', 'wo', 'kernel']\n",
            "Initialize PyTorch weight ['encoder', 'block_004', 'layer_001', 'DenseReluDense', 'wo', 'kernel']\n",
            "Skipping encoder/block_004/layer_001/DenseReluDense/wo/kernel_slot_vc\n",
            "Skipping encoder/block_004/layer_001/DenseReluDense/wo/kernel_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['encoder', 'block_004', 'layer_001', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['encoder', 'block_004', 'layer_001', 'layer_norm', 'scale']\n",
            "Skipping encoder/block_004/layer_001/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 512) for ['encoder', 'block_005', 'layer_000', 'SelfAttention', 'k']\n",
            "Initialize PyTorch weight ['encoder', 'block_005', 'layer_000', 'SelfAttention', 'k']\n",
            "Skipping encoder/block_005/layer_000/SelfAttention/k_slot_vc\n",
            "Skipping encoder/block_005/layer_000/SelfAttention/k_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['encoder', 'block_005', 'layer_000', 'SelfAttention', 'o']\n",
            "Initialize PyTorch weight ['encoder', 'block_005', 'layer_000', 'SelfAttention', 'o']\n",
            "Skipping encoder/block_005/layer_000/SelfAttention/o_slot_vc\n",
            "Skipping encoder/block_005/layer_000/SelfAttention/o_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['encoder', 'block_005', 'layer_000', 'SelfAttention', 'q']\n",
            "Initialize PyTorch weight ['encoder', 'block_005', 'layer_000', 'SelfAttention', 'q']\n",
            "Skipping encoder/block_005/layer_000/SelfAttention/q_slot_vc\n",
            "Skipping encoder/block_005/layer_000/SelfAttention/q_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['encoder', 'block_005', 'layer_000', 'SelfAttention', 'v']\n",
            "Initialize PyTorch weight ['encoder', 'block_005', 'layer_000', 'SelfAttention', 'v']\n",
            "Skipping encoder/block_005/layer_000/SelfAttention/v_slot_vc\n",
            "Skipping encoder/block_005/layer_000/SelfAttention/v_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['encoder', 'block_005', 'layer_000', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['encoder', 'block_005', 'layer_000', 'layer_norm', 'scale']\n",
            "Skipping encoder/block_005/layer_000/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 2048) for ['encoder', 'block_005', 'layer_001', 'DenseReluDense', 'wi', 'kernel']\n",
            "Initialize PyTorch weight ['encoder', 'block_005', 'layer_001', 'DenseReluDense', 'wi', 'kernel']\n",
            "Skipping encoder/block_005/layer_001/DenseReluDense/wi/kernel_slot_vc\n",
            "Skipping encoder/block_005/layer_001/DenseReluDense/wi/kernel_slot_vr\n",
            "Transposing numpy weight of shape (2048, 512) for ['encoder', 'block_005', 'layer_001', 'DenseReluDense', 'wo', 'kernel']\n",
            "Initialize PyTorch weight ['encoder', 'block_005', 'layer_001', 'DenseReluDense', 'wo', 'kernel']\n",
            "Skipping encoder/block_005/layer_001/DenseReluDense/wo/kernel_slot_vc\n",
            "Skipping encoder/block_005/layer_001/DenseReluDense/wo/kernel_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['encoder', 'block_005', 'layer_001', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['encoder', 'block_005', 'layer_001', 'layer_norm', 'scale']\n",
            "Skipping encoder/block_005/layer_001/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512,) for ['encoder', 'final_layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['encoder', 'final_layer_norm', 'scale']\n",
            "Skipping encoder/final_layer_norm/scale_slot_v\n",
            "Skipping global_step\n",
            "Initialize PyTorch weight ['shared', 'embedding']\n",
            "Skipping shared/embedding_slot_vc\n",
            "Skipping shared/embedding_slot_vr\n",
            "Weights not copied to PyTorch model: \n",
            "Save PyTorch model to ./dumps\n",
            "Configuration saved in ./dumps/config.json\n",
            "Model weights saved in ./dumps/pytorch_model.bin\n",
            "code&comment2code: private void runLocked(final Runnable action) { try { Exception releaseException = null; final InterProcessLock lock = new InterProcessSemaphoreMutex(zooKeeperHolder.get(), ROOT_PATH + \"/lock\"); lock.acquire(); try { action.run(); } finally { try { lock.release(); } catch (final Exception ex) { LOG.warn(\"Failed to release lock\", ex); releaseException = ex; } } if (null != releaseException) { throw releaseException; } } catch (final RuntimeException ex) { throw ex; } catch (final Exception ex) { throw new RuntimeException(ex); } }\n",
            "Shouldn't this be give a higher severity? Presumably, if we can't release the lock, it's still there, and processes waiting for it will be stuck.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbBqAZvxqAcM"
      },
      "source": [
        "class EvalDataset(torch.utils.data.Dataset):\n",
        "  samples = []\n",
        "\n",
        "  def __init__(self,  data_dir_path):\n",
        "    data_dir = data_dir_path\n",
        "    datasets = ['test.source', 'test.target']\n",
        "    self.samples = []    \n",
        "\n",
        "    input_file = open(os.path.join(data_dir, 'test.source'), 'r')\n",
        "    output_file = open(os.path.join(data_dir, 'test.target'), 'r')\n",
        "\n",
        "    lines_input = input_file.readlines()\n",
        "    output_lines = output_file.readlines()\n",
        "\n",
        "    for (inp, out) in zip(lines_input, output_lines):\n",
        "      self.samples.append((inp.rstrip(), out.rstrip()))\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    return self.samples[index]\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.samples)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T5MKKN4MjnpL",
        "outputId": "1aa8baa4-d6e3-4b6f-f816-052f499478c7"
      },
      "source": [
        "beam_size = 2\n",
        "batch_size = 8\n",
        "data_dir = \"/content/data\"\n",
        "tokenizer_name = \"./current_model/TestModel.model\" \n",
        "model_name_or_path = \"./dumps/pytorch_model.bin\"\n",
        "config_name = \"./current_model/config.json\"\n",
        "\n",
        "dataset = EvalDataset(data_dir)\n",
        "dloader = DataLoader(dataset=dataset, batch_size=batch_size, num_workers=0)\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # My envirnment uses CPU\n",
        "\n",
        "t5_tokenizer = T5Tokenizer.from_pretrained(tokenizer_name)\n",
        "\n",
        "t5_config = T5Config.from_pretrained(config_name)\n",
        "t5_mlm = T5ForConditionalGeneration.from_pretrained(model_name_or_path, config=t5_config).to(DEVICE)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Calling T5Tokenizer.from_pretrained() with the path to a single file or url is deprecated\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sRBQbX1Mm8W6",
        "outputId": "014bd4f0-7213-43c3-99e0-8cd5ed4d7581"
      },
      "source": [
        "predictions = []\n",
        "code_bleues = []\n",
        "\n",
        "perfect = 0\n",
        "almost_perfect = 0\n",
        "\n",
        "# indexes for batches\n",
        "old = 0\n",
        "new = batch_size * beam_size\n",
        "\n",
        "for batch in tqdm(dloader): \n",
        "  encoded = t5_tokenizer.batch_encode_plus(batch[0], add_special_tokens=False, return_tensors='pt', padding=True)\n",
        "  \n",
        "  input_ids = encoded['input_ids'].to(DEVICE)\n",
        "  attention_mask = encoded['attention_mask'].to(DEVICE)\n",
        "\n",
        "  outputs = t5_mlm.generate(\n",
        "      input_ids=input_ids,\n",
        "      max_length=512, #Change here \n",
        "      num_beams=beam_size,\n",
        "      attention_mask=attention_mask,\n",
        "      early_stopping=True,\n",
        "      num_return_sequences=beam_size).to(DEVICE)\n",
        "\n",
        "  predictions.extend(t5_tokenizer.batch_decode(outputs, skip_special_tokens=True))\n",
        "\n",
        "  to_analyze = predictions[old:new]\n",
        "  target_list = batch[1]\n",
        "  input_list = batch[0]\n",
        "\n",
        "  idx = 0\n",
        "  for (input_item, target_item) in zip(input_list,target_list):\n",
        "\n",
        "    flag_perfect = False\n",
        "    flag_almost_perfect = False\n",
        "\n",
        "    target_item = \" \".join(target_item.split(' '))\n",
        "    best_code_bleu = 0\n",
        "    \n",
        "    for i in range(beam_size):\n",
        "      prediction_item = \" \".join(to_analyze[idx].split(' '))\n",
        "\n",
        "      if not flag_perfect and prediction_item == target_item:\n",
        "        perfect += 1\n",
        "        flag_perfect = True\n",
        "\n",
        "      if not flag_almost_perfect and \"\".join(prediction_item.split(' ')) == \"\".join(target_item.split(' ')):\n",
        "        almost_perfect += 1\n",
        "        flag_almost_perfect = True\n",
        "\n",
        "      idx += 1\n",
        "\n",
        "      with open(\"code_bleu_target.txt\", \"w\") as target_cb:\n",
        "        target_cb.write(target_item + \"\\n\")\n",
        "      with open(\"code_bleu_prediction.txt\", \"w\") as prediction_cb:\n",
        "        prediction_cb.write(prediction_item + \"\\n\")\n",
        "\n",
        "      try:\n",
        "        result = !cd /content/CodeXGLUE/Code-Code/code-to-code-trans/evaluator/CodeBLEU/ && python calc_code_bleu.py --refs ./../../../../../code_bleu_target.txt --hyp ./../../../../../code_bleu_prediction.txt --lang java --params 0.25,0.25,0.25,0.25\n",
        "        code_bleu = float(result[1][result[1].index(\"CodeBLEU score:  \")+17:])\n",
        "        \n",
        "      except: \n",
        "        result = !cd /content/CodeXGLUE/Code-Code/code-to-code-trans/evaluator/CodeBLEU/ && python calc_code_bleu.py --refs ./../../../../../code_bleu_target.txt --hyp ./../../../../../code_bleu_prediction.txt --lang java --params 0.333,0.333,0.333,0\n",
        "        code_bleu = float(result[2][result[2].index(\"CodeBLEU score:  \")+17:])\n",
        "\n",
        "      best_code_bleu = code_bleu if code_bleu > best_code_bleu else best_code_bleu\n",
        "        \n",
        "    code_bleues.append(best_code_bleu)\n",
        "\n",
        "  old = new\n",
        "  new = new + (batch_size * beam_size)\n",
        "\n",
        "perfect_percentage = (perfect/len(dataset))*100\n",
        "almost_perfect_percentage = (almost_perfect/len(dataset))*100\n",
        "mean_cb = statistics.mean(code_bleues)\n",
        "median_cb = statistics.median(code_bleues)\n",
        "stdev_cb = statistics.stdev(code_bleues)\n",
        "\n",
        "print()\n",
        "print('#perfect prediction: ', perfect)\n",
        "print('Perfect prediction {}%'.format(round(perfect_percentage, 2)))\n",
        "\n",
        "print('#almost perfect prediction: ', almost_perfect)\n",
        "print('Almost Perfect prediction {}%'.format(round(almost_perfect_percentage, 2)))\n",
        "\n",
        "print('Mean CodeBLEU :', round(mean_cb, 2))\n",
        "print('Median CodeBLEU :', round(median_cb, 2))\n",
        "print('stdev CodeBLEU :', round(stdev_cb, 2))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [01:01<00:00,  6.14s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "#perfect prediction:  50\n",
            "Perfect prediction 62.5%\n",
            "#almost perfect prediction:  56\n",
            "Almost Perfect prediction 70.0%\n",
            "Mean Bleu : 0.71\n",
            "Median Bleu : 0.98\n",
            "stdev Bleu : 0.36\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pYOv-1jUXJny",
        "outputId": "bfd5a1bd-4c31-424b-c412-eeba22a952e8"
      },
      "source": [
        "code = ' public static void main(String[] args) { System.out.println(\" hello world\") }'#@param {type:\"string\"}\n",
        "markedCode = ' public static void main(String[] args) { <START> System.out.println(\"hello world\") <END>}'#@param {type:\"string\"}\n",
        "comment = \"print using a logger\" #@param {type:\"string\"}\n",
        "task = \"code&comment2code: \" #@param [\"code2comment: \", \"code&comment2code: \", \"markedCode2code: \", \"code2code: \"]\n",
        "\n",
        "if task == \"code&comment2code: \":\n",
        "  input = task + \" <code>\" + code + \"</code><technical_language>\" + comment + \"</technical_language>\"\n",
        "elif task == \"markedCode2code: \":\n",
        "  input = task + markedCode\n",
        "else:\n",
        "  input = task + code\n",
        "\n",
        "print(input)\n",
        "encoded = t5_tokenizer.encode(input, add_special_tokens=False, return_tensors='pt', padding=True).to(DEVICE)\n",
        "\n",
        "input_ids = encoded\n",
        "outputs = t5_mlm.generate(\n",
        "      input_ids=input_ids,\n",
        "      max_length=512, #Change here \n",
        "      num_beams=1,\n",
        "      early_stopping=True,\n",
        "      num_return_sequences=1).to(DEVICE)\n",
        "\n",
        "print(code)\n",
        "print(t5_tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "code&comment2code:  <code> public static void main(String[] args) { System.out.println(\" hello world\") }</code><technical_language>print using a logger</technical_language>\n",
            " public static void main(String[] args) { System.out.println(\" hello world\") }\n",
            "public static void main(String[] args) { logger. hello world(); }\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}