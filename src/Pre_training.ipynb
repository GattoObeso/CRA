{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "Pre-training.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/masies/CRA/blob/main/Pre_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kSChpBmA53GI",
        "outputId": "cc5e9c27-33d1-484f-d634-e3fdbe4e925e"
      },
      "source": [
        "!pip3 install tensorflow\n",
        "%tensorflow_version 2.x\n",
        "!pip3 install --upgrade pip\n",
        "#!pip install -qU t5\n",
        "!pip3 install git+https://github.com/google-research/text-to-text-transfer-transformer.git #extra_id_x support\n",
        "\n",
        "import functools\n",
        "import os\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.enable_eager_execution()\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "import t5\n",
        "\n",
        "#Set the base dir(Google cloud bucket)\n",
        "BASE_DIR = \"gs://code_review_automation\" \n",
        "\n",
        "if not BASE_DIR or BASE_DIR == \"gs://\":\n",
        "  raise ValueError(\"You must enter a BASE_DIR.\")\n",
        "ON_CLOUD = True\n",
        "\n",
        "\n",
        "if ON_CLOUD:\n",
        "  import tensorflow_gcs_config\n",
        "  from google.colab import auth\n",
        "  # Set credentials for GCS reading/writing from Colab and TPU.\n",
        "  TPU_TOPOLOGY = \"2x2\"\n",
        "  try:\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
        "    TPU_ADDRESS = tpu.get_master()\n",
        "    print('Running on TPU:', TPU_ADDRESS)\n",
        "  except ValueError:\n",
        "    raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n",
        "  auth.authenticate_user()\n",
        "  tf.config.experimental_connect_to_host(TPU_ADDRESS)\n",
        "  tensorflow_gcs_config.configure_gcs_from_colab_auth()\n",
        "\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "# Improve logging.\n",
        "from contextlib import contextmanager\n",
        "import logging as py_logging\n",
        "\n",
        "if ON_CLOUD:\n",
        "  tf.get_logger().propagate = False\n",
        "  py_logging.root.setLevel('INFO')\n",
        "\n",
        "@contextmanager\n",
        "def tf_verbosity_level(level):\n",
        "  og_level = tf.logging.get_verbosity()\n",
        "  tf.logging.set_verbosity(level)\n",
        "  yield\n",
        "  tf.logging.set_verbosity(og_level)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (2.4.1)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.12.4)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.12.1)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.7.4.3)\n",
            "Requirement already satisfied: grpcio~=1.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.32.0)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.19.5)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.12)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.36.2)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.3.3)\n",
            "Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.4.1)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py~=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.10.0)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.10.0)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.9.2->tensorflow) (54.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow) (3.3.4)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow) (2.23.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow) (0.4.3)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow) (1.27.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow) (1.8.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow) (3.7.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2020.12.5)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow) (1.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (4.2.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (4.7.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.4->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow) (3.1.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (0.4.8)\n",
            "Collecting pip\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fe/ef/60d7ba03b5c442309ef42e7d69959f73aacccd0d86008362a681c4698e83/pip-21.0.1-py3-none-any.whl (1.5MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5MB 6.6MB/s \n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Found existing installation: pip 19.3.1\n",
            "    Uninstalling pip-19.3.1:\n",
            "      Successfully uninstalled pip-19.3.1\n",
            "Successfully installed pip-21.0.1\n",
            "Collecting git+https://github.com/google-research/text-to-text-transfer-transformer.git\n",
            "  Cloning https://github.com/google-research/text-to-text-transfer-transformer.git to /tmp/pip-req-build-sk7hj9mi\n",
            "  Running command git clone -q https://github.com/google-research/text-to-text-transfer-transformer.git /tmp/pip-req-build-sk7hj9mi\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from t5==0.9.0) (0.10.0)\n",
            "Requirement already satisfied: babel in /usr/local/lib/python3.7/dist-packages (from t5==0.9.0) (2.9.0)\n",
            "Requirement already satisfied: gin-config in /usr/local/lib/python3.7/dist-packages (from t5==0.9.0) (0.4.0)\n",
            "Collecting mesh-tensorflow[transformer]>=0.1.13\n",
            "  Downloading mesh_tensorflow-0.1.18-py3-none-any.whl (361 kB)\n",
            "\u001b[K     |████████████████████████████████| 361 kB 6.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from t5==0.9.0) (3.2.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from t5==0.9.0) (1.19.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from t5==0.9.0) (1.1.5)\n",
            "Collecting rouge-score\n",
            "  Downloading rouge_score-0.0.4-py2.py3-none-any.whl (22 kB)\n",
            "Collecting sacrebleu\n",
            "  Downloading sacrebleu-1.5.1-py3-none-any.whl (54 kB)\n",
            "\u001b[K     |████████████████████████████████| 54 kB 2.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from t5==0.9.0) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from t5==0.9.0) (1.4.1)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 10.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.14 in /usr/local/lib/python3.7/dist-packages (from t5==0.9.0) (1.15.0)\n",
            "Collecting tensorflow-text\n",
            "  Downloading tensorflow_text-2.4.3-cp37-cp37m-manylinux1_x86_64.whl (3.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.4 MB 23.5 MB/s \n",
            "\u001b[?25hCollecting tfds-nightly\n",
            "  Downloading tfds_nightly-4.2.0.dev202103170106-py3-none-any.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 59.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from t5==0.9.0) (1.8.0+cu101)\n",
            "Collecting transformers>=2.7.0\n",
            "  Downloading transformers-4.4.1-py3-none-any.whl (2.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1 MB 64.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from mesh-tensorflow[transformer]>=0.1.13->t5==0.9.0) (0.16.0)\n",
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.7/dist-packages (from mesh-tensorflow[transformer]>=0.1.13->t5==0.9.0) (4.0.1)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.43.tar.gz (883 kB)\n",
            "\u001b[K     |████████████████████████████████| 883 kB 82.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers>=2.7.0->t5==0.9.0) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers>=2.7.0->t5==0.9.0) (2019.12.20)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.1-cp37-cp37m-manylinux2010_x86_64.whl (3.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2 MB 64.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers>=2.7.0->t5==0.9.0) (3.7.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers>=2.7.0->t5==0.9.0) (4.41.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers>=2.7.0->t5==0.9.0) (20.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers>=2.7.0->t5==0.9.0) (3.0.12)\n",
            "Requirement already satisfied: pytz>=2015.7 in /usr/local/lib/python3.7/dist-packages (from babel->t5==0.9.0) (2018.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers>=2.7.0->t5==0.9.0) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers>=2.7.0->t5==0.9.0) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers>=2.7.0->t5==0.9.0) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->t5==0.9.0) (2.8.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>=2.7.0->t5==0.9.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>=2.7.0->t5==0.9.0) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>=2.7.0->t5==0.9.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>=2.7.0->t5==0.9.0) (2.10)\n",
            "Collecting portalocker==2.0.0\n",
            "  Downloading portalocker-2.0.0-py2.py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=2.7.0->t5==0.9.0) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=2.7.0->t5==0.9.0) (1.0.1)\n",
            "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->mesh-tensorflow[transformer]>=0.1.13->t5==0.9.0) (20.3.0)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->mesh-tensorflow[transformer]>=0.1.13->t5==0.9.0) (0.28.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->mesh-tensorflow[transformer]>=0.1.13->t5==0.9.0) (1.1.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->mesh-tensorflow[transformer]>=0.1.13->t5==0.9.0) (5.1.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->mesh-tensorflow[transformer]>=0.1.13->t5==0.9.0) (0.3.3)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->mesh-tensorflow[transformer]>=0.1.13->t5==0.9.0) (0.1.5)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->mesh-tensorflow[transformer]>=0.1.13->t5==0.9.0) (3.12.4)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->mesh-tensorflow[transformer]>=0.1.13->t5==0.9.0) (2.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.6.1->tensorflow-datasets->mesh-tensorflow[transformer]>=0.1.13->t5==0.9.0) (54.0.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-metadata->tensorflow-datasets->mesh-tensorflow[transformer]>=0.1.13->t5==0.9.0) (1.53.0)\n",
            "Requirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-text->t5==0.9.0) (0.11.0)\n",
            "Requirement already satisfied: tensorflow<2.5,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-text->t5==0.9.0) (2.4.1)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text->t5==0.9.0) (0.2.0)\n",
            "Requirement already satisfied: h5py~=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text->t5==0.9.0) (2.10.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text->t5==0.9.0) (2.4.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text->t5==0.9.0) (1.1.2)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text->t5==0.9.0) (0.3.3)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text->t5==0.9.0) (1.12)\n",
            "Requirement already satisfied: grpcio~=1.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text->t5==0.9.0) (1.32.0)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text->t5==0.9.0) (1.12.1)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text->t5==0.9.0) (3.3.0)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text->t5==0.9.0) (0.36.2)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text->t5==0.9.0) (1.6.3)\n",
            "Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text->t5==0.9.0) (2.4.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text->t5==0.9.0) (1.27.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text->t5==0.9.0) (1.8.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text->t5==0.9.0) (3.3.4)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text->t5==0.9.0) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text->t5==0.9.0) (0.4.3)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text->t5==0.9.0) (4.2.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text->t5==0.9.0) (4.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text->t5==0.9.0) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text->t5==0.9.0) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text->t5==0.9.0) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text->t5==0.9.0) (3.1.0)\n",
            "Building wheels for collected packages: t5, sacremoses\n",
            "  Building wheel for t5 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for t5: filename=t5-0.9.0-py3-none-any.whl size=235483 sha256=516be2d4e8b00cfe8c4b3a618d2f0d9de66f28c0e234305bd33cf4ba6df5daee\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-w5ns62e2/wheels/bf/e8/36/0d7b7d2aa6a91236ea133d24d501fb1faf73d6bf2579f05c96\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-py3-none-any.whl size=893258 sha256=ef56f3fdd4bf66f39051966802ddb514d39b1b68b426c2e856812282dbf5c906\n",
            "  Stored in directory: /root/.cache/pip/wheels/69/09/d1/bf058f7d6fa0ecba2ce7c66be3b8d012beb4bf61a6e0c101c0\n",
            "Successfully built t5 sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, portalocker, mesh-tensorflow, transformers, tfds-nightly, tensorflow-text, sentencepiece, sacrebleu, rouge-score, t5\n",
            "Successfully installed mesh-tensorflow-0.1.18 portalocker-2.0.0 rouge-score-0.0.4 sacrebleu-1.5.1 sacremoses-0.0.43 sentencepiece-0.1.95 t5-0.9.0 tensorflow-text-2.4.3 tfds-nightly-4.2.0.dev202103170106 tokenizers-0.10.1 transformers-4.4.1\n",
            "Running on TPU: grpc://10.65.126.74:8470\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "id": "p4UHw7Yo6GCK"
      },
      "source": [
        "nq_tsv_path = {\n",
        "    \"train\":'gs://code_review_automation/raw_data/pretraining/processed/pre-training.tsv'\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5DJVOe896Lw"
      },
      "source": [
        "from t5.data import postprocessors as t5_postprocessors\n",
        "from t5.seqio import Feature,SentencePieceVocabulary\n",
        "\n",
        "# # Set the path of sentencepiece model and vocab files\n",
        "# # Must be the same used for the pre-trained phase\n",
        "vocab_model_path = 'gs://code_review_automation/CodeReviewModel/TestModel.model'\n",
        "vocab_path = 'gs://code_review_automation/CodeReviewModel/TestModel.vocab'\n",
        "\n",
        "TaskRegistry = t5.data.TaskRegistry\n",
        "TfdsTask = t5.data.TfdsTask\n",
        "\n",
        "def get_default_vocabulary():\n",
        "  return SentencePieceVocabulary(vocab_model_path, 100)\n",
        "\n",
        "DEFAULT_OUTPUT_FEATURES = {\n",
        "    \"inputs\": Feature(\n",
        "        vocabulary=get_default_vocabulary(), add_eos=False, required=True),\n",
        "\n",
        "    \"targets\": Feature(\n",
        "        vocabulary=get_default_vocabulary(), add_eos=False)\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ncBkh1fH7yh0",
        "outputId": "18801089-cd34-4401-b1ed-96b94e20ac84"
      },
      "source": [
        "def nq_dataset_fn(split, shuffle_files=True):\n",
        "  # We only have one file for each split.\n",
        "  del shuffle_files\n",
        "\n",
        "  # Load lines from the text file as examples.\n",
        "  ds = tf.data.TextLineDataset(nq_tsv_path[split])\n",
        "  ds = ds.map(\n",
        "      functools.partial(tf.io.decode_csv, record_defaults=[\"string\",\"string\"],\n",
        "                        field_delim=\"\\t\", use_quote_delim=False),\n",
        "      num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "  ds = ds.map(lambda *ex: dict(zip([\"input\", \"output\"], ex)))\n",
        "  return ds\n",
        "\n",
        "\n",
        "# print(\"A few raw train examples...\")\n",
        "for ex in tfds.as_numpy(nq_dataset_fn(\"train\").take(5)):\n",
        "  print(ex)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'input': b'<extra_id_0> <technical_language> Bind indexed elements to the<extra_id_1> collection. @param name the<extra_id_2> of the<extra_id_3> to bind<extra_id_4> param target the target bindable @param elementBinder the binder<extra_id_5> use for elements @param<extra_id_6> Type the aggregate type, may be a collection<extra_id_7> an array @param elementType<extra_id_8> type @param result<extra_id_9> destination for results <extra_id_10> <code> protected final void bindIndexed(ConfigurationPropertyName name, Bindable<?> target, AggregateElementBinder<extra_id_11> Binder, ResolvableType aggregateType, ResolvableType elementType, IndexedCollectionSupplier result<extra_id_12> {<extra_id_13> (ConfigurationPropertySource<extra_id_14> : getContext().getSources())<extra_id_15> bindIndexed(source, name, target, elementBinder, result<extra_id_16> aggregate<extra_id_17> , elementType); if (result.wasSuppl<extra_id_18> ()<extra_id_19> result.<extra_id_20> ()<extra_id_21> null) { return;<extra_id_22> } } </code></s>', 'output': b'<extra_id_0> supplied<extra_id_1> name<extra_id_2> property<extra_id_3> @<extra_id_4> to<extra_id_5> aggregate<extra_id_6> or<extra_id_7> the element<extra_id_8> the<extra_id_9> </technical_language><extra_id_10> element<extra_id_11> )<extra_id_12> for<extra_id_13> source<extra_id_14> {<extra_id_15> ,<extra_id_16> Type<extra_id_17> ied<extra_id_18> &&<extra_id_19> get<extra_id_20> !=<extra_id_21> }<extra_id_22>'}\n",
            "{'input': b'<technical_language> Set {@link ServletRegistrationBean}<extra_id_0> that the filter will be registered against. @param servletRegistrationBeans<extra_id_1> Servlet registration<extra_id_2> </technical_language><code> public<extra_id_3> setServlet<extra_id_4> s( Collection<?<extra_id_5> ServletRegistrationBean<?>><extra_id_6> RegistrationBeans) { Assert.notNull<extra_id_7> servletRegistrationBeans, \"ServletRegistrationBeans<extra_id_8> be null\"); this.servletRegistrationBeans =<extra_id_9> LinkedHashSet<extra_id_10> servletRegistrationBeans); } </code></s>', 'output': b'<extra_id_0> s<extra_id_1> the<extra_id_2> beans<extra_id_3> void<extra_id_4> RegistrationBean<extra_id_5> extends<extra_id_6> servlet<extra_id_7> (<extra_id_8> must not<extra_id_9> new<extra_id_10> <>(<extra_id_11>'}\n",
            "{'input': b'<technical_language> Add<extra_id_0> link ServletRegistrationBean}s for the filter. @param<extra_id_1> RegistrationBeans the<extra_id_2> registration beans to<extra_id_3> @see #<extra_id_4> ServletRegistrationBeans </technical_language><code> public void<extra_id_5> Servlet<extra_id_6> s( ServletRegistrationBean<?>... servletRegistrationBeans) { Assert.notNull(servletRegistrationBeans<extra_id_7> \"ServletRegistrationBeans must not<extra_id_8> null\");<extra_id_9> .addAll(this.servletRegistrationBean<extra_id_10> , servletRegistrationBeans<extra_id_11> } </code></s>', 'output': b'<extra_id_0> {@<extra_id_1> servlet<extra_id_2> servlet<extra_id_3> add<extra_id_4> set<extra_id_5> add<extra_id_6> RegistrationBean<extra_id_7> ,<extra_id_8> be<extra_id_9> Collections<extra_id_10> s<extra_id_11> );<extra_id_12>'}\n",
            "{'input': b'<extra_id_0> Set servlet names that the filter will be registered<extra_id_1> .<extra_id_2> will replace any previously specified servlet names. @param servletNames the servlet names @<extra_id_3> #setServlet<extra_id_4> @see<extra_id_5> setUrlPatterns </technical_language><extra_id_6> void set<extra_id_7> (Collection<String> servletNames<extra_id_8> { Assert.notNull(servletNames, \"ServletNames must not be null<extra_id_9> this.servletNames = new LinkedHashSet<>(servletNames); } </code></s>', 'output': b'<extra_id_0> <technical_language><extra_id_1> against<extra_id_2> This<extra_id_3> see<extra_id_4> RegistrationBeans<extra_id_5> #<extra_id_6> <code> public<extra_id_7> ServletNames<extra_id_8> )<extra_id_9> \");<extra_id_10>'}\n",
            "{'input': b'<technical_language> Add servlet names for the<extra_id_0> . @param servletNames the<extra_id_1> names to<extra_id_2> </technical_language><code> public<extra_id_3> ServletNames(String... servletNames)<extra_id_4> notNull(servletNames, \"ServletNames must not be null\"); this.servletNames.addAll(Arrays.asList(servletNames<extra_id_5> } </code></s>', 'output': b'<extra_id_0> filter<extra_id_1> servlet<extra_id_2> add<extra_id_3> void add<extra_id_4> { Assert.<extra_id_5> ));<extra_id_6>'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z998BqAT42eL"
      },
      "source": [
        "def preprocessing(ds):\n",
        "  def to_inputs_and_targets(ex):\n",
        "        inputs = tf.strings.join([ ex['input']], separator=' ')\n",
        "        class_label = tf.strings.join([ex['output']], separator=' ')\n",
        "        return {'inputs': inputs, 'targets': class_label }\n",
        "  return ds.map(to_inputs_and_targets, num_parallel_calls=tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pq_ljAs373oK",
        "outputId": "1dcb2fa8-a889-4234-8198-9260aaff2adc"
      },
      "source": [
        "#Create a new training task\n",
        "t5.data.TaskRegistry.remove('pretraining')\n",
        "t5.data.TaskRegistry.add(\n",
        "    \"pretraining\",\n",
        "    t5.data.Task,\n",
        "    dataset_fn=nq_dataset_fn,\n",
        "    splits=[\"train\", \"validation\"],\n",
        "    text_preprocessor=[preprocessing],\n",
        "    output_features = DEFAULT_OUTPUT_FEATURES,\n",
        "    metric_fns=[t5.evaluation.metrics.accuracy],\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<t5.data.dataset_providers.FunctionTask at 0x7fc4fa3e2850>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XFAAvLfG7528",
        "outputId": "220b8888-86f7-46b1-a0bb-b02255e35dca"
      },
      "source": [
        "nq_task = t5.data.TaskRegistry.get(\"pretraining\")\n",
        "ds = nq_task.get_dataset(split=\"train\", sequence_length={\"inputs\": 512, \"targets\": 512})\n",
        "print(\"A few preprocessed training examples...\")\n",
        "for ex in tfds.as_numpy(ds.take(5)):\n",
        "  print(ex)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/t5/seqio/preprocessors.py:65: UserWarning: Creating resources inside a function passed to Dataset.map() is not supported. Create each resource outside the function, and capture it inside the function to use it.\n",
            "  _tokenize, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "A few preprocessed training examples...\n",
            "{'inputs_pretokenized': b'<technical_language> Returns every possible list<extra_id_0> can be<extra_id_1> by choosing one element from each<extra_id_2> the given sets<extra_id_3> order; the<extra_id_4> n-ary Cartesian product<extra_id_5> of the<extra_id_6> . For example: {@<extra_id_7> Sets.<extra_id_8> esianProduct(<extra_id_9> of(1, 2), ImmutableSet.of<extra_id_10> A\",<extra_id_11> \", \"C\")) } returns a set containing six lists:<extra_id_12> code ImmutableList.of(1, \"A\")} {@code ImmutableList.of(1, \"B\")}<extra_id_13> ImmutableList.of(1, \"C<extra_id_14> }<extra_id_15> .of(2, \"A<extra_id_16> }<extra_id_17> code ImmutableList.of(2,<extra_id_18> B\")} {@<extra_id_19> ImmutableList.of(2, \"C<extra_id_20> } The result is guaranteed to be in the<extra_id_21> traditional\", lexicographical order for Cartesian products<extra_id_22> you would get from nesting<extra_id_23> loops: {@code for (<extra_id_24> b0 : sets.<extra_id_25> (0)) { for (B b1<extra_id_26> : sets.get(1)) { ... ImmutableList<B> tuple = ImmutableList.of<extra_id_27> b0, b1, ...<extra_id_28> // operate on tuple } } } Note that<extra_id_29> any input set is empty,<extra_id_30> Cartesian product will also<extra_id_31> empty. If no sets at all<extra_id_32> provided (an empty list), the resulting Cartesian product has one element, an<extra_id_33> list (counter-intuitive,<extra_id_34> mathematically consistent).<extra_id_35> notes: while the cartesian<extra_id_36> of sets<extra_id_37> size {@code<extra_id_38> , n, p<extra_id_39> is a set of size {@code m x n x p}, its actual<extra_id_40> consumption is much smaller. When the cartesian set is constructed, the<extra_id_41> are merely copied. Only as the resulting set is iterated<extra_id_42> lists created, and these<extra_id_43> not retained after iteration.<extra_id_44> param sets the sets<extra_id_45> choose elements from, in the order that the<extra_id_46> chosen from those sets should appear in the resulting lists @param <B> any common base class<extra_id_47> by all axes<extra_id_48> just {@link<extra_id_49> }) @<extra_id_50> the Cartesian product, as an immutable set containing immutable lists @throws NullPointerException if {@code sets}, any one of the {@<extra_id_51> sets<extra_id_52> or any element of a provided set<extra_id_53> null<extra_id_54> 2.0<extra_id_55> </technical_language><code> @SafeVarargs public static <B> Set<<extra_id_56> <B>> cartesian<extra_id_57> (Set<? extends B>... sets) { return<extra_id_58> Product(Arrays.<extra_id_59> (sets)); } </code></s>', 'inputs': array([    7,     5,   480,   678,   501,   141,    40,  8797,    25,\n",
            "         163,   248,    52,    45,    40,  8797,    25,   163,   985,\n",
            "          84, 11218,   128,   316,    76,   267,    40,  8797,    25,\n",
            "         163,  2697,    10,   226,  2504,    40,  8797,    25,   163,\n",
            "        5638,   398,    24,    10,    40,  8797,    25,   163,  9864,\n",
            "         453,    35,  3797,     7, 22679,  1868,    40,  8797,    25,\n",
            "         163, 16209,    26,    10,    40,  8797,    25,   163, 23559,\n",
            "           7,     8,   255,   194,    20,   106,    40,  8797,    25,\n",
            "         163, 28020,  1928,     8,    40,  8797,    25,   163,  9981,\n",
            "          32,     7,   864,  4216,  2210,     9,    40,  8797,    25,\n",
            "         163,    25,  1313,    32,    26,  1786,  1633,    11, 11810,\n",
            "           8,   676,    40,  8797,    25,   163,    25,  1532,    32,\n",
            "         191,    86,    40,  8797,    25,   163,    25,  2031,    32,\n",
            "           7,    86,    41,   371,   727,    13,   519,    17,   127,\n",
            "        1104, 12506,  2846,    20,    40,  8797,    25,   163,    25,\n",
            "        1572,    32,    89,  6431,     8,   676,  1786,    41,   239,\n",
            "         223,    96,   106,   220,  6431,     8,   676,  1786,    41,\n",
            "         399,   223,    96,    40,  8797,    25,   163,    25,  2900,\n",
            "          32,  6431,     8,   676,  1786,    41,   371,    40,  8797,\n",
            "          25,   163,    25,  2909,    32,    13,    40,  8797,    25,\n",
            "         163,    25,  3060,    32,     7,     8,   676,  3317,    41,\n",
            "         239,    40,  8797,    25,   163,    25,  1932,    32,    13,\n",
            "          40,  8797,    25,   163,    25,  3305,    32,    89,  6431,\n",
            "           8,   676,  3317,    40,  8797,    25,   163,    25,  2469,\n",
            "          32,   628,   223,    96,   106,    40,  8797,    25,   163,\n",
            "          25,  3508,    32,  6431,     8,   676,  3317,    41,   371,\n",
            "          40,  8797,    25,   163,    25,  2301,    32,    13,    59,\n",
            "         149,    21,  3652,    14,    45,    31,    10,    40,  8797,\n",
            "          25,   163,    25,  3202,    32, 11528,    86,     7, 27720,\n",
            "         398,    34,     7, 22679,  6511,    40,  8797,    25,   163,\n",
            "          25,  3423,    32,    22,   140,    99,    76, 12981,    40,\n",
            "        8797,    25,   163,    25,  3255,    32,  3832,    20,   106,\n",
            "         220,    34,    23,    40,  8797,    25,   163,    25,  3038,\n",
            "          32,   311,   539,     7,    20,  2504,     8,    40,  8797,\n",
            "          25,   163,    25,  2968,    32,     7,  8040,    15,    34,\n",
            "          23,   399,   311,  3447,  8797,    25,   163,    25,  4207,\n",
            "          32,     7,    20,  2504,     8,    62,  9316,    15,   318,\n",
            "        6431,    40,   399,    32,  4809,    16,  6431,     8,   676,\n",
            "          40,  8797,    25,   163,    25,  3964,    32,   311,  2757,\n",
            "         311,   529,   318,    40,  8797,    25,   163,    25,  4649,\n",
            "          32,  2851,  6133,    70,  4809,    13,    13,    13,   545,\n",
            "          38,    40,  8797,    25,   163,    25,  4340,    32,   174,\n",
            "         250,   127,    21,   662,    11,    40,  8797,    25,   163,\n",
            "          25,  2954,    32,     7, 22679,  1868,    69,   213,    40,\n",
            "        8797,    25,   163,    25,  3622,    32,   662,     8,    88,\n",
            "         196,  2504,   130,   114,    40,  8797,    25,   163,    25,\n",
            "        1472,    32,   845,    23,  2478,   662,   141,   334,    10,\n",
            "        2422,     7, 22679,  1868,   160,   128,   316,    11,    65,\n",
            "          40,  8797,    25,   163,    25,  4099,    32,   141,    23,\n",
            "        3380,    35, 23435,    11,    40,  8797,    25,   163,    25,\n",
            "        4234,    32, 30550,  4753,   147,    40,  8797,    25,   163,\n",
            "          25,  5217,    32,  6616,    20,   219,    10, 28691,    40,\n",
            "        8797,    25,   163,    25,  5905,    32,    26,  2504,    40,\n",
            "        8797,    25,   163,    25,  5903,    32,   393,   106,   220,\n",
            "          40,  8797,    25,   163,    25,  5957,    32,     7,    11,\n",
            "         453,    11,   332,    40,  8797,    25,   163,    25,  5185,\n",
            "          32,    21,    17,   127,    26,   393,   106,   220],\n",
            "      dtype=int32), 'targets_pretokenized': b'<extra_id_0> that<extra_id_1> formed<extra_id_2> of<extra_id_3> in<extra_id_4> \"<extra_id_5> \"<extra_id_6> sets<extra_id_7> code<extra_id_8> cart<extra_id_9> ImmutableSet.<extra_id_10> (\"<extra_id_11> \"B<extra_id_12> {@<extra_id_13> {@code<extra_id_14> \")<extra_id_15> {@code ImmutableList<extra_id_16> \")<extra_id_17> {@<extra_id_18> \"<extra_id_19> code<extra_id_20> \")<extra_id_21> \"<extra_id_22> that<extra_id_23> for<extra_id_24> B<extra_id_25> get<extra_id_26> <extra_id_27> (<extra_id_28> );<extra_id_29> if<extra_id_30> the<extra_id_31> be<extra_id_32> are<extra_id_33> empty<extra_id_34> but<extra_id_35> Performance<extra_id_36> product<extra_id_37> of<extra_id_38> m<extra_id_39> }<extra_id_40> memory<extra_id_41> input sets<extra_id_42> are the individual<extra_id_43> are<extra_id_44> @<extra_id_45> to<extra_id_46> elements<extra_id_47> shared<extra_id_48> (often<extra_id_49> Object<extra_id_50> return<extra_id_51> code<extra_id_52> },<extra_id_53> is<extra_id_54> @since<extra_id_55> <extra_id_56> List<extra_id_57> Product<extra_id_58> cartesian<extra_id_59> asList<extra_id_60>', 'targets': array([32099,    38,    40,  8797,    25,   163,   985, 15589,    40,\n",
            "        8797,    25,   163,  2697,    26,    40,  8797,    25,   163,\n",
            "        5638,    31,    40,  8797,    25,   163,  9864,  2928,  8797,\n",
            "          25,   163, 16209,  2928,  8797,    25,   163, 23559,  2504,\n",
            "          40,  8797,    25,   163, 28020,    89,    40,  8797,    25,\n",
            "         163,  9981,    32,  9440,    40,  8797,    25,   163,    25,\n",
            "        1313,    32, 11810,     8,    40,  8797,    25,   163,    25,\n",
            "        1532,    32,     7,  2457,  8797,    25,   163,    25,  2031,\n",
            "          32,    41,   399,    40,  8797,    25,   163,    25,  1572,\n",
            "          32,   106,    40,  8797,    25,   163,    25,  2900,    32,\n",
            "         106,   220,    40,  8797,    25,   163,    25,  2909,    32,\n",
            "           7,   223,    40,  8797,    25,   163,    25,  3060,    32,\n",
            "         106,   220,  6431,    40,  8797,    25,   163,    25,  1932,\n",
            "          32,     7,   223,    40,  8797,    25,   163,    25,  3305,\n",
            "          32,   106,    40,  8797,    25,   163,    25,  2469,    32,\n",
            "        2928,  8797,    25,   163,    25,  3508,    32,    89,    40,\n",
            "        8797,    25,   163,    25,  2301,    32,     7,   223,    40,\n",
            "        8797,    25,   163,    25,  3202,    32,  2928,  8797,    25,\n",
            "         163,    25,  3423,    32,    38,    40,  8797,    25,   163,\n",
            "          25,  3255,    32,    34,    40,  8797,    25,   163,    25,\n",
            "        3038,    32,   628,    40,  8797,    25,   163,    25,  2968,\n",
            "          32,    99,    40,  8797,    25,   163,    25,  4207,    32,\n",
            "       32072,    23,    40,  8797,    25,   163,    25,  4649,    32,\n",
            "           7,    18,    40,  8797,    25,   163,    25,  4340,    32,\n",
            "          28,    40,  8797,    25,   163,    25,  2954,    32,    10,\n",
            "          40,  8797,    25,   163,    25,  3622,    32,    45,    40,\n",
            "        8797,    25,   163,    25,  1472,    32,    63,    40,  8797,\n",
            "          25,   163,    25,  4099,    32,   662,    40,  8797,    25,\n",
            "         163,    25,  4234,    32,   112,    40,  8797,    25,   163,\n",
            "          25,  5217,    32, 11923,    40,  8797,    25,   163,    25,\n",
            "        5905,    32,  1868,    40,  8797,    25,   163,    25,  5903,\n",
            "          32,    26,    40,  8797,    25,   163,    25,  5957,    32,\n",
            "         201,    40,  8797,    25,   163,    25,  5185,    32,    13,\n",
            "          40,  8797,    25,   163,    25,  3327,    32,   582,    40,\n",
            "        8797,    25,   163,    25,  7304,    32,   250,  2504,    40,\n",
            "        8797,    25,   163,    25,  5276,    32,    63,    10,  3236,\n",
            "          40,  8797,    25,   163,    25,  5940,    32,    63,    40,\n",
            "        8797,    25,   163,    25,  5453,    32,    27,    40,  8797,\n",
            "          25,   163,    25,  4815,    32,    14,    40,  8797,    25,\n",
            "         163,    25,  6267,    32,   495,    40,  8797,    25,   163,\n",
            "          25,  4696,    32,  1962,    40,  8797,    25,   163,    25,\n",
            "        4614,    32,    23, 23727,    40,  8797,    25,   163,    25,\n",
            "        5854,    32,   215,    40,  8797,    25,   163,    25,  4163,\n",
            "          32,    42,    40,  8797,    25,   163,    25,  7911,    32,\n",
            "          89,    40,  8797,    25,   163,    25,  6607,    32,    13,\n",
            "          11,    40,  8797,    25,   163,    25,  6668,    32,    21,\n",
            "          40,  8797,    25,   163,    25,  6097,    32,    27,  1408,\n",
            "          40,  8797,    25,   163,    25,  6179,    32, 32043,   170,\n",
            "          40,  8797,    25,   163,    25,  6921,    32,  4620,    40,\n",
            "        8797,    25,   163,    25,  6609,    32, 28691,    40,  8797,\n",
            "          25,   163,    25,  4848,    32,     7,  1465,    40,  8797,\n",
            "          25,   163,    25,  3579,    32], dtype=int32)}\n",
            "{'inputs_pretokenized': b'<extra_id_0> <technical_language> Populates a map<extra_id_1> reading<extra_id_2> input stream,<extra_id_3> part<extra_id_4> deserialization<extra_id_5> See {@link #<extra_id_6> Map} for the data format. </technical_language><code> static <K, V> void populateMap(Map<K, V> map,<extra_id_7> InputStream stream) throws IOException, ClassNotFoundException { int size =<extra_id_8> .readInt();<extra_id_9> Map(map, stream, size);<extra_id_10> <extra_id_11> </s>', 'inputs': array([32099,     7,     5,     7, 13836,    19,    17,   295,    40,\n",
            "        8797,    25,   163,   985,  1152,    40,  8797,    25,   163,\n",
            "        2697,   250,   457,    11,    40,  8797,    25,   163,  5638,\n",
            "         550,    40,  8797,    25,   163,  9864,  7922,    40,  8797,\n",
            "          25,   163, 16209,   653,   106,   135,   479,    40,  8797,\n",
            "          25,   163, 23559,   400,    96,    34,    10,   137,   572,\n",
            "           8,     7,     6,     3,    83,    60,   613,    11,  1056,\n",
            "          32,    71,  3651,   216,     9,   216,    40,   613,    11,\n",
            "        1056,    32,   295,    11,    40,  8797,    25,   163, 28020,\n",
            "        1483,   457,    12,   169,   331,    11,  4844,    66,    15,\n",
            "          75,   393,    16,    40,  8797,    25,   163,  9981,    32,\n",
            "           7,     8,  7436,    39,    40,  8797,    25,   163,    25,\n",
            "        1313,    32,   400,     9,   514,    11,   457,    11,   393,\n",
            "          18,    40,  8797,    25,   163,    25,  1532,    32, 32088,\n",
            "         246,    19,    32], dtype=int32), 'targets_pretokenized': b'<extra_id_0> by<extra_id_1> an<extra_id_2> as<extra_id_3> of<extra_id_4> .<extra_id_5> write<extra_id_6> Object<extra_id_7> stream<extra_id_8> populate<extra_id_9> }<extra_id_10> </code><extra_id_11>', 'targets': array([32099,    84,    40,  8797,    25,   163,   985,    65,    40,\n",
            "        8797,    25,   163,  2697,    64,    40,  8797,    25,   163,\n",
            "        5638,    26,    40,  8797,    25,   163,  9864,     7,     8,\n",
            "          40,  8797,    25,   163, 16209,   377,    40,  8797,    25,\n",
            "         163, 23559,   215,    40,  8797,    25,   163, 28020,   457,\n",
            "          40,  8797,    25,   163,  9981,    32,  3651,    40,  8797,\n",
            "          25,   163,    25,  1313,    32,    13,    40,  8797,    25,\n",
            "         163,    25,  1532,    32,     7,     4,    40,  8797,    25,\n",
            "         163,    25,  2031,    32], dtype=int32)}\n",
            "{'inputs_pretokenized': b'<technical_language> Add<extra_id_0> link Valve}<extra_id_1> should be applied to<extra_id_2> Tomcat {@link <extra_id_3> }. @param<extra_id_4> Valves the valve<extra_id_5> to add </technical_language><code> public void addContextValves(Valve<extra_id_6> contextValves) { Assert.<extra_id_7> (contextValves, \"Valves must not be null\"); this.contextValve<extra_id_8> .<extra_id_9> (Arrays.<extra_id_10> (contextValves)); } </code></s>', 'inputs': array([    7,     5,   999,    40,  8797,    25,   163,   248,     7,\n",
            "         135,     7, 22581,    96,    40,  8797,    25,   163,   985,\n",
            "         104,    45,  2526,    14,    40,  8797,    25,   163,  2697,\n",
            "        1927,   106,   135, 32096,    13,     8,    27,    51,    40,\n",
            "        8797,    25,   163,  9864,     7, 22581,    19,    10,  1055,\n",
            "         507,    40,  8797,    25,   163, 16209,    14,   180,     7,\n",
            "           6,     3,    44,    71,   180,   242, 22581,    19,     9,\n",
            "       22581,    40,  8797,    25,   163, 23559,   304, 22581,    19,\n",
            "          12,    15,  3898,     8,    40,  8797,    25,   163, 28020,\n",
            "          23,   680, 22581,    19,    11,    41, 22581,    19,   341,\n",
            "          56,    45,    49,    78,    36,     8,   680, 22581,    40,\n",
            "        8797,    25,   163,  9981,    32,     7,     8,    40,  8797,\n",
            "          25,   163,    25,  1313,    32,    23,  1537,     8,    40,\n",
            "        8797,    25,   163,    25,  1532,    32,    23,   680, 22581,\n",
            "          19,   164,    13,     7,     4,   210,    19,    32],\n",
            "      dtype=int32), 'targets_pretokenized': b'<extra_id_0> {@<extra_id_1> s that<extra_id_2> the<extra_id_3> Context<extra_id_4> context<extra_id_5> s<extra_id_6> ...<extra_id_7> notNull<extra_id_8> s<extra_id_9> addAll<extra_id_10> asList<extra_id_11>', 'targets': array([32099,   106,    40,  8797,    25,   163,   985,     7,    19,\n",
            "          38,    40,  8797,    25,   163,  2697,    10,    40,  8797,\n",
            "          25,   163,  5638,     7,   242,    40,  8797,    25,   163,\n",
            "        9864,   304,    40,  8797,    25,   163, 16209,     7,    19,\n",
            "          40,  8797,    25,   163, 23559,   318,    40,  8797,    25,\n",
            "         163, 28020,     7,  3780,    40,  8797,    25,   163,  9981,\n",
            "          32,     7,    19,    40,  8797,    25,   163,    25,  1313,\n",
            "          32,     7,  1615,    40,  8797,    25,   163,    25,  1532,\n",
            "          32,     7,  1465,    40,  8797,    25,   163,    25,  2031,\n",
            "          32], dtype=int32)}\n",
            "{'inputs_pretokenized': b'<technical_language> Returns the<extra_id_0> at<extra_id_1> code index<extra_id_2> if it exists and is an int or can be coerced to an int. @param index the index to get the value<extra_id_3> @return<extra_id_4> {@code<extra_id_5> @<extra_id_6> JSONException if the value<extra_id_7> {@code index} doesn\\'t exist<extra_id_8> be coerced to<extra_id_9> int<extra_id_10> </technical_language><extra_id_11> public int <extra_id_12> (int index) throws JSONException { Object<extra_id_13> = get(index); Integer<extra_id_14> = JSON.<extra_id_15> Integer(object); if (result == null) { throw JSON.typeMismatch(index, object, \"int\"); } return result; } </code></s>', 'inputs': array([    7,     5,   480,    10,    40,  8797,    25,   163,   248,\n",
            "         130,    40,  8797,    25,   163,   985,    89,   305,    40,\n",
            "        8797,    25,   163,  2697,    28,    37,  1374,    30,    21,\n",
            "          65,    75,    74,    52,    45, 28941,    14,    65,    75,\n",
            "           8,    27,    51,   305,    10,   305,    14,    99,    10,\n",
            "          80,    40,  8797,    25,   163,  5638,    27,    91,    40,\n",
            "        8797,    25,   163,  9864,   106,   220,    40,  8797,    25,\n",
            "         163, 16209,    27,    40,  8797,    25,   163, 23559,  7901,\n",
            "          28,    10,    80,    40,  8797,    25,   163, 28020,   106,\n",
            "         220,   305,    96,   368,    29,    57,  1192,    40,  8797,\n",
            "          25,   163,  9981,    32,    45, 28941,    14,    40,  8797,\n",
            "          25,   163,    25,  1313,    32,    75,    40,  8797,    25,\n",
            "         163,    25,  1532,    32,     7,     6,    40,  8797,    25,\n",
            "         163,    25,  2031,    32,    44,    75, 32087,    23,   102,\n",
            "         305,    12,   169,  7901,    15,   215,    40,  8797,    25,\n",
            "         163,    25,  2900,    32,    16,    99,     9,   551,    18,\n",
            "         384,    40,  8797,    25,   163,    25,  2909,    32,    16,\n",
            "         679,     8,    40,  8797,    25,   163,    25,  3060,    32,\n",
            "         384,     9,  1236,    18,    28,    23,   729,    85,    49,\n",
            "          12,    15,   161,   679,     8,   401, 13756,     9,   551,\n",
            "          11,   116,    11,    41,   102,    78,    13,    42,   149,\n",
            "          24,    13,     7,     4,   210,    19,    32], dtype=int32), 'targets_pretokenized': b'<extra_id_0> value<extra_id_1> {@<extra_id_2> }<extra_id_3> from<extra_id_4> the<extra_id_5> value}<extra_id_6> throws<extra_id_7> at<extra_id_8> or cannot<extra_id_9> an<extra_id_10> .<extra_id_11> <code><extra_id_12> getInt<extra_id_13> object<extra_id_14> result<extra_id_15> to<extra_id_16>', 'targets': array([32099,    80,    40,  8797,    25,   163,   985,   106,    40,\n",
            "        8797,    25,   163,  2697,    13,    40,  8797,    25,   163,\n",
            "        5638,    76,    40,  8797,    25,   163,  9864,    10,    40,\n",
            "        8797,    25,   163, 16209,    80,    96,    40,  8797,    25,\n",
            "         163, 23559,   169,    40,  8797,    25,   163, 28020,   130,\n",
            "          40,  8797,    25,   163,  9981,    32,    74,   473,    40,\n",
            "        8797,    25,   163,    25,  1313,    32,    65,    40,  8797,\n",
            "          25,   163,    25,  1532,    32,     7,     8,    40,  8797,\n",
            "          25,   163,    25,  2031,    32,     7,     3,    40,  8797,\n",
            "          25,   163,    25,  1572,    32,     7,  2839,    40,  8797,\n",
            "          25,   163,    25,  2900,    32,   116,    40,  8797,    25,\n",
            "         163,    25,  2909,    32,   149,    40,  8797,    25,   163,\n",
            "          25,  3060,    32,    14,    40,  8797,    25,   163,    25,\n",
            "        1932,    32], dtype=int32)}\n",
            "{'inputs_pretokenized': b'<extra_id_0> Add URL patterns<extra_id_1> as defined<extra_id_2> the Servlet specification,<extra_id_3> the filter will be registered<extra_id_4> . @param urlPatterns<extra_id_5> URL patterns </technical_language><code> public void addUrlPatterns(String... urlPatterns) { Assert<extra_id_6> notNull(<extra_id_7> , \"UrlPatterns must not be null\"); Collections.addAll(this.urlPatterns<extra_id_8> urlPatterns); } </code></s>', 'inputs': array([32099,   999,   634,  4323,    40,  8797,    25,   163,   985,\n",
            "          64,   855,    40,  8797,    25,   163,  2697,    10,  2733,\n",
            "        2342,    11,    40,  8797,    25,   163,  5638,    10,   742,\n",
            "          69,    45,  2292,    40,  8797,    25,   163,  9864,     7,\n",
            "           8,    27,    51,     7, 27024,    40,  8797,    25,   163,\n",
            "       16209,   634,  4323,     7,     6,     3,    44,    71,   180,\n",
            "         875,  8653,     9,    50,   416,     7, 27024,    12,    15,\n",
            "        3898,    40,  8797,    25,   163, 23559,     7,  3780,     9,\n",
            "          40,  8797,    25,   163, 28020,     7,    11,    41,   875,\n",
            "        8653,   341,    56,    45,    49,    78,  1254,     8,  1615,\n",
            "           9,   188,     8, 27024,    40,  8797,    25,   163,  9981,\n",
            "          32,     7, 27024,    18,    13,     7,     4,   210,    19,\n",
            "          32], dtype=int32), 'targets_pretokenized': b'<extra_id_0> <technical_language><extra_id_1> ,<extra_id_2> in<extra_id_3> that<extra_id_4> against<extra_id_5> the<extra_id_6> .<extra_id_7> urlPatterns<extra_id_8> , <extra_id_9>', 'targets': array([32099,     7,     5,    40,  8797,    25,   163,   985,     7,\n",
            "          11,    40,  8797,    25,   163,  2697,    31,    40,  8797,\n",
            "          25,   163,  5638,    38,    40,  8797,    25,   163,  9864,\n",
            "        1881,    40,  8797,    25,   163, 16209,    10,    40,  8797,\n",
            "          25,   163, 23559,     7,     8,    40,  8797,    25,   163,\n",
            "       28020,     7, 27024,    40,  8797,    25,   163,  9981,    32,\n",
            "           7,    11, 32090], dtype=int32)}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YgvnbVWU78nz"
      },
      "source": [
        "from mesh_tensorflow.transformer.learning_rate_schedules import learning_rate_schedule_noam\n",
        "\n",
        "#See https://github.com/google-research/text-to-text-transfer-transformer if you want to scale up the model\n",
        "MODEL_SIZE = \"small\"  \n",
        "\n",
        "MODEL_DIR = 'gs://code_review_automation/model_dumps'\n",
        "\n",
        "\n",
        "model_parallelism, train_batch_size, keep_checkpoint_max = {\n",
        "    \"small\": (1, 512, 16),\n",
        "    \"base\": (2, 128, 8),\n",
        "    \"large\": (8, 64, 4),\n",
        "    \"3B\": (8, 16, 1),\n",
        "    \"11B\": (8, 16, 1)}[MODEL_SIZE]\n",
        "\n",
        "\n",
        "tf.io.gfile.makedirs(MODEL_DIR)\n",
        "\n",
        "model = t5.models.MtfModel(\n",
        "    model_dir=MODEL_DIR,\n",
        "    tpu=TPU_ADDRESS,\n",
        "    tpu_topology=TPU_TOPOLOGY,\n",
        "    model_parallelism=model_parallelism,\n",
        "    batch_size=train_batch_size,\n",
        "    sequence_length={\"inputs\": 512, \"targets\": 512},\n",
        "    learning_rate_schedule = learning_rate_schedule_noam,\n",
        "    save_checkpoints_steps=5000,\n",
        "    keep_checkpoint_max=keep_checkpoint_max if ON_CLOUD else None\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tmdx3v9z8DY6",
        "outputId": "fd3b7238-820e-46cf-ab3d-8262601f1f30"
      },
      "source": [
        "!gsutil cp gs://code_review_automation/CodeReviewModel/operative_config.gin ./operative_config.gin \n",
        "PATH_GIN_FILE = './operative_config.gin'\n",
        "import gin\n",
        "with gin.unlock_config():    \n",
        "    gin.parse_config_file(PATH_GIN_FILE)\n",
        "    TRAIN_STEPS = 200000\n",
        "    model.train(\"pretraining\", steps=TRAIN_STEPS)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying gs://code_review_automation/CodeReviewModel/operative_config.gin...\n",
            "/ [0 files][    0.0 B/ 11.5 KiB]                                                \r/ [1 files][ 11.5 KiB/ 11.5 KiB]                                                \r\n",
            "Operation completed over 1 objects/11.5 KiB.                                     \n",
            "INFO:tensorflow:Using config: {'_model_dir': 'gs://code_review_automation/model_dumps', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': None, '_session_config': graph_options {\n",
            "  rewrite_options {\n",
            "    disable_meta_optimizer: true\n",
            "  }\n",
            "}\n",
            "cluster_def {\n",
            "  job {\n",
            "    name: \"worker\"\n",
            "    tasks {\n",
            "      key: 0\n",
            "      value: \"10.65.126.74:8470\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({'worker': ['10.65.126.74:8470']}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': 'grpc://10.65.126.74:8470', '_evaluation_master': 'grpc://10.65.126.74:8470', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=100, num_shards=None, num_cores_per_replica=1, per_host_input_for_training=4, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2, experimental_host_call_every_n_steps=1, experimental_allow_per_host_v2_parallel_get_next=False, experimental_feed_hook=None), '_cluster': <tensorflow.python.distribute.cluster_resolver.tpu.tpu_cluster_resolver.TPUClusterResolver object at 0x7fc4fa1d7490>}\n",
            "INFO:tensorflow:_TPUContext: eval_on_tpu True\n",
            "INFO:tensorflow:Skipping training since max_steps has already saved.\n",
            "INFO:tensorflow:training_loop marked as finished\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}