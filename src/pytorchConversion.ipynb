{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "pytorchConversion.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPAFuU5PPNksxeheYGkAMoD",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/masies/CRA/blob/main/pytorchConversion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8i-CsM5qG8P4",
        "outputId": "864dce74-1221-4caa-97d4-c8a0ee04b526"
      },
      "source": [
        "!pip install sentencepiece==0.1.94\n",
        "!pip install -q torch==1.4.0 -f https://download.pytorch.org/whl/cu101/torch_stable.html\n",
        "!pip install -q transformers==3.5.0 fast-trees\n",
        "!git clone -q https://github.com/microsoft/CodeXGLUE.git\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "from transformers import T5Tokenizer, T5Config, T5ForConditionalGeneration\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import logging\n",
        "import shutil\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import json\n",
        "import sys\n",
        "import statistics\n",
        "\n",
        "\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "project_id = 'helical-loop-303918'\n",
        "bucket_name = 'code_review_automation'\n",
        "\n",
        "!gcloud config set project {project_id}\n",
        "\n",
        "!mkdir core\n",
        "\n",
        "!gsutil -m cp -r \\\n",
        "  \"gs://code_review_automation/pyTorch_coversion/core/Logger.py\" \\\n",
        "  ./core\n",
        "\n",
        "from core.Logger import Logger\n",
        "\n",
        "# script for conversion in pythorch\n",
        "!gsutil cp gs://{bucket_name}/pyTorch_coversion/code/tf_2_pytorch_T5.py ./tf_2_pytorch_T5.py\n",
        "\n",
        "# Download the configuration file\n",
        "!gsutil cp gs://{bucket_name}/pyTorch_coversion/config/config.json ./current_model/config.json\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentencepiece==0.1.94\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6e/f0/7614029138ec9422f1a3ed3cd82c3bfc0821157e8032ca1828cee6b198bb/sentencepiece-0.1.94-cp37-cp37m-manylinux2014_x86_64.whl (1.1MB)\n",
            "\r\u001b[K     |▎                               | 10kB 16.9MB/s eta 0:00:01\r\u001b[K     |▋                               | 20kB 23.0MB/s eta 0:00:01\r\u001b[K     |▉                               | 30kB 28.1MB/s eta 0:00:01\r\u001b[K     |█▏                              | 40kB 23.6MB/s eta 0:00:01\r\u001b[K     |█▌                              | 51kB 18.0MB/s eta 0:00:01\r\u001b[K     |█▊                              | 61kB 17.3MB/s eta 0:00:01\r\u001b[K     |██                              | 71kB 13.1MB/s eta 0:00:01\r\u001b[K     |██▍                             | 81kB 13.9MB/s eta 0:00:01\r\u001b[K     |██▋                             | 92kB 15.1MB/s eta 0:00:01\r\u001b[K     |███                             | 102kB 14.0MB/s eta 0:00:01\r\u001b[K     |███▎                            | 112kB 14.0MB/s eta 0:00:01\r\u001b[K     |███▌                            | 122kB 14.0MB/s eta 0:00:01\r\u001b[K     |███▉                            | 133kB 14.0MB/s eta 0:00:01\r\u001b[K     |████▏                           | 143kB 14.0MB/s eta 0:00:01\r\u001b[K     |████▍                           | 153kB 14.0MB/s eta 0:00:01\r\u001b[K     |████▊                           | 163kB 14.0MB/s eta 0:00:01\r\u001b[K     |█████                           | 174kB 14.0MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 184kB 14.0MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 194kB 14.0MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 204kB 14.0MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 215kB 14.0MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 225kB 14.0MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 235kB 14.0MB/s eta 0:00:01\r\u001b[K     |███████                         | 245kB 14.0MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 256kB 14.0MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 266kB 14.0MB/s eta 0:00:01\r\u001b[K     |████████                        | 276kB 14.0MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 286kB 14.0MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 296kB 14.0MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 307kB 14.0MB/s eta 0:00:01\r\u001b[K     |█████████                       | 317kB 14.0MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 327kB 14.0MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 337kB 14.0MB/s eta 0:00:01\r\u001b[K     |██████████                      | 348kB 14.0MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 358kB 14.0MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 368kB 14.0MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 378kB 14.0MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 389kB 14.0MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 399kB 14.0MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 409kB 14.0MB/s eta 0:00:01\r\u001b[K     |████████████                    | 419kB 14.0MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 430kB 14.0MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 440kB 14.0MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 450kB 14.0MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 460kB 14.0MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 471kB 14.0MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 481kB 14.0MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 491kB 14.0MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 501kB 14.0MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 512kB 14.0MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 522kB 14.0MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 532kB 14.0MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 542kB 14.0MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 552kB 14.0MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 563kB 14.0MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 573kB 14.0MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 583kB 14.0MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 593kB 14.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 604kB 14.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 614kB 14.0MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 624kB 14.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 634kB 14.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 645kB 14.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 655kB 14.0MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 665kB 14.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 675kB 14.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 686kB 14.0MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 696kB 14.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 706kB 14.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 716kB 14.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 727kB 14.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 737kB 14.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 747kB 14.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 757kB 14.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 768kB 14.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 778kB 14.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 788kB 14.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 798kB 14.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 808kB 14.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 819kB 14.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 829kB 14.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 839kB 14.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 849kB 14.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 860kB 14.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 870kB 14.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 880kB 14.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 890kB 14.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 901kB 14.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 911kB 14.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 921kB 14.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 931kB 14.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 942kB 14.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 952kB 14.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 962kB 14.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 972kB 14.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 983kB 14.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 993kB 14.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.0MB 14.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.0MB 14.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.0MB 14.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.0MB 14.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.0MB 14.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.1MB 14.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 1.1MB 14.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 1.1MB 14.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.1MB 14.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 1.1MB 14.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 1.1MB 14.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.1MB 14.0MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.94\n",
            "\u001b[K     |████████████████████████████████| 753.4MB 22kB/s \n",
            "\u001b[31mERROR: torchvision 0.9.1+cu101 has requirement torch==1.8.1, but you'll have torch 1.4.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: torchtext 0.9.1 has requirement torch==1.8.1, but you'll have torch 1.4.0 which is incompatible.\u001b[0m\n",
            "\u001b[K     |████████████████████████████████| 1.3MB 11.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 870kB 48.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.9MB 55.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1MB 46.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 112kB 58.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 163kB 55.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 9.1MB/s \n",
            "\u001b[?25h  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for tree-sitter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Updated property [core/project].\n",
            "Copying gs://code_review_automation/pyTorch_coversion/core/Logger.py...\n",
            "/ [1/1 files][   1016 B/   1016 B] 100% Done                                    \n",
            "Operation completed over 1 objects/1016.0 B.                                     \n",
            "Copying gs://code_review_automation/pyTorch_coversion/code/tf_2_pytorch_T5.py...\n",
            "/ [1 files][  2.3 KiB/  2.3 KiB]                                                \n",
            "Operation completed over 1 objects/2.3 KiB.                                      \n",
            "Copying gs://code_review_automation/pyTorch_coversion/config/config.json...\n",
            "/ [1 files][  463.0 B/  463.0 B]                                                \n",
            "Operation completed over 1 objects/463.0 B.                                      \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-xKBubBHvGD",
        "outputId": "621543cc-1bc3-4199-fc61-b706ef49cef7"
      },
      "source": [
        "model_number = 395000\n",
        "dataset_version = \"v2\"\n",
        "model_type = \"codeANDcomment_code\"\n",
        "trained_type =\"pretrained\"\n",
        "\n",
        "# Download the selected best model\n",
        "!mkdir dumps\n",
        "!mkdir current_model\n",
        "!gsutil -m cp \\\n",
        "  \"gs://code_review_automation/fine_tuning/HP_tuning/final_version/comment_{dataset_version}/{model_type}/{trained_type}/model.ckpt-{model_number}.data-00000-of-00002\" \\\n",
        "  \"gs://code_review_automation/fine_tuning/HP_tuning/final_version/comment_{dataset_version}/{model_type}/{trained_type}/model.ckpt-{model_number}.data-00001-of-00002\" \\\n",
        "  \"gs://code_review_automation/fine_tuning/HP_tuning/final_version/comment_{dataset_version}/{model_type}/{trained_type}/model.ckpt-{model_number}.index\" \\\n",
        "  \"gs://code_review_automation/fine_tuning/HP_tuning/final_version/comment_{dataset_version}/{model_type}/{trained_type}/model.ckpt-{model_number}.meta\" \\\n",
        "  ./current_model/\n",
        "\n",
        "# Download the validation script\n",
        "# !gsutil cp gs://{bucket_name}/pyTorch_coversion/code/validation.py ./validation.py\n",
        "\n",
        "# Download the model and vocab\n",
        "!gsutil cp gs://{bucket_name}/CodeReviewModel/TestModel.model ./current_model/TestModel.model\n",
        "!gsutil cp gs://{bucket_name}/CodeReviewModel/TestModel.vocab ./current_model/TestModel.vocab\n",
        "\n",
        "# download the test set\n",
        "!gsutil cp gs://{bucket_name}/dataset/old/fineTuningDataset_{dataset_version}/codeANDcomment_code/test.tsv ./data/test.tsv\n",
        "\n",
        "# prepare source and target files\n",
        "df = pd.read_csv(\"./data/test.tsv\", sep='\\t', names=[\"source\",\"target\"])\n",
        "\n",
        "# initialize source and target files\n",
        "f = open(\"./data/test.source\", \"w\")\n",
        "f.close()\n",
        "f = open(\"./data/test.target\", \"w\")\n",
        "f.close()\n",
        "\n",
        "with open(\"./data/test.source\", \"a\") as source:\n",
        "  with open(\"./data/test.target\", \"a\") as target:\n",
        "    for index, row in df.iterrows():\n",
        "      # if index < 80:\n",
        "      source.write(\"code&comment2code: \" + row.source + \"\\n\")\n",
        "      target.write(row.target + \"\\n\")\n",
        "\n",
        "# Convert the model\n",
        "!python3 ./tf_2_pytorch_T5.py --tf_checkpoint_path ./current_model/model.ckpt-{model_number} --config_file ./current_model/config.json --pytorch_dump_path ./dumps\n",
        "\n",
        "!head -n 1 ./data/test.source\n",
        "!head -n 1 ./data/test.target"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘current_model’: File exists\n",
            "Copying gs://code_review_automation/fine_tuning/HP_tuning/final_version/comment_v2/codeANDcomment_code/pretrained/model.ckpt-285000.data-00000-of-00002...\n",
            "Copying gs://code_review_automation/fine_tuning/HP_tuning/final_version/comment_v2/codeANDcomment_code/pretrained/model.ckpt-285000.data-00001-of-00002...\n",
            "Copying gs://code_review_automation/fine_tuning/HP_tuning/final_version/comment_v2/codeANDcomment_code/pretrained/model.ckpt-285000.index...\n",
            "Copying gs://code_review_automation/fine_tuning/HP_tuning/final_version/comment_v2/codeANDcomment_code/pretrained/model.ckpt-285000.meta...\n",
            "/ [4/4 files][126.0 MiB/126.0 MiB] 100% Done                                    \n",
            "Operation completed over 4 objects/126.0 MiB.                                    \n",
            "Copying gs://code_review_automation/CodeReviewModel/TestModel.model...\n",
            "/ [1 files][762.0 KiB/762.0 KiB]                                                \n",
            "Operation completed over 1 objects/762.0 KiB.                                    \n",
            "Copying gs://code_review_automation/CodeReviewModel/TestModel.vocab...\n",
            "/ [1 files][557.4 KiB/557.4 KiB]                                                \n",
            "Operation completed over 1 objects/557.4 KiB.                                    \n",
            "Copying gs://code_review_automation/dataset/old/fineTuningDataset_v2/codeANDcomment_code/test.tsv...\n",
            "/ [1 files][  1.1 MiB/  1.1 MiB]                                                \n",
            "Operation completed over 1 objects/1.1 MiB.                                      \n",
            "2021-04-14 13:57:31.675724: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "Building PyTorch model from configuration: T5Config {\n",
            "  \"architectures\": [\n",
            "    \"T5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 512,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"n_positions\": 512,\n",
            "  \"num_decoder_layers\": 6,\n",
            "  \"num_heads\": 8,\n",
            "  \"num_layers\": 6,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "Converting TensorFlow checkpoint from /content/current_model/model.ckpt-285000\n",
            "Loading TF weight decoder/block_000/layer_000/SelfAttention/k with shape [512, 512]\n",
            "Loading TF weight decoder/block_000/layer_000/SelfAttention/k_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_000/layer_000/SelfAttention/k_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_000/layer_000/SelfAttention/o with shape [512, 512]\n",
            "Loading TF weight decoder/block_000/layer_000/SelfAttention/o_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_000/layer_000/SelfAttention/o_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_000/layer_000/SelfAttention/q with shape [512, 512]\n",
            "Loading TF weight decoder/block_000/layer_000/SelfAttention/q_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_000/layer_000/SelfAttention/q_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_000/layer_000/SelfAttention/relative_attention_bias with shape [8, 32]\n",
            "Loading TF weight decoder/block_000/layer_000/SelfAttention/relative_attention_bias_slot_v with shape [8, 32]\n",
            "Loading TF weight decoder/block_000/layer_000/SelfAttention/v with shape [512, 512]\n",
            "Loading TF weight decoder/block_000/layer_000/SelfAttention/v_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_000/layer_000/SelfAttention/v_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_000/layer_000/layer_norm/scale with shape [512]\n",
            "Loading TF weight decoder/block_000/layer_000/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight decoder/block_000/layer_001/EncDecAttention/k with shape [512, 512]\n",
            "Loading TF weight decoder/block_000/layer_001/EncDecAttention/k_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_000/layer_001/EncDecAttention/k_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_000/layer_001/EncDecAttention/o with shape [512, 512]\n",
            "Loading TF weight decoder/block_000/layer_001/EncDecAttention/o_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_000/layer_001/EncDecAttention/o_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_000/layer_001/EncDecAttention/q with shape [512, 512]\n",
            "Loading TF weight decoder/block_000/layer_001/EncDecAttention/q_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_000/layer_001/EncDecAttention/q_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_000/layer_001/EncDecAttention/v with shape [512, 512]\n",
            "Loading TF weight decoder/block_000/layer_001/EncDecAttention/v_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_000/layer_001/EncDecAttention/v_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_000/layer_001/layer_norm/scale with shape [512]\n",
            "Loading TF weight decoder/block_000/layer_001/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight decoder/block_000/layer_002/DenseReluDense/wi/kernel with shape [512, 2048]\n",
            "Loading TF weight decoder/block_000/layer_002/DenseReluDense/wi/kernel_slot_vc with shape [2048]\n",
            "Loading TF weight decoder/block_000/layer_002/DenseReluDense/wi/kernel_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_000/layer_002/DenseReluDense/wo/kernel with shape [2048, 512]\n",
            "Loading TF weight decoder/block_000/layer_002/DenseReluDense/wo/kernel_slot_vc with shape [2048]\n",
            "Loading TF weight decoder/block_000/layer_002/DenseReluDense/wo/kernel_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_000/layer_002/layer_norm/scale with shape [512]\n",
            "Loading TF weight decoder/block_000/layer_002/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight decoder/block_001/layer_000/SelfAttention/k with shape [512, 512]\n",
            "Loading TF weight decoder/block_001/layer_000/SelfAttention/k_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_001/layer_000/SelfAttention/k_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_001/layer_000/SelfAttention/o with shape [512, 512]\n",
            "Loading TF weight decoder/block_001/layer_000/SelfAttention/o_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_001/layer_000/SelfAttention/o_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_001/layer_000/SelfAttention/q with shape [512, 512]\n",
            "Loading TF weight decoder/block_001/layer_000/SelfAttention/q_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_001/layer_000/SelfAttention/q_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_001/layer_000/SelfAttention/v with shape [512, 512]\n",
            "Loading TF weight decoder/block_001/layer_000/SelfAttention/v_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_001/layer_000/SelfAttention/v_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_001/layer_000/layer_norm/scale with shape [512]\n",
            "Loading TF weight decoder/block_001/layer_000/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight decoder/block_001/layer_001/EncDecAttention/k with shape [512, 512]\n",
            "Loading TF weight decoder/block_001/layer_001/EncDecAttention/k_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_001/layer_001/EncDecAttention/k_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_001/layer_001/EncDecAttention/o with shape [512, 512]\n",
            "Loading TF weight decoder/block_001/layer_001/EncDecAttention/o_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_001/layer_001/EncDecAttention/o_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_001/layer_001/EncDecAttention/q with shape [512, 512]\n",
            "Loading TF weight decoder/block_001/layer_001/EncDecAttention/q_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_001/layer_001/EncDecAttention/q_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_001/layer_001/EncDecAttention/v with shape [512, 512]\n",
            "Loading TF weight decoder/block_001/layer_001/EncDecAttention/v_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_001/layer_001/EncDecAttention/v_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_001/layer_001/layer_norm/scale with shape [512]\n",
            "Loading TF weight decoder/block_001/layer_001/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight decoder/block_001/layer_002/DenseReluDense/wi/kernel with shape [512, 2048]\n",
            "Loading TF weight decoder/block_001/layer_002/DenseReluDense/wi/kernel_slot_vc with shape [2048]\n",
            "Loading TF weight decoder/block_001/layer_002/DenseReluDense/wi/kernel_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_001/layer_002/DenseReluDense/wo/kernel with shape [2048, 512]\n",
            "Loading TF weight decoder/block_001/layer_002/DenseReluDense/wo/kernel_slot_vc with shape [2048]\n",
            "Loading TF weight decoder/block_001/layer_002/DenseReluDense/wo/kernel_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_001/layer_002/layer_norm/scale with shape [512]\n",
            "Loading TF weight decoder/block_001/layer_002/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight decoder/block_002/layer_000/SelfAttention/k with shape [512, 512]\n",
            "Loading TF weight decoder/block_002/layer_000/SelfAttention/k_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_002/layer_000/SelfAttention/k_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_002/layer_000/SelfAttention/o with shape [512, 512]\n",
            "Loading TF weight decoder/block_002/layer_000/SelfAttention/o_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_002/layer_000/SelfAttention/o_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_002/layer_000/SelfAttention/q with shape [512, 512]\n",
            "Loading TF weight decoder/block_002/layer_000/SelfAttention/q_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_002/layer_000/SelfAttention/q_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_002/layer_000/SelfAttention/v with shape [512, 512]\n",
            "Loading TF weight decoder/block_002/layer_000/SelfAttention/v_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_002/layer_000/SelfAttention/v_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_002/layer_000/layer_norm/scale with shape [512]\n",
            "Loading TF weight decoder/block_002/layer_000/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight decoder/block_002/layer_001/EncDecAttention/k with shape [512, 512]\n",
            "Loading TF weight decoder/block_002/layer_001/EncDecAttention/k_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_002/layer_001/EncDecAttention/k_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_002/layer_001/EncDecAttention/o with shape [512, 512]\n",
            "Loading TF weight decoder/block_002/layer_001/EncDecAttention/o_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_002/layer_001/EncDecAttention/o_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_002/layer_001/EncDecAttention/q with shape [512, 512]\n",
            "Loading TF weight decoder/block_002/layer_001/EncDecAttention/q_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_002/layer_001/EncDecAttention/q_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_002/layer_001/EncDecAttention/v with shape [512, 512]\n",
            "Loading TF weight decoder/block_002/layer_001/EncDecAttention/v_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_002/layer_001/EncDecAttention/v_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_002/layer_001/layer_norm/scale with shape [512]\n",
            "Loading TF weight decoder/block_002/layer_001/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight decoder/block_002/layer_002/DenseReluDense/wi/kernel with shape [512, 2048]\n",
            "Loading TF weight decoder/block_002/layer_002/DenseReluDense/wi/kernel_slot_vc with shape [2048]\n",
            "Loading TF weight decoder/block_002/layer_002/DenseReluDense/wi/kernel_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_002/layer_002/DenseReluDense/wo/kernel with shape [2048, 512]\n",
            "Loading TF weight decoder/block_002/layer_002/DenseReluDense/wo/kernel_slot_vc with shape [2048]\n",
            "Loading TF weight decoder/block_002/layer_002/DenseReluDense/wo/kernel_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_002/layer_002/layer_norm/scale with shape [512]\n",
            "Loading TF weight decoder/block_002/layer_002/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight decoder/block_003/layer_000/SelfAttention/k with shape [512, 512]\n",
            "Loading TF weight decoder/block_003/layer_000/SelfAttention/k_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_003/layer_000/SelfAttention/k_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_003/layer_000/SelfAttention/o with shape [512, 512]\n",
            "Loading TF weight decoder/block_003/layer_000/SelfAttention/o_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_003/layer_000/SelfAttention/o_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_003/layer_000/SelfAttention/q with shape [512, 512]\n",
            "Loading TF weight decoder/block_003/layer_000/SelfAttention/q_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_003/layer_000/SelfAttention/q_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_003/layer_000/SelfAttention/v with shape [512, 512]\n",
            "Loading TF weight decoder/block_003/layer_000/SelfAttention/v_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_003/layer_000/SelfAttention/v_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_003/layer_000/layer_norm/scale with shape [512]\n",
            "Loading TF weight decoder/block_003/layer_000/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight decoder/block_003/layer_001/EncDecAttention/k with shape [512, 512]\n",
            "Loading TF weight decoder/block_003/layer_001/EncDecAttention/k_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_003/layer_001/EncDecAttention/k_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_003/layer_001/EncDecAttention/o with shape [512, 512]\n",
            "Loading TF weight decoder/block_003/layer_001/EncDecAttention/o_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_003/layer_001/EncDecAttention/o_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_003/layer_001/EncDecAttention/q with shape [512, 512]\n",
            "Loading TF weight decoder/block_003/layer_001/EncDecAttention/q_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_003/layer_001/EncDecAttention/q_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_003/layer_001/EncDecAttention/v with shape [512, 512]\n",
            "Loading TF weight decoder/block_003/layer_001/EncDecAttention/v_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_003/layer_001/EncDecAttention/v_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_003/layer_001/layer_norm/scale with shape [512]\n",
            "Loading TF weight decoder/block_003/layer_001/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight decoder/block_003/layer_002/DenseReluDense/wi/kernel with shape [512, 2048]\n",
            "Loading TF weight decoder/block_003/layer_002/DenseReluDense/wi/kernel_slot_vc with shape [2048]\n",
            "Loading TF weight decoder/block_003/layer_002/DenseReluDense/wi/kernel_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_003/layer_002/DenseReluDense/wo/kernel with shape [2048, 512]\n",
            "Loading TF weight decoder/block_003/layer_002/DenseReluDense/wo/kernel_slot_vc with shape [2048]\n",
            "Loading TF weight decoder/block_003/layer_002/DenseReluDense/wo/kernel_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_003/layer_002/layer_norm/scale with shape [512]\n",
            "Loading TF weight decoder/block_003/layer_002/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight decoder/block_004/layer_000/SelfAttention/k with shape [512, 512]\n",
            "Loading TF weight decoder/block_004/layer_000/SelfAttention/k_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_004/layer_000/SelfAttention/k_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_004/layer_000/SelfAttention/o with shape [512, 512]\n",
            "Loading TF weight decoder/block_004/layer_000/SelfAttention/o_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_004/layer_000/SelfAttention/o_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_004/layer_000/SelfAttention/q with shape [512, 512]\n",
            "Loading TF weight decoder/block_004/layer_000/SelfAttention/q_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_004/layer_000/SelfAttention/q_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_004/layer_000/SelfAttention/v with shape [512, 512]\n",
            "Loading TF weight decoder/block_004/layer_000/SelfAttention/v_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_004/layer_000/SelfAttention/v_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_004/layer_000/layer_norm/scale with shape [512]\n",
            "Loading TF weight decoder/block_004/layer_000/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight decoder/block_004/layer_001/EncDecAttention/k with shape [512, 512]\n",
            "Loading TF weight decoder/block_004/layer_001/EncDecAttention/k_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_004/layer_001/EncDecAttention/k_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_004/layer_001/EncDecAttention/o with shape [512, 512]\n",
            "Loading TF weight decoder/block_004/layer_001/EncDecAttention/o_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_004/layer_001/EncDecAttention/o_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_004/layer_001/EncDecAttention/q with shape [512, 512]\n",
            "Loading TF weight decoder/block_004/layer_001/EncDecAttention/q_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_004/layer_001/EncDecAttention/q_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_004/layer_001/EncDecAttention/v with shape [512, 512]\n",
            "Loading TF weight decoder/block_004/layer_001/EncDecAttention/v_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_004/layer_001/EncDecAttention/v_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_004/layer_001/layer_norm/scale with shape [512]\n",
            "Loading TF weight decoder/block_004/layer_001/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight decoder/block_004/layer_002/DenseReluDense/wi/kernel with shape [512, 2048]\n",
            "Loading TF weight decoder/block_004/layer_002/DenseReluDense/wi/kernel_slot_vc with shape [2048]\n",
            "Loading TF weight decoder/block_004/layer_002/DenseReluDense/wi/kernel_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_004/layer_002/DenseReluDense/wo/kernel with shape [2048, 512]\n",
            "Loading TF weight decoder/block_004/layer_002/DenseReluDense/wo/kernel_slot_vc with shape [2048]\n",
            "Loading TF weight decoder/block_004/layer_002/DenseReluDense/wo/kernel_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_004/layer_002/layer_norm/scale with shape [512]\n",
            "Loading TF weight decoder/block_004/layer_002/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight decoder/block_005/layer_000/SelfAttention/k with shape [512, 512]\n",
            "Loading TF weight decoder/block_005/layer_000/SelfAttention/k_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_005/layer_000/SelfAttention/k_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_005/layer_000/SelfAttention/o with shape [512, 512]\n",
            "Loading TF weight decoder/block_005/layer_000/SelfAttention/o_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_005/layer_000/SelfAttention/o_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_005/layer_000/SelfAttention/q with shape [512, 512]\n",
            "Loading TF weight decoder/block_005/layer_000/SelfAttention/q_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_005/layer_000/SelfAttention/q_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_005/layer_000/SelfAttention/v with shape [512, 512]\n",
            "Loading TF weight decoder/block_005/layer_000/SelfAttention/v_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_005/layer_000/SelfAttention/v_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_005/layer_000/layer_norm/scale with shape [512]\n",
            "Loading TF weight decoder/block_005/layer_000/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight decoder/block_005/layer_001/EncDecAttention/k with shape [512, 512]\n",
            "Loading TF weight decoder/block_005/layer_001/EncDecAttention/k_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_005/layer_001/EncDecAttention/k_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_005/layer_001/EncDecAttention/o with shape [512, 512]\n",
            "Loading TF weight decoder/block_005/layer_001/EncDecAttention/o_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_005/layer_001/EncDecAttention/o_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_005/layer_001/EncDecAttention/q with shape [512, 512]\n",
            "Loading TF weight decoder/block_005/layer_001/EncDecAttention/q_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_005/layer_001/EncDecAttention/q_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_005/layer_001/EncDecAttention/v with shape [512, 512]\n",
            "Loading TF weight decoder/block_005/layer_001/EncDecAttention/v_slot_vc with shape [512]\n",
            "Loading TF weight decoder/block_005/layer_001/EncDecAttention/v_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_005/layer_001/layer_norm/scale with shape [512]\n",
            "Loading TF weight decoder/block_005/layer_001/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight decoder/block_005/layer_002/DenseReluDense/wi/kernel with shape [512, 2048]\n",
            "Loading TF weight decoder/block_005/layer_002/DenseReluDense/wi/kernel_slot_vc with shape [2048]\n",
            "Loading TF weight decoder/block_005/layer_002/DenseReluDense/wi/kernel_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_005/layer_002/DenseReluDense/wo/kernel with shape [2048, 512]\n",
            "Loading TF weight decoder/block_005/layer_002/DenseReluDense/wo/kernel_slot_vc with shape [2048]\n",
            "Loading TF weight decoder/block_005/layer_002/DenseReluDense/wo/kernel_slot_vr with shape [512]\n",
            "Loading TF weight decoder/block_005/layer_002/layer_norm/scale with shape [512]\n",
            "Loading TF weight decoder/block_005/layer_002/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight decoder/final_layer_norm/scale with shape [512]\n",
            "Loading TF weight decoder/final_layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight encoder/block_000/layer_000/SelfAttention/k with shape [512, 512]\n",
            "Loading TF weight encoder/block_000/layer_000/SelfAttention/k_slot_vc with shape [512]\n",
            "Loading TF weight encoder/block_000/layer_000/SelfAttention/k_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_000/layer_000/SelfAttention/o with shape [512, 512]\n",
            "Loading TF weight encoder/block_000/layer_000/SelfAttention/o_slot_vc with shape [512]\n",
            "Loading TF weight encoder/block_000/layer_000/SelfAttention/o_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_000/layer_000/SelfAttention/q with shape [512, 512]\n",
            "Loading TF weight encoder/block_000/layer_000/SelfAttention/q_slot_vc with shape [512]\n",
            "Loading TF weight encoder/block_000/layer_000/SelfAttention/q_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_000/layer_000/SelfAttention/relative_attention_bias with shape [8, 32]\n",
            "Loading TF weight encoder/block_000/layer_000/SelfAttention/relative_attention_bias_slot_v with shape [8, 32]\n",
            "Loading TF weight encoder/block_000/layer_000/SelfAttention/v with shape [512, 512]\n",
            "Loading TF weight encoder/block_000/layer_000/SelfAttention/v_slot_vc with shape [512]\n",
            "Loading TF weight encoder/block_000/layer_000/SelfAttention/v_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_000/layer_000/layer_norm/scale with shape [512]\n",
            "Loading TF weight encoder/block_000/layer_000/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight encoder/block_000/layer_001/DenseReluDense/wi/kernel with shape [512, 2048]\n",
            "Loading TF weight encoder/block_000/layer_001/DenseReluDense/wi/kernel_slot_vc with shape [2048]\n",
            "Loading TF weight encoder/block_000/layer_001/DenseReluDense/wi/kernel_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_000/layer_001/DenseReluDense/wo/kernel with shape [2048, 512]\n",
            "Loading TF weight encoder/block_000/layer_001/DenseReluDense/wo/kernel_slot_vc with shape [2048]\n",
            "Loading TF weight encoder/block_000/layer_001/DenseReluDense/wo/kernel_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_000/layer_001/layer_norm/scale with shape [512]\n",
            "Loading TF weight encoder/block_000/layer_001/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight encoder/block_001/layer_000/SelfAttention/k with shape [512, 512]\n",
            "Loading TF weight encoder/block_001/layer_000/SelfAttention/k_slot_vc with shape [512]\n",
            "Loading TF weight encoder/block_001/layer_000/SelfAttention/k_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_001/layer_000/SelfAttention/o with shape [512, 512]\n",
            "Loading TF weight encoder/block_001/layer_000/SelfAttention/o_slot_vc with shape [512]\n",
            "Loading TF weight encoder/block_001/layer_000/SelfAttention/o_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_001/layer_000/SelfAttention/q with shape [512, 512]\n",
            "Loading TF weight encoder/block_001/layer_000/SelfAttention/q_slot_vc with shape [512]\n",
            "Loading TF weight encoder/block_001/layer_000/SelfAttention/q_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_001/layer_000/SelfAttention/v with shape [512, 512]\n",
            "Loading TF weight encoder/block_001/layer_000/SelfAttention/v_slot_vc with shape [512]\n",
            "Loading TF weight encoder/block_001/layer_000/SelfAttention/v_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_001/layer_000/layer_norm/scale with shape [512]\n",
            "Loading TF weight encoder/block_001/layer_000/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight encoder/block_001/layer_001/DenseReluDense/wi/kernel with shape [512, 2048]\n",
            "Loading TF weight encoder/block_001/layer_001/DenseReluDense/wi/kernel_slot_vc with shape [2048]\n",
            "Loading TF weight encoder/block_001/layer_001/DenseReluDense/wi/kernel_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_001/layer_001/DenseReluDense/wo/kernel with shape [2048, 512]\n",
            "Loading TF weight encoder/block_001/layer_001/DenseReluDense/wo/kernel_slot_vc with shape [2048]\n",
            "Loading TF weight encoder/block_001/layer_001/DenseReluDense/wo/kernel_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_001/layer_001/layer_norm/scale with shape [512]\n",
            "Loading TF weight encoder/block_001/layer_001/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight encoder/block_002/layer_000/SelfAttention/k with shape [512, 512]\n",
            "Loading TF weight encoder/block_002/layer_000/SelfAttention/k_slot_vc with shape [512]\n",
            "Loading TF weight encoder/block_002/layer_000/SelfAttention/k_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_002/layer_000/SelfAttention/o with shape [512, 512]\n",
            "Loading TF weight encoder/block_002/layer_000/SelfAttention/o_slot_vc with shape [512]\n",
            "Loading TF weight encoder/block_002/layer_000/SelfAttention/o_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_002/layer_000/SelfAttention/q with shape [512, 512]\n",
            "Loading TF weight encoder/block_002/layer_000/SelfAttention/q_slot_vc with shape [512]\n",
            "Loading TF weight encoder/block_002/layer_000/SelfAttention/q_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_002/layer_000/SelfAttention/v with shape [512, 512]\n",
            "Loading TF weight encoder/block_002/layer_000/SelfAttention/v_slot_vc with shape [512]\n",
            "Loading TF weight encoder/block_002/layer_000/SelfAttention/v_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_002/layer_000/layer_norm/scale with shape [512]\n",
            "Loading TF weight encoder/block_002/layer_000/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight encoder/block_002/layer_001/DenseReluDense/wi/kernel with shape [512, 2048]\n",
            "Loading TF weight encoder/block_002/layer_001/DenseReluDense/wi/kernel_slot_vc with shape [2048]\n",
            "Loading TF weight encoder/block_002/layer_001/DenseReluDense/wi/kernel_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_002/layer_001/DenseReluDense/wo/kernel with shape [2048, 512]\n",
            "Loading TF weight encoder/block_002/layer_001/DenseReluDense/wo/kernel_slot_vc with shape [2048]\n",
            "Loading TF weight encoder/block_002/layer_001/DenseReluDense/wo/kernel_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_002/layer_001/layer_norm/scale with shape [512]\n",
            "Loading TF weight encoder/block_002/layer_001/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight encoder/block_003/layer_000/SelfAttention/k with shape [512, 512]\n",
            "Loading TF weight encoder/block_003/layer_000/SelfAttention/k_slot_vc with shape [512]\n",
            "Loading TF weight encoder/block_003/layer_000/SelfAttention/k_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_003/layer_000/SelfAttention/o with shape [512, 512]\n",
            "Loading TF weight encoder/block_003/layer_000/SelfAttention/o_slot_vc with shape [512]\n",
            "Loading TF weight encoder/block_003/layer_000/SelfAttention/o_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_003/layer_000/SelfAttention/q with shape [512, 512]\n",
            "Loading TF weight encoder/block_003/layer_000/SelfAttention/q_slot_vc with shape [512]\n",
            "Loading TF weight encoder/block_003/layer_000/SelfAttention/q_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_003/layer_000/SelfAttention/v with shape [512, 512]\n",
            "Loading TF weight encoder/block_003/layer_000/SelfAttention/v_slot_vc with shape [512]\n",
            "Loading TF weight encoder/block_003/layer_000/SelfAttention/v_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_003/layer_000/layer_norm/scale with shape [512]\n",
            "Loading TF weight encoder/block_003/layer_000/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight encoder/block_003/layer_001/DenseReluDense/wi/kernel with shape [512, 2048]\n",
            "Loading TF weight encoder/block_003/layer_001/DenseReluDense/wi/kernel_slot_vc with shape [2048]\n",
            "Loading TF weight encoder/block_003/layer_001/DenseReluDense/wi/kernel_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_003/layer_001/DenseReluDense/wo/kernel with shape [2048, 512]\n",
            "Loading TF weight encoder/block_003/layer_001/DenseReluDense/wo/kernel_slot_vc with shape [2048]\n",
            "Loading TF weight encoder/block_003/layer_001/DenseReluDense/wo/kernel_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_003/layer_001/layer_norm/scale with shape [512]\n",
            "Loading TF weight encoder/block_003/layer_001/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight encoder/block_004/layer_000/SelfAttention/k with shape [512, 512]\n",
            "Loading TF weight encoder/block_004/layer_000/SelfAttention/k_slot_vc with shape [512]\n",
            "Loading TF weight encoder/block_004/layer_000/SelfAttention/k_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_004/layer_000/SelfAttention/o with shape [512, 512]\n",
            "Loading TF weight encoder/block_004/layer_000/SelfAttention/o_slot_vc with shape [512]\n",
            "Loading TF weight encoder/block_004/layer_000/SelfAttention/o_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_004/layer_000/SelfAttention/q with shape [512, 512]\n",
            "Loading TF weight encoder/block_004/layer_000/SelfAttention/q_slot_vc with shape [512]\n",
            "Loading TF weight encoder/block_004/layer_000/SelfAttention/q_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_004/layer_000/SelfAttention/v with shape [512, 512]\n",
            "Loading TF weight encoder/block_004/layer_000/SelfAttention/v_slot_vc with shape [512]\n",
            "Loading TF weight encoder/block_004/layer_000/SelfAttention/v_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_004/layer_000/layer_norm/scale with shape [512]\n",
            "Loading TF weight encoder/block_004/layer_000/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight encoder/block_004/layer_001/DenseReluDense/wi/kernel with shape [512, 2048]\n",
            "Loading TF weight encoder/block_004/layer_001/DenseReluDense/wi/kernel_slot_vc with shape [2048]\n",
            "Loading TF weight encoder/block_004/layer_001/DenseReluDense/wi/kernel_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_004/layer_001/DenseReluDense/wo/kernel with shape [2048, 512]\n",
            "Loading TF weight encoder/block_004/layer_001/DenseReluDense/wo/kernel_slot_vc with shape [2048]\n",
            "Loading TF weight encoder/block_004/layer_001/DenseReluDense/wo/kernel_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_004/layer_001/layer_norm/scale with shape [512]\n",
            "Loading TF weight encoder/block_004/layer_001/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight encoder/block_005/layer_000/SelfAttention/k with shape [512, 512]\n",
            "Loading TF weight encoder/block_005/layer_000/SelfAttention/k_slot_vc with shape [512]\n",
            "Loading TF weight encoder/block_005/layer_000/SelfAttention/k_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_005/layer_000/SelfAttention/o with shape [512, 512]\n",
            "Loading TF weight encoder/block_005/layer_000/SelfAttention/o_slot_vc with shape [512]\n",
            "Loading TF weight encoder/block_005/layer_000/SelfAttention/o_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_005/layer_000/SelfAttention/q with shape [512, 512]\n",
            "Loading TF weight encoder/block_005/layer_000/SelfAttention/q_slot_vc with shape [512]\n",
            "Loading TF weight encoder/block_005/layer_000/SelfAttention/q_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_005/layer_000/SelfAttention/v with shape [512, 512]\n",
            "Loading TF weight encoder/block_005/layer_000/SelfAttention/v_slot_vc with shape [512]\n",
            "Loading TF weight encoder/block_005/layer_000/SelfAttention/v_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_005/layer_000/layer_norm/scale with shape [512]\n",
            "Loading TF weight encoder/block_005/layer_000/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight encoder/block_005/layer_001/DenseReluDense/wi/kernel with shape [512, 2048]\n",
            "Loading TF weight encoder/block_005/layer_001/DenseReluDense/wi/kernel_slot_vc with shape [2048]\n",
            "Loading TF weight encoder/block_005/layer_001/DenseReluDense/wi/kernel_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_005/layer_001/DenseReluDense/wo/kernel with shape [2048, 512]\n",
            "Loading TF weight encoder/block_005/layer_001/DenseReluDense/wo/kernel_slot_vc with shape [2048]\n",
            "Loading TF weight encoder/block_005/layer_001/DenseReluDense/wo/kernel_slot_vr with shape [512]\n",
            "Loading TF weight encoder/block_005/layer_001/layer_norm/scale with shape [512]\n",
            "Loading TF weight encoder/block_005/layer_001/layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight encoder/final_layer_norm/scale with shape [512]\n",
            "Loading TF weight encoder/final_layer_norm/scale_slot_v with shape [512]\n",
            "Loading TF weight global_step with shape []\n",
            "Loading TF weight shared/embedding with shape [32128, 512]\n",
            "Loading TF weight shared/embedding_slot_vc with shape [32128]\n",
            "Loading TF weight shared/embedding_slot_vr with shape [512]\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_000', 'layer_000', 'SelfAttention', 'k']\n",
            "Initialize PyTorch weight ['decoder', 'block_000', 'layer_000', 'SelfAttention', 'k']\n",
            "Skipping decoder/block_000/layer_000/SelfAttention/k_slot_vc\n",
            "Skipping decoder/block_000/layer_000/SelfAttention/k_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_000', 'layer_000', 'SelfAttention', 'o']\n",
            "Initialize PyTorch weight ['decoder', 'block_000', 'layer_000', 'SelfAttention', 'o']\n",
            "Skipping decoder/block_000/layer_000/SelfAttention/o_slot_vc\n",
            "Skipping decoder/block_000/layer_000/SelfAttention/o_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_000', 'layer_000', 'SelfAttention', 'q']\n",
            "Initialize PyTorch weight ['decoder', 'block_000', 'layer_000', 'SelfAttention', 'q']\n",
            "Skipping decoder/block_000/layer_000/SelfAttention/q_slot_vc\n",
            "Skipping decoder/block_000/layer_000/SelfAttention/q_slot_vr\n",
            "Transposing numpy weight of shape (8, 32) for ['decoder', 'block_000', 'layer_000', 'SelfAttention', 'relative_attention_bias']\n",
            "Initialize PyTorch weight ['decoder', 'block_000', 'layer_000', 'SelfAttention', 'relative_attention_bias']\n",
            "Skipping decoder/block_000/layer_000/SelfAttention/relative_attention_bias_slot_v\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_000', 'layer_000', 'SelfAttention', 'v']\n",
            "Initialize PyTorch weight ['decoder', 'block_000', 'layer_000', 'SelfAttention', 'v']\n",
            "Skipping decoder/block_000/layer_000/SelfAttention/v_slot_vc\n",
            "Skipping decoder/block_000/layer_000/SelfAttention/v_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['decoder', 'block_000', 'layer_000', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['decoder', 'block_000', 'layer_000', 'layer_norm', 'scale']\n",
            "Skipping decoder/block_000/layer_000/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_000', 'layer_001', 'EncDecAttention', 'k']\n",
            "Initialize PyTorch weight ['decoder', 'block_000', 'layer_001', 'EncDecAttention', 'k']\n",
            "Skipping decoder/block_000/layer_001/EncDecAttention/k_slot_vc\n",
            "Skipping decoder/block_000/layer_001/EncDecAttention/k_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_000', 'layer_001', 'EncDecAttention', 'o']\n",
            "Initialize PyTorch weight ['decoder', 'block_000', 'layer_001', 'EncDecAttention', 'o']\n",
            "Skipping decoder/block_000/layer_001/EncDecAttention/o_slot_vc\n",
            "Skipping decoder/block_000/layer_001/EncDecAttention/o_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_000', 'layer_001', 'EncDecAttention', 'q']\n",
            "Initialize PyTorch weight ['decoder', 'block_000', 'layer_001', 'EncDecAttention', 'q']\n",
            "Skipping decoder/block_000/layer_001/EncDecAttention/q_slot_vc\n",
            "Skipping decoder/block_000/layer_001/EncDecAttention/q_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_000', 'layer_001', 'EncDecAttention', 'v']\n",
            "Initialize PyTorch weight ['decoder', 'block_000', 'layer_001', 'EncDecAttention', 'v']\n",
            "Skipping decoder/block_000/layer_001/EncDecAttention/v_slot_vc\n",
            "Skipping decoder/block_000/layer_001/EncDecAttention/v_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['decoder', 'block_000', 'layer_001', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['decoder', 'block_000', 'layer_001', 'layer_norm', 'scale']\n",
            "Skipping decoder/block_000/layer_001/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 2048) for ['decoder', 'block_000', 'layer_002', 'DenseReluDense', 'wi', 'kernel']\n",
            "Initialize PyTorch weight ['decoder', 'block_000', 'layer_002', 'DenseReluDense', 'wi', 'kernel']\n",
            "Skipping decoder/block_000/layer_002/DenseReluDense/wi/kernel_slot_vc\n",
            "Skipping decoder/block_000/layer_002/DenseReluDense/wi/kernel_slot_vr\n",
            "Transposing numpy weight of shape (2048, 512) for ['decoder', 'block_000', 'layer_002', 'DenseReluDense', 'wo', 'kernel']\n",
            "Initialize PyTorch weight ['decoder', 'block_000', 'layer_002', 'DenseReluDense', 'wo', 'kernel']\n",
            "Skipping decoder/block_000/layer_002/DenseReluDense/wo/kernel_slot_vc\n",
            "Skipping decoder/block_000/layer_002/DenseReluDense/wo/kernel_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['decoder', 'block_000', 'layer_002', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['decoder', 'block_000', 'layer_002', 'layer_norm', 'scale']\n",
            "Skipping decoder/block_000/layer_002/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_001', 'layer_000', 'SelfAttention', 'k']\n",
            "Initialize PyTorch weight ['decoder', 'block_001', 'layer_000', 'SelfAttention', 'k']\n",
            "Skipping decoder/block_001/layer_000/SelfAttention/k_slot_vc\n",
            "Skipping decoder/block_001/layer_000/SelfAttention/k_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_001', 'layer_000', 'SelfAttention', 'o']\n",
            "Initialize PyTorch weight ['decoder', 'block_001', 'layer_000', 'SelfAttention', 'o']\n",
            "Skipping decoder/block_001/layer_000/SelfAttention/o_slot_vc\n",
            "Skipping decoder/block_001/layer_000/SelfAttention/o_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_001', 'layer_000', 'SelfAttention', 'q']\n",
            "Initialize PyTorch weight ['decoder', 'block_001', 'layer_000', 'SelfAttention', 'q']\n",
            "Skipping decoder/block_001/layer_000/SelfAttention/q_slot_vc\n",
            "Skipping decoder/block_001/layer_000/SelfAttention/q_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_001', 'layer_000', 'SelfAttention', 'v']\n",
            "Initialize PyTorch weight ['decoder', 'block_001', 'layer_000', 'SelfAttention', 'v']\n",
            "Skipping decoder/block_001/layer_000/SelfAttention/v_slot_vc\n",
            "Skipping decoder/block_001/layer_000/SelfAttention/v_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['decoder', 'block_001', 'layer_000', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['decoder', 'block_001', 'layer_000', 'layer_norm', 'scale']\n",
            "Skipping decoder/block_001/layer_000/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_001', 'layer_001', 'EncDecAttention', 'k']\n",
            "Initialize PyTorch weight ['decoder', 'block_001', 'layer_001', 'EncDecAttention', 'k']\n",
            "Skipping decoder/block_001/layer_001/EncDecAttention/k_slot_vc\n",
            "Skipping decoder/block_001/layer_001/EncDecAttention/k_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_001', 'layer_001', 'EncDecAttention', 'o']\n",
            "Initialize PyTorch weight ['decoder', 'block_001', 'layer_001', 'EncDecAttention', 'o']\n",
            "Skipping decoder/block_001/layer_001/EncDecAttention/o_slot_vc\n",
            "Skipping decoder/block_001/layer_001/EncDecAttention/o_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_001', 'layer_001', 'EncDecAttention', 'q']\n",
            "Initialize PyTorch weight ['decoder', 'block_001', 'layer_001', 'EncDecAttention', 'q']\n",
            "Skipping decoder/block_001/layer_001/EncDecAttention/q_slot_vc\n",
            "Skipping decoder/block_001/layer_001/EncDecAttention/q_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_001', 'layer_001', 'EncDecAttention', 'v']\n",
            "Initialize PyTorch weight ['decoder', 'block_001', 'layer_001', 'EncDecAttention', 'v']\n",
            "Skipping decoder/block_001/layer_001/EncDecAttention/v_slot_vc\n",
            "Skipping decoder/block_001/layer_001/EncDecAttention/v_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['decoder', 'block_001', 'layer_001', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['decoder', 'block_001', 'layer_001', 'layer_norm', 'scale']\n",
            "Skipping decoder/block_001/layer_001/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 2048) for ['decoder', 'block_001', 'layer_002', 'DenseReluDense', 'wi', 'kernel']\n",
            "Initialize PyTorch weight ['decoder', 'block_001', 'layer_002', 'DenseReluDense', 'wi', 'kernel']\n",
            "Skipping decoder/block_001/layer_002/DenseReluDense/wi/kernel_slot_vc\n",
            "Skipping decoder/block_001/layer_002/DenseReluDense/wi/kernel_slot_vr\n",
            "Transposing numpy weight of shape (2048, 512) for ['decoder', 'block_001', 'layer_002', 'DenseReluDense', 'wo', 'kernel']\n",
            "Initialize PyTorch weight ['decoder', 'block_001', 'layer_002', 'DenseReluDense', 'wo', 'kernel']\n",
            "Skipping decoder/block_001/layer_002/DenseReluDense/wo/kernel_slot_vc\n",
            "Skipping decoder/block_001/layer_002/DenseReluDense/wo/kernel_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['decoder', 'block_001', 'layer_002', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['decoder', 'block_001', 'layer_002', 'layer_norm', 'scale']\n",
            "Skipping decoder/block_001/layer_002/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_002', 'layer_000', 'SelfAttention', 'k']\n",
            "Initialize PyTorch weight ['decoder', 'block_002', 'layer_000', 'SelfAttention', 'k']\n",
            "Skipping decoder/block_002/layer_000/SelfAttention/k_slot_vc\n",
            "Skipping decoder/block_002/layer_000/SelfAttention/k_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_002', 'layer_000', 'SelfAttention', 'o']\n",
            "Initialize PyTorch weight ['decoder', 'block_002', 'layer_000', 'SelfAttention', 'o']\n",
            "Skipping decoder/block_002/layer_000/SelfAttention/o_slot_vc\n",
            "Skipping decoder/block_002/layer_000/SelfAttention/o_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_002', 'layer_000', 'SelfAttention', 'q']\n",
            "Initialize PyTorch weight ['decoder', 'block_002', 'layer_000', 'SelfAttention', 'q']\n",
            "Skipping decoder/block_002/layer_000/SelfAttention/q_slot_vc\n",
            "Skipping decoder/block_002/layer_000/SelfAttention/q_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_002', 'layer_000', 'SelfAttention', 'v']\n",
            "Initialize PyTorch weight ['decoder', 'block_002', 'layer_000', 'SelfAttention', 'v']\n",
            "Skipping decoder/block_002/layer_000/SelfAttention/v_slot_vc\n",
            "Skipping decoder/block_002/layer_000/SelfAttention/v_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['decoder', 'block_002', 'layer_000', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['decoder', 'block_002', 'layer_000', 'layer_norm', 'scale']\n",
            "Skipping decoder/block_002/layer_000/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_002', 'layer_001', 'EncDecAttention', 'k']\n",
            "Initialize PyTorch weight ['decoder', 'block_002', 'layer_001', 'EncDecAttention', 'k']\n",
            "Skipping decoder/block_002/layer_001/EncDecAttention/k_slot_vc\n",
            "Skipping decoder/block_002/layer_001/EncDecAttention/k_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_002', 'layer_001', 'EncDecAttention', 'o']\n",
            "Initialize PyTorch weight ['decoder', 'block_002', 'layer_001', 'EncDecAttention', 'o']\n",
            "Skipping decoder/block_002/layer_001/EncDecAttention/o_slot_vc\n",
            "Skipping decoder/block_002/layer_001/EncDecAttention/o_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_002', 'layer_001', 'EncDecAttention', 'q']\n",
            "Initialize PyTorch weight ['decoder', 'block_002', 'layer_001', 'EncDecAttention', 'q']\n",
            "Skipping decoder/block_002/layer_001/EncDecAttention/q_slot_vc\n",
            "Skipping decoder/block_002/layer_001/EncDecAttention/q_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_002', 'layer_001', 'EncDecAttention', 'v']\n",
            "Initialize PyTorch weight ['decoder', 'block_002', 'layer_001', 'EncDecAttention', 'v']\n",
            "Skipping decoder/block_002/layer_001/EncDecAttention/v_slot_vc\n",
            "Skipping decoder/block_002/layer_001/EncDecAttention/v_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['decoder', 'block_002', 'layer_001', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['decoder', 'block_002', 'layer_001', 'layer_norm', 'scale']\n",
            "Skipping decoder/block_002/layer_001/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 2048) for ['decoder', 'block_002', 'layer_002', 'DenseReluDense', 'wi', 'kernel']\n",
            "Initialize PyTorch weight ['decoder', 'block_002', 'layer_002', 'DenseReluDense', 'wi', 'kernel']\n",
            "Skipping decoder/block_002/layer_002/DenseReluDense/wi/kernel_slot_vc\n",
            "Skipping decoder/block_002/layer_002/DenseReluDense/wi/kernel_slot_vr\n",
            "Transposing numpy weight of shape (2048, 512) for ['decoder', 'block_002', 'layer_002', 'DenseReluDense', 'wo', 'kernel']\n",
            "Initialize PyTorch weight ['decoder', 'block_002', 'layer_002', 'DenseReluDense', 'wo', 'kernel']\n",
            "Skipping decoder/block_002/layer_002/DenseReluDense/wo/kernel_slot_vc\n",
            "Skipping decoder/block_002/layer_002/DenseReluDense/wo/kernel_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['decoder', 'block_002', 'layer_002', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['decoder', 'block_002', 'layer_002', 'layer_norm', 'scale']\n",
            "Skipping decoder/block_002/layer_002/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_003', 'layer_000', 'SelfAttention', 'k']\n",
            "Initialize PyTorch weight ['decoder', 'block_003', 'layer_000', 'SelfAttention', 'k']\n",
            "Skipping decoder/block_003/layer_000/SelfAttention/k_slot_vc\n",
            "Skipping decoder/block_003/layer_000/SelfAttention/k_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_003', 'layer_000', 'SelfAttention', 'o']\n",
            "Initialize PyTorch weight ['decoder', 'block_003', 'layer_000', 'SelfAttention', 'o']\n",
            "Skipping decoder/block_003/layer_000/SelfAttention/o_slot_vc\n",
            "Skipping decoder/block_003/layer_000/SelfAttention/o_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_003', 'layer_000', 'SelfAttention', 'q']\n",
            "Initialize PyTorch weight ['decoder', 'block_003', 'layer_000', 'SelfAttention', 'q']\n",
            "Skipping decoder/block_003/layer_000/SelfAttention/q_slot_vc\n",
            "Skipping decoder/block_003/layer_000/SelfAttention/q_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_003', 'layer_000', 'SelfAttention', 'v']\n",
            "Initialize PyTorch weight ['decoder', 'block_003', 'layer_000', 'SelfAttention', 'v']\n",
            "Skipping decoder/block_003/layer_000/SelfAttention/v_slot_vc\n",
            "Skipping decoder/block_003/layer_000/SelfAttention/v_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['decoder', 'block_003', 'layer_000', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['decoder', 'block_003', 'layer_000', 'layer_norm', 'scale']\n",
            "Skipping decoder/block_003/layer_000/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_003', 'layer_001', 'EncDecAttention', 'k']\n",
            "Initialize PyTorch weight ['decoder', 'block_003', 'layer_001', 'EncDecAttention', 'k']\n",
            "Skipping decoder/block_003/layer_001/EncDecAttention/k_slot_vc\n",
            "Skipping decoder/block_003/layer_001/EncDecAttention/k_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_003', 'layer_001', 'EncDecAttention', 'o']\n",
            "Initialize PyTorch weight ['decoder', 'block_003', 'layer_001', 'EncDecAttention', 'o']\n",
            "Skipping decoder/block_003/layer_001/EncDecAttention/o_slot_vc\n",
            "Skipping decoder/block_003/layer_001/EncDecAttention/o_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_003', 'layer_001', 'EncDecAttention', 'q']\n",
            "Initialize PyTorch weight ['decoder', 'block_003', 'layer_001', 'EncDecAttention', 'q']\n",
            "Skipping decoder/block_003/layer_001/EncDecAttention/q_slot_vc\n",
            "Skipping decoder/block_003/layer_001/EncDecAttention/q_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_003', 'layer_001', 'EncDecAttention', 'v']\n",
            "Initialize PyTorch weight ['decoder', 'block_003', 'layer_001', 'EncDecAttention', 'v']\n",
            "Skipping decoder/block_003/layer_001/EncDecAttention/v_slot_vc\n",
            "Skipping decoder/block_003/layer_001/EncDecAttention/v_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['decoder', 'block_003', 'layer_001', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['decoder', 'block_003', 'layer_001', 'layer_norm', 'scale']\n",
            "Skipping decoder/block_003/layer_001/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 2048) for ['decoder', 'block_003', 'layer_002', 'DenseReluDense', 'wi', 'kernel']\n",
            "Initialize PyTorch weight ['decoder', 'block_003', 'layer_002', 'DenseReluDense', 'wi', 'kernel']\n",
            "Skipping decoder/block_003/layer_002/DenseReluDense/wi/kernel_slot_vc\n",
            "Skipping decoder/block_003/layer_002/DenseReluDense/wi/kernel_slot_vr\n",
            "Transposing numpy weight of shape (2048, 512) for ['decoder', 'block_003', 'layer_002', 'DenseReluDense', 'wo', 'kernel']\n",
            "Initialize PyTorch weight ['decoder', 'block_003', 'layer_002', 'DenseReluDense', 'wo', 'kernel']\n",
            "Skipping decoder/block_003/layer_002/DenseReluDense/wo/kernel_slot_vc\n",
            "Skipping decoder/block_003/layer_002/DenseReluDense/wo/kernel_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['decoder', 'block_003', 'layer_002', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['decoder', 'block_003', 'layer_002', 'layer_norm', 'scale']\n",
            "Skipping decoder/block_003/layer_002/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_004', 'layer_000', 'SelfAttention', 'k']\n",
            "Initialize PyTorch weight ['decoder', 'block_004', 'layer_000', 'SelfAttention', 'k']\n",
            "Skipping decoder/block_004/layer_000/SelfAttention/k_slot_vc\n",
            "Skipping decoder/block_004/layer_000/SelfAttention/k_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_004', 'layer_000', 'SelfAttention', 'o']\n",
            "Initialize PyTorch weight ['decoder', 'block_004', 'layer_000', 'SelfAttention', 'o']\n",
            "Skipping decoder/block_004/layer_000/SelfAttention/o_slot_vc\n",
            "Skipping decoder/block_004/layer_000/SelfAttention/o_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_004', 'layer_000', 'SelfAttention', 'q']\n",
            "Initialize PyTorch weight ['decoder', 'block_004', 'layer_000', 'SelfAttention', 'q']\n",
            "Skipping decoder/block_004/layer_000/SelfAttention/q_slot_vc\n",
            "Skipping decoder/block_004/layer_000/SelfAttention/q_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_004', 'layer_000', 'SelfAttention', 'v']\n",
            "Initialize PyTorch weight ['decoder', 'block_004', 'layer_000', 'SelfAttention', 'v']\n",
            "Skipping decoder/block_004/layer_000/SelfAttention/v_slot_vc\n",
            "Skipping decoder/block_004/layer_000/SelfAttention/v_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['decoder', 'block_004', 'layer_000', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['decoder', 'block_004', 'layer_000', 'layer_norm', 'scale']\n",
            "Skipping decoder/block_004/layer_000/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_004', 'layer_001', 'EncDecAttention', 'k']\n",
            "Initialize PyTorch weight ['decoder', 'block_004', 'layer_001', 'EncDecAttention', 'k']\n",
            "Skipping decoder/block_004/layer_001/EncDecAttention/k_slot_vc\n",
            "Skipping decoder/block_004/layer_001/EncDecAttention/k_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_004', 'layer_001', 'EncDecAttention', 'o']\n",
            "Initialize PyTorch weight ['decoder', 'block_004', 'layer_001', 'EncDecAttention', 'o']\n",
            "Skipping decoder/block_004/layer_001/EncDecAttention/o_slot_vc\n",
            "Skipping decoder/block_004/layer_001/EncDecAttention/o_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_004', 'layer_001', 'EncDecAttention', 'q']\n",
            "Initialize PyTorch weight ['decoder', 'block_004', 'layer_001', 'EncDecAttention', 'q']\n",
            "Skipping decoder/block_004/layer_001/EncDecAttention/q_slot_vc\n",
            "Skipping decoder/block_004/layer_001/EncDecAttention/q_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_004', 'layer_001', 'EncDecAttention', 'v']\n",
            "Initialize PyTorch weight ['decoder', 'block_004', 'layer_001', 'EncDecAttention', 'v']\n",
            "Skipping decoder/block_004/layer_001/EncDecAttention/v_slot_vc\n",
            "Skipping decoder/block_004/layer_001/EncDecAttention/v_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['decoder', 'block_004', 'layer_001', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['decoder', 'block_004', 'layer_001', 'layer_norm', 'scale']\n",
            "Skipping decoder/block_004/layer_001/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 2048) for ['decoder', 'block_004', 'layer_002', 'DenseReluDense', 'wi', 'kernel']\n",
            "Initialize PyTorch weight ['decoder', 'block_004', 'layer_002', 'DenseReluDense', 'wi', 'kernel']\n",
            "Skipping decoder/block_004/layer_002/DenseReluDense/wi/kernel_slot_vc\n",
            "Skipping decoder/block_004/layer_002/DenseReluDense/wi/kernel_slot_vr\n",
            "Transposing numpy weight of shape (2048, 512) for ['decoder', 'block_004', 'layer_002', 'DenseReluDense', 'wo', 'kernel']\n",
            "Initialize PyTorch weight ['decoder', 'block_004', 'layer_002', 'DenseReluDense', 'wo', 'kernel']\n",
            "Skipping decoder/block_004/layer_002/DenseReluDense/wo/kernel_slot_vc\n",
            "Skipping decoder/block_004/layer_002/DenseReluDense/wo/kernel_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['decoder', 'block_004', 'layer_002', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['decoder', 'block_004', 'layer_002', 'layer_norm', 'scale']\n",
            "Skipping decoder/block_004/layer_002/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_005', 'layer_000', 'SelfAttention', 'k']\n",
            "Initialize PyTorch weight ['decoder', 'block_005', 'layer_000', 'SelfAttention', 'k']\n",
            "Skipping decoder/block_005/layer_000/SelfAttention/k_slot_vc\n",
            "Skipping decoder/block_005/layer_000/SelfAttention/k_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_005', 'layer_000', 'SelfAttention', 'o']\n",
            "Initialize PyTorch weight ['decoder', 'block_005', 'layer_000', 'SelfAttention', 'o']\n",
            "Skipping decoder/block_005/layer_000/SelfAttention/o_slot_vc\n",
            "Skipping decoder/block_005/layer_000/SelfAttention/o_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_005', 'layer_000', 'SelfAttention', 'q']\n",
            "Initialize PyTorch weight ['decoder', 'block_005', 'layer_000', 'SelfAttention', 'q']\n",
            "Skipping decoder/block_005/layer_000/SelfAttention/q_slot_vc\n",
            "Skipping decoder/block_005/layer_000/SelfAttention/q_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_005', 'layer_000', 'SelfAttention', 'v']\n",
            "Initialize PyTorch weight ['decoder', 'block_005', 'layer_000', 'SelfAttention', 'v']\n",
            "Skipping decoder/block_005/layer_000/SelfAttention/v_slot_vc\n",
            "Skipping decoder/block_005/layer_000/SelfAttention/v_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['decoder', 'block_005', 'layer_000', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['decoder', 'block_005', 'layer_000', 'layer_norm', 'scale']\n",
            "Skipping decoder/block_005/layer_000/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_005', 'layer_001', 'EncDecAttention', 'k']\n",
            "Initialize PyTorch weight ['decoder', 'block_005', 'layer_001', 'EncDecAttention', 'k']\n",
            "Skipping decoder/block_005/layer_001/EncDecAttention/k_slot_vc\n",
            "Skipping decoder/block_005/layer_001/EncDecAttention/k_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_005', 'layer_001', 'EncDecAttention', 'o']\n",
            "Initialize PyTorch weight ['decoder', 'block_005', 'layer_001', 'EncDecAttention', 'o']\n",
            "Skipping decoder/block_005/layer_001/EncDecAttention/o_slot_vc\n",
            "Skipping decoder/block_005/layer_001/EncDecAttention/o_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_005', 'layer_001', 'EncDecAttention', 'q']\n",
            "Initialize PyTorch weight ['decoder', 'block_005', 'layer_001', 'EncDecAttention', 'q']\n",
            "Skipping decoder/block_005/layer_001/EncDecAttention/q_slot_vc\n",
            "Skipping decoder/block_005/layer_001/EncDecAttention/q_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['decoder', 'block_005', 'layer_001', 'EncDecAttention', 'v']\n",
            "Initialize PyTorch weight ['decoder', 'block_005', 'layer_001', 'EncDecAttention', 'v']\n",
            "Skipping decoder/block_005/layer_001/EncDecAttention/v_slot_vc\n",
            "Skipping decoder/block_005/layer_001/EncDecAttention/v_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['decoder', 'block_005', 'layer_001', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['decoder', 'block_005', 'layer_001', 'layer_norm', 'scale']\n",
            "Skipping decoder/block_005/layer_001/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 2048) for ['decoder', 'block_005', 'layer_002', 'DenseReluDense', 'wi', 'kernel']\n",
            "Initialize PyTorch weight ['decoder', 'block_005', 'layer_002', 'DenseReluDense', 'wi', 'kernel']\n",
            "Skipping decoder/block_005/layer_002/DenseReluDense/wi/kernel_slot_vc\n",
            "Skipping decoder/block_005/layer_002/DenseReluDense/wi/kernel_slot_vr\n",
            "Transposing numpy weight of shape (2048, 512) for ['decoder', 'block_005', 'layer_002', 'DenseReluDense', 'wo', 'kernel']\n",
            "Initialize PyTorch weight ['decoder', 'block_005', 'layer_002', 'DenseReluDense', 'wo', 'kernel']\n",
            "Skipping decoder/block_005/layer_002/DenseReluDense/wo/kernel_slot_vc\n",
            "Skipping decoder/block_005/layer_002/DenseReluDense/wo/kernel_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['decoder', 'block_005', 'layer_002', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['decoder', 'block_005', 'layer_002', 'layer_norm', 'scale']\n",
            "Skipping decoder/block_005/layer_002/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512,) for ['decoder', 'final_layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['decoder', 'final_layer_norm', 'scale']\n",
            "Skipping decoder/final_layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 512) for ['encoder', 'block_000', 'layer_000', 'SelfAttention', 'k']\n",
            "Initialize PyTorch weight ['encoder', 'block_000', 'layer_000', 'SelfAttention', 'k']\n",
            "Skipping encoder/block_000/layer_000/SelfAttention/k_slot_vc\n",
            "Skipping encoder/block_000/layer_000/SelfAttention/k_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['encoder', 'block_000', 'layer_000', 'SelfAttention', 'o']\n",
            "Initialize PyTorch weight ['encoder', 'block_000', 'layer_000', 'SelfAttention', 'o']\n",
            "Skipping encoder/block_000/layer_000/SelfAttention/o_slot_vc\n",
            "Skipping encoder/block_000/layer_000/SelfAttention/o_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['encoder', 'block_000', 'layer_000', 'SelfAttention', 'q']\n",
            "Initialize PyTorch weight ['encoder', 'block_000', 'layer_000', 'SelfAttention', 'q']\n",
            "Skipping encoder/block_000/layer_000/SelfAttention/q_slot_vc\n",
            "Skipping encoder/block_000/layer_000/SelfAttention/q_slot_vr\n",
            "Transposing numpy weight of shape (8, 32) for ['encoder', 'block_000', 'layer_000', 'SelfAttention', 'relative_attention_bias']\n",
            "Initialize PyTorch weight ['encoder', 'block_000', 'layer_000', 'SelfAttention', 'relative_attention_bias']\n",
            "Skipping encoder/block_000/layer_000/SelfAttention/relative_attention_bias_slot_v\n",
            "Transposing numpy weight of shape (512, 512) for ['encoder', 'block_000', 'layer_000', 'SelfAttention', 'v']\n",
            "Initialize PyTorch weight ['encoder', 'block_000', 'layer_000', 'SelfAttention', 'v']\n",
            "Skipping encoder/block_000/layer_000/SelfAttention/v_slot_vc\n",
            "Skipping encoder/block_000/layer_000/SelfAttention/v_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['encoder', 'block_000', 'layer_000', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['encoder', 'block_000', 'layer_000', 'layer_norm', 'scale']\n",
            "Skipping encoder/block_000/layer_000/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 2048) for ['encoder', 'block_000', 'layer_001', 'DenseReluDense', 'wi', 'kernel']\n",
            "Initialize PyTorch weight ['encoder', 'block_000', 'layer_001', 'DenseReluDense', 'wi', 'kernel']\n",
            "Skipping encoder/block_000/layer_001/DenseReluDense/wi/kernel_slot_vc\n",
            "Skipping encoder/block_000/layer_001/DenseReluDense/wi/kernel_slot_vr\n",
            "Transposing numpy weight of shape (2048, 512) for ['encoder', 'block_000', 'layer_001', 'DenseReluDense', 'wo', 'kernel']\n",
            "Initialize PyTorch weight ['encoder', 'block_000', 'layer_001', 'DenseReluDense', 'wo', 'kernel']\n",
            "Skipping encoder/block_000/layer_001/DenseReluDense/wo/kernel_slot_vc\n",
            "Skipping encoder/block_000/layer_001/DenseReluDense/wo/kernel_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['encoder', 'block_000', 'layer_001', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['encoder', 'block_000', 'layer_001', 'layer_norm', 'scale']\n",
            "Skipping encoder/block_000/layer_001/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 512) for ['encoder', 'block_001', 'layer_000', 'SelfAttention', 'k']\n",
            "Initialize PyTorch weight ['encoder', 'block_001', 'layer_000', 'SelfAttention', 'k']\n",
            "Skipping encoder/block_001/layer_000/SelfAttention/k_slot_vc\n",
            "Skipping encoder/block_001/layer_000/SelfAttention/k_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['encoder', 'block_001', 'layer_000', 'SelfAttention', 'o']\n",
            "Initialize PyTorch weight ['encoder', 'block_001', 'layer_000', 'SelfAttention', 'o']\n",
            "Skipping encoder/block_001/layer_000/SelfAttention/o_slot_vc\n",
            "Skipping encoder/block_001/layer_000/SelfAttention/o_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['encoder', 'block_001', 'layer_000', 'SelfAttention', 'q']\n",
            "Initialize PyTorch weight ['encoder', 'block_001', 'layer_000', 'SelfAttention', 'q']\n",
            "Skipping encoder/block_001/layer_000/SelfAttention/q_slot_vc\n",
            "Skipping encoder/block_001/layer_000/SelfAttention/q_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['encoder', 'block_001', 'layer_000', 'SelfAttention', 'v']\n",
            "Initialize PyTorch weight ['encoder', 'block_001', 'layer_000', 'SelfAttention', 'v']\n",
            "Skipping encoder/block_001/layer_000/SelfAttention/v_slot_vc\n",
            "Skipping encoder/block_001/layer_000/SelfAttention/v_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['encoder', 'block_001', 'layer_000', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['encoder', 'block_001', 'layer_000', 'layer_norm', 'scale']\n",
            "Skipping encoder/block_001/layer_000/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 2048) for ['encoder', 'block_001', 'layer_001', 'DenseReluDense', 'wi', 'kernel']\n",
            "Initialize PyTorch weight ['encoder', 'block_001', 'layer_001', 'DenseReluDense', 'wi', 'kernel']\n",
            "Skipping encoder/block_001/layer_001/DenseReluDense/wi/kernel_slot_vc\n",
            "Skipping encoder/block_001/layer_001/DenseReluDense/wi/kernel_slot_vr\n",
            "Transposing numpy weight of shape (2048, 512) for ['encoder', 'block_001', 'layer_001', 'DenseReluDense', 'wo', 'kernel']\n",
            "Initialize PyTorch weight ['encoder', 'block_001', 'layer_001', 'DenseReluDense', 'wo', 'kernel']\n",
            "Skipping encoder/block_001/layer_001/DenseReluDense/wo/kernel_slot_vc\n",
            "Skipping encoder/block_001/layer_001/DenseReluDense/wo/kernel_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['encoder', 'block_001', 'layer_001', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['encoder', 'block_001', 'layer_001', 'layer_norm', 'scale']\n",
            "Skipping encoder/block_001/layer_001/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 512) for ['encoder', 'block_002', 'layer_000', 'SelfAttention', 'k']\n",
            "Initialize PyTorch weight ['encoder', 'block_002', 'layer_000', 'SelfAttention', 'k']\n",
            "Skipping encoder/block_002/layer_000/SelfAttention/k_slot_vc\n",
            "Skipping encoder/block_002/layer_000/SelfAttention/k_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['encoder', 'block_002', 'layer_000', 'SelfAttention', 'o']\n",
            "Initialize PyTorch weight ['encoder', 'block_002', 'layer_000', 'SelfAttention', 'o']\n",
            "Skipping encoder/block_002/layer_000/SelfAttention/o_slot_vc\n",
            "Skipping encoder/block_002/layer_000/SelfAttention/o_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['encoder', 'block_002', 'layer_000', 'SelfAttention', 'q']\n",
            "Initialize PyTorch weight ['encoder', 'block_002', 'layer_000', 'SelfAttention', 'q']\n",
            "Skipping encoder/block_002/layer_000/SelfAttention/q_slot_vc\n",
            "Skipping encoder/block_002/layer_000/SelfAttention/q_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['encoder', 'block_002', 'layer_000', 'SelfAttention', 'v']\n",
            "Initialize PyTorch weight ['encoder', 'block_002', 'layer_000', 'SelfAttention', 'v']\n",
            "Skipping encoder/block_002/layer_000/SelfAttention/v_slot_vc\n",
            "Skipping encoder/block_002/layer_000/SelfAttention/v_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['encoder', 'block_002', 'layer_000', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['encoder', 'block_002', 'layer_000', 'layer_norm', 'scale']\n",
            "Skipping encoder/block_002/layer_000/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 2048) for ['encoder', 'block_002', 'layer_001', 'DenseReluDense', 'wi', 'kernel']\n",
            "Initialize PyTorch weight ['encoder', 'block_002', 'layer_001', 'DenseReluDense', 'wi', 'kernel']\n",
            "Skipping encoder/block_002/layer_001/DenseReluDense/wi/kernel_slot_vc\n",
            "Skipping encoder/block_002/layer_001/DenseReluDense/wi/kernel_slot_vr\n",
            "Transposing numpy weight of shape (2048, 512) for ['encoder', 'block_002', 'layer_001', 'DenseReluDense', 'wo', 'kernel']\n",
            "Initialize PyTorch weight ['encoder', 'block_002', 'layer_001', 'DenseReluDense', 'wo', 'kernel']\n",
            "Skipping encoder/block_002/layer_001/DenseReluDense/wo/kernel_slot_vc\n",
            "Skipping encoder/block_002/layer_001/DenseReluDense/wo/kernel_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['encoder', 'block_002', 'layer_001', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['encoder', 'block_002', 'layer_001', 'layer_norm', 'scale']\n",
            "Skipping encoder/block_002/layer_001/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 512) for ['encoder', 'block_003', 'layer_000', 'SelfAttention', 'k']\n",
            "Initialize PyTorch weight ['encoder', 'block_003', 'layer_000', 'SelfAttention', 'k']\n",
            "Skipping encoder/block_003/layer_000/SelfAttention/k_slot_vc\n",
            "Skipping encoder/block_003/layer_000/SelfAttention/k_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['encoder', 'block_003', 'layer_000', 'SelfAttention', 'o']\n",
            "Initialize PyTorch weight ['encoder', 'block_003', 'layer_000', 'SelfAttention', 'o']\n",
            "Skipping encoder/block_003/layer_000/SelfAttention/o_slot_vc\n",
            "Skipping encoder/block_003/layer_000/SelfAttention/o_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['encoder', 'block_003', 'layer_000', 'SelfAttention', 'q']\n",
            "Initialize PyTorch weight ['encoder', 'block_003', 'layer_000', 'SelfAttention', 'q']\n",
            "Skipping encoder/block_003/layer_000/SelfAttention/q_slot_vc\n",
            "Skipping encoder/block_003/layer_000/SelfAttention/q_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['encoder', 'block_003', 'layer_000', 'SelfAttention', 'v']\n",
            "Initialize PyTorch weight ['encoder', 'block_003', 'layer_000', 'SelfAttention', 'v']\n",
            "Skipping encoder/block_003/layer_000/SelfAttention/v_slot_vc\n",
            "Skipping encoder/block_003/layer_000/SelfAttention/v_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['encoder', 'block_003', 'layer_000', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['encoder', 'block_003', 'layer_000', 'layer_norm', 'scale']\n",
            "Skipping encoder/block_003/layer_000/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 2048) for ['encoder', 'block_003', 'layer_001', 'DenseReluDense', 'wi', 'kernel']\n",
            "Initialize PyTorch weight ['encoder', 'block_003', 'layer_001', 'DenseReluDense', 'wi', 'kernel']\n",
            "Skipping encoder/block_003/layer_001/DenseReluDense/wi/kernel_slot_vc\n",
            "Skipping encoder/block_003/layer_001/DenseReluDense/wi/kernel_slot_vr\n",
            "Transposing numpy weight of shape (2048, 512) for ['encoder', 'block_003', 'layer_001', 'DenseReluDense', 'wo', 'kernel']\n",
            "Initialize PyTorch weight ['encoder', 'block_003', 'layer_001', 'DenseReluDense', 'wo', 'kernel']\n",
            "Skipping encoder/block_003/layer_001/DenseReluDense/wo/kernel_slot_vc\n",
            "Skipping encoder/block_003/layer_001/DenseReluDense/wo/kernel_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['encoder', 'block_003', 'layer_001', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['encoder', 'block_003', 'layer_001', 'layer_norm', 'scale']\n",
            "Skipping encoder/block_003/layer_001/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 512) for ['encoder', 'block_004', 'layer_000', 'SelfAttention', 'k']\n",
            "Initialize PyTorch weight ['encoder', 'block_004', 'layer_000', 'SelfAttention', 'k']\n",
            "Skipping encoder/block_004/layer_000/SelfAttention/k_slot_vc\n",
            "Skipping encoder/block_004/layer_000/SelfAttention/k_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['encoder', 'block_004', 'layer_000', 'SelfAttention', 'o']\n",
            "Initialize PyTorch weight ['encoder', 'block_004', 'layer_000', 'SelfAttention', 'o']\n",
            "Skipping encoder/block_004/layer_000/SelfAttention/o_slot_vc\n",
            "Skipping encoder/block_004/layer_000/SelfAttention/o_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['encoder', 'block_004', 'layer_000', 'SelfAttention', 'q']\n",
            "Initialize PyTorch weight ['encoder', 'block_004', 'layer_000', 'SelfAttention', 'q']\n",
            "Skipping encoder/block_004/layer_000/SelfAttention/q_slot_vc\n",
            "Skipping encoder/block_004/layer_000/SelfAttention/q_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['encoder', 'block_004', 'layer_000', 'SelfAttention', 'v']\n",
            "Initialize PyTorch weight ['encoder', 'block_004', 'layer_000', 'SelfAttention', 'v']\n",
            "Skipping encoder/block_004/layer_000/SelfAttention/v_slot_vc\n",
            "Skipping encoder/block_004/layer_000/SelfAttention/v_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['encoder', 'block_004', 'layer_000', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['encoder', 'block_004', 'layer_000', 'layer_norm', 'scale']\n",
            "Skipping encoder/block_004/layer_000/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 2048) for ['encoder', 'block_004', 'layer_001', 'DenseReluDense', 'wi', 'kernel']\n",
            "Initialize PyTorch weight ['encoder', 'block_004', 'layer_001', 'DenseReluDense', 'wi', 'kernel']\n",
            "Skipping encoder/block_004/layer_001/DenseReluDense/wi/kernel_slot_vc\n",
            "Skipping encoder/block_004/layer_001/DenseReluDense/wi/kernel_slot_vr\n",
            "Transposing numpy weight of shape (2048, 512) for ['encoder', 'block_004', 'layer_001', 'DenseReluDense', 'wo', 'kernel']\n",
            "Initialize PyTorch weight ['encoder', 'block_004', 'layer_001', 'DenseReluDense', 'wo', 'kernel']\n",
            "Skipping encoder/block_004/layer_001/DenseReluDense/wo/kernel_slot_vc\n",
            "Skipping encoder/block_004/layer_001/DenseReluDense/wo/kernel_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['encoder', 'block_004', 'layer_001', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['encoder', 'block_004', 'layer_001', 'layer_norm', 'scale']\n",
            "Skipping encoder/block_004/layer_001/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 512) for ['encoder', 'block_005', 'layer_000', 'SelfAttention', 'k']\n",
            "Initialize PyTorch weight ['encoder', 'block_005', 'layer_000', 'SelfAttention', 'k']\n",
            "Skipping encoder/block_005/layer_000/SelfAttention/k_slot_vc\n",
            "Skipping encoder/block_005/layer_000/SelfAttention/k_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['encoder', 'block_005', 'layer_000', 'SelfAttention', 'o']\n",
            "Initialize PyTorch weight ['encoder', 'block_005', 'layer_000', 'SelfAttention', 'o']\n",
            "Skipping encoder/block_005/layer_000/SelfAttention/o_slot_vc\n",
            "Skipping encoder/block_005/layer_000/SelfAttention/o_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['encoder', 'block_005', 'layer_000', 'SelfAttention', 'q']\n",
            "Initialize PyTorch weight ['encoder', 'block_005', 'layer_000', 'SelfAttention', 'q']\n",
            "Skipping encoder/block_005/layer_000/SelfAttention/q_slot_vc\n",
            "Skipping encoder/block_005/layer_000/SelfAttention/q_slot_vr\n",
            "Transposing numpy weight of shape (512, 512) for ['encoder', 'block_005', 'layer_000', 'SelfAttention', 'v']\n",
            "Initialize PyTorch weight ['encoder', 'block_005', 'layer_000', 'SelfAttention', 'v']\n",
            "Skipping encoder/block_005/layer_000/SelfAttention/v_slot_vc\n",
            "Skipping encoder/block_005/layer_000/SelfAttention/v_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['encoder', 'block_005', 'layer_000', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['encoder', 'block_005', 'layer_000', 'layer_norm', 'scale']\n",
            "Skipping encoder/block_005/layer_000/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512, 2048) for ['encoder', 'block_005', 'layer_001', 'DenseReluDense', 'wi', 'kernel']\n",
            "Initialize PyTorch weight ['encoder', 'block_005', 'layer_001', 'DenseReluDense', 'wi', 'kernel']\n",
            "Skipping encoder/block_005/layer_001/DenseReluDense/wi/kernel_slot_vc\n",
            "Skipping encoder/block_005/layer_001/DenseReluDense/wi/kernel_slot_vr\n",
            "Transposing numpy weight of shape (2048, 512) for ['encoder', 'block_005', 'layer_001', 'DenseReluDense', 'wo', 'kernel']\n",
            "Initialize PyTorch weight ['encoder', 'block_005', 'layer_001', 'DenseReluDense', 'wo', 'kernel']\n",
            "Skipping encoder/block_005/layer_001/DenseReluDense/wo/kernel_slot_vc\n",
            "Skipping encoder/block_005/layer_001/DenseReluDense/wo/kernel_slot_vr\n",
            "Transposing numpy weight of shape (512,) for ['encoder', 'block_005', 'layer_001', 'layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['encoder', 'block_005', 'layer_001', 'layer_norm', 'scale']\n",
            "Skipping encoder/block_005/layer_001/layer_norm/scale_slot_v\n",
            "Transposing numpy weight of shape (512,) for ['encoder', 'final_layer_norm', 'scale']\n",
            "Initialize PyTorch weight ['encoder', 'final_layer_norm', 'scale']\n",
            "Skipping encoder/final_layer_norm/scale_slot_v\n",
            "Skipping global_step\n",
            "Initialize PyTorch weight ['shared', 'embedding']\n",
            "Skipping shared/embedding_slot_vc\n",
            "Skipping shared/embedding_slot_vr\n",
            "Weights not copied to PyTorch model: \n",
            "Save PyTorch model to ./dumps\n",
            "Configuration saved in ./dumps/config.json\n",
            "Model weights saved in ./dumps/pytorch_model.bin\n",
            "code&comment2code: <code> private static Set<String> getRootQualifiers(ResourceTypes resourceTypes) { return resourceTypes.getRoots().stream() .map(ResourceType::getQualifier) <START> .filter(q -> !DEPRECATED_QUALIFIERS.contains(q)) <END> .collect(Collectors.toCollection(TreeSet::new)); } </code><technical_language> I LIBRARY more </technical_language>\n",
            "private static Set<String> getRootQualifiers(ResourceTypes resourceTypes) { return resourceTypes.getRoots().stream() .map(ResourceType::getQualifier) .collect(Collectors.toCollection(TreeSet::new)); }\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbBqAZvxqAcM"
      },
      "source": [
        "class EvalDataset(torch.utils.data.Dataset):\n",
        "  samples = []\n",
        "\n",
        "  def __init__(self,  data_dir_path):\n",
        "    data_dir = data_dir_path\n",
        "    datasets = ['test.source', 'test.target']\n",
        "    self.samples = []    \n",
        "\n",
        "    input_file = open(os.path.join(data_dir, 'test.source'), 'r')\n",
        "    output_file = open(os.path.join(data_dir, 'test.target'), 'r')\n",
        "\n",
        "    lines_input = input_file.readlines()\n",
        "    output_lines = output_file.readlines()\n",
        "\n",
        "    for (inp, out) in zip(lines_input, output_lines):\n",
        "      self.samples.append((inp.rstrip(), out.rstrip()))\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    return self.samples[index]\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.samples)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T5MKKN4MjnpL",
        "outputId": "269a4864-8f43-494d-c970-e17292b8f612"
      },
      "source": [
        "beam_size = 10\n",
        "batch_size = 8\n",
        "data_dir = \"/content/data\"\n",
        "tokenizer_name = \"./current_model/TestModel.model\" \n",
        "model_name_or_path = \"./dumps/pytorch_model.bin\"\n",
        "config_name = \"./current_model/config.json\"\n",
        "\n",
        "dataset = EvalDataset(data_dir)\n",
        "dloader = DataLoader(dataset=dataset, batch_size=batch_size, num_workers=0)\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # My envirnment uses CPU\n",
        "\n",
        "t5_tokenizer = T5Tokenizer.from_pretrained(tokenizer_name)\n",
        "\n",
        "t5_config = T5Config.from_pretrained(config_name)\n",
        "t5_mlm = T5ForConditionalGeneration.from_pretrained(model_name_or_path, config=t5_config).to(DEVICE)\n",
        "# t5_mlm.eval()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Calling T5Tokenizer.from_pretrained() with the path to a single file or url is deprecated\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sRBQbX1Mm8W6",
        "outputId": "caecfb10-ea4f-4706-ce7a-e4639e2540ef"
      },
      "source": [
        "predictions = []\n",
        "code_bleues = []\n",
        "\n",
        "perfect = 0\n",
        "almost_perfect = 0\n",
        "\n",
        "# indexes for batches\n",
        "old = 0\n",
        "new = batch_size * beam_size\n",
        "\n",
        "for batch in tqdm(dloader): \n",
        "  encoded = t5_tokenizer.batch_encode_plus(batch[0], add_special_tokens=False, return_tensors='pt', padding=True)\n",
        "  \n",
        "  input_ids = encoded['input_ids'].to(DEVICE)\n",
        "  attention_mask = encoded['attention_mask'].to(DEVICE)\n",
        "\n",
        "  outputs = t5_mlm.generate(\n",
        "      input_ids=input_ids,\n",
        "      max_length=512, #Change here \n",
        "      num_beams=beam_size,\n",
        "      attention_mask=attention_mask,\n",
        "      early_stopping=True,\n",
        "      num_return_sequences=beam_size).to(DEVICE)\n",
        "\n",
        "  predictions.extend(t5_tokenizer.batch_decode(outputs, skip_special_tokens=True))\n",
        "\n",
        "  to_analyze = predictions[old:new]\n",
        "  target_list = batch[1]\n",
        "  input_list = batch[0]\n",
        "\n",
        "  idx = 0\n",
        "  for (input_item, target_item) in zip(input_list,target_list):\n",
        "\n",
        "    flag_perfect = False\n",
        "    flag_almost_perfect = False\n",
        "\n",
        "    target_item = \" \".join(target_item.split(' '))\n",
        "    best_code_bleu = 0\n",
        "    \n",
        "    for i in range(beam_size):\n",
        "      prediction_item = \" \".join(to_analyze[idx].split(' '))\n",
        "\n",
        "      if not flag_perfect and prediction_item == target_item:\n",
        "        perfect += 1\n",
        "        flag_perfect = True\n",
        "\n",
        "      if not flag_almost_perfect and \"\".join(prediction_item.split(' ')) == \"\".join(target_item.split(' ')):\n",
        "        almost_perfect += 1\n",
        "        flag_almost_perfect = True\n",
        "\n",
        "      idx += 1\n",
        "\n",
        "      with open(\"code_bleu_target.txt\", \"w\") as target_cb:\n",
        "        target_cb.write(target_item + \"\\n\")\n",
        "      with open(\"code_bleu_prediction.txt\", \"w\") as prediction_cb:\n",
        "        prediction_cb.write(prediction_item + \"\\n\")\n",
        "\n",
        "      try:\n",
        "        result = !cd /content/CodeXGLUE/Code-Code/code-to-code-trans/evaluator/CodeBLEU/ && python calc_code_bleu.py --refs ./../../../../../code_bleu_target.txt --hyp ./../../../../../code_bleu_prediction.txt --lang java --params 0.25,0.25,0.25,0.25\n",
        "        code_bleu = float(result[1][result[1].index(\"CodeBLEU score:  \")+17:])\n",
        "        \n",
        "      except: \n",
        "        result = !cd /content/CodeXGLUE/Code-Code/code-to-code-trans/evaluator/CodeBLEU/ && python calc_code_bleu.py --refs ./../../../../../code_bleu_target.txt --hyp ./../../../../../code_bleu_prediction.txt --lang java --params 0.333,0.333,0.333,0\n",
        "        code_bleu = float(result[2][result[2].index(\"CodeBLEU score:  \")+17:])\n",
        "\n",
        "      best_code_bleu = code_bleu if code_bleu > best_code_bleu else best_code_bleu\n",
        "        \n",
        "    code_bleues.append(best_code_bleu)\n",
        "\n",
        "  old = new\n",
        "  new = new + (batch_size * beam_size)\n",
        "\n",
        "perfect_percentage = (perfect/len(dataset))*100\n",
        "almost_perfect_percentage = (almost_perfect/len(dataset))*100\n",
        "mean_cb = statistics.mean(code_bleues)\n",
        "median_cb = statistics.median(code_bleues)\n",
        "stdev_cb = statistics.stdev(code_bleues)\n",
        "\n",
        "print()\n",
        "print('#perfect prediction: ', perfect)\n",
        "print('Perfect prediction {}%'.format(round(perfect_percentage, 2)))\n",
        "\n",
        "print('#almost perfect prediction: ', almost_perfect)\n",
        "print('Almost Perfect prediction {}%'.format(round(almost_perfect_percentage, 2)))\n",
        "\n",
        "print('Mean Bleu :', round(mean_cb, 2))\n",
        "print('Median Bleu :', round(median_cb, 2))\n",
        "print('stdev Bleu :', round(stdev_cb, 2))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 214/214 [1:14:57<00:00, 21.02s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "#perfect prediction:  595\n",
            "Perfect prediction 34.84%\n",
            "#almost perfect prediction:  733\n",
            "Almost Perfect prediction 42.92%\n",
            "Mean Bleu : 0.85\n",
            "Median Bleu : 0.89\n",
            "stdev Bleu : 0.16\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UDFMVaOHVl3u",
        "outputId": "ffe40444-aa6a-484f-f1ce-d0a7328ac17f"
      },
      "source": [
        "# run validation script\n",
        "!python3 validation.py --beam_size 3 --batch_size 24 --data_dir /content/data/ --tokenizer_name ./current_model/TestModel.model --model_name_or_path ./dumps/pytorch_model.bin --config_name ./current_model/config.json"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-04-12 16:16:42.434763: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "Calling T5Tokenizer.from_pretrained() with the path to a single file or url is deprecated\n",
            "100% 4/4 [00:19<00:00,  4.93s/it]\n",
            "#perfect prediction:  29\n",
            "Perfect prediction 36.25\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sMPTFagVOUmH",
        "outputId": "dbf6b0f2-1b09-4d91-dba5-9a0d49b3d287"
      },
      "source": [
        "!cd /content/CodeXGLUE/Code-Code/code-to-code-trans/evaluator/CodeBLEU/ && python calc_code_bleu.py --refs ./../../../../../code_bleu_target.txt --hyp ./../../../../../code_bleu_prediction.txt --lang java --params 0.25,0.25,0.25,0.25"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ngram match: 0.6703420896351792, weighted ngram match: 0.7390293031678882, syntax_match: 1.0, dataflow_match: 1.0',\n",
              " 'CodeBLEU score:  0.8523428482007669']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pYOv-1jUXJny",
        "outputId": "78e34132-18f2-44b1-9fc7-ee564f3f39c4"
      },
      "source": [
        "code = \"public static void main(String[] args) { \\u003CSTART> Int count = 3; int lotteria = 3 \\u003CEND>}\"#@param {type:\"string\"}\n",
        "comment = \"lotteria = 4\" #@param {type:\"string\"}\n",
        "input = \"code&comment2code: <code>\" + code + \"</code><technical_language>\" + comment + \"</technical_language>\"\n",
        "encoded = t5_tokenizer.encode(input, add_special_tokens=False, return_tensors='pt', padding=True).to(DEVICE)\n",
        "\n",
        "\n",
        "input_ids = encoded\n",
        "outputs = t5_mlm.generate(\n",
        "      input_ids=input_ids,\n",
        "      max_length=512, #Change here \n",
        "      num_beams=1,\n",
        "      early_stopping=True,\n",
        "      num_return_sequences=1).to(DEVICE)\n",
        "\n",
        "print(code)\n",
        "print(t5_tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "public static void main(String[] args) { <START> Int count = 3; int lotteria = 3 <END>}\n",
            "public static void main(String[] args) { Int count = 4; int lotteria = 4; }\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}